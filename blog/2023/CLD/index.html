<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Score-based generative modeling with critically-damped Langevin diffusion</p> <h1 id="ideas">Ideas</h1> <ul> <li>The diffusion process is the key to improve <strong>synthesis quality</strong> or** sampling speed**.</li> <li>Inspired by <strong>statistical mechanics.</strong> </li> <li> <table> <tbody> <tr> <td>The score of the conditional distribution $p_t(\boldsymbol{v}_t</td> <td>\boldsymbol{x}_t)$ is an arguably easier task than learning the score of $p_t(\boldsymbol{x}_t)$.</td> </tr> </tbody> </table> </li> </ul> <h1 id="background">Background</h1> <ul> <li>The forward process: $d \mathbf{u}_t=\boldsymbol{f}\left(\mathbf{u}_t, t\right) d t+\boldsymbol{G}\left(\mathbf{u}_t, t\right) d \mathbf{w}_t, \quad t \in[0, T]$</li> <li>The backward process: $d \overline{\mathbf{u}}<em>t=\left[-\boldsymbol{f}\left(\overline{\mathbf{u}}_t, T-t\right)+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) \boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right)^{\top} \nabla</em>{\overline{\mathbf{u}}<em>t} \log p</em>{T-t}\left(\overline{\mathbf{u}}_t\right)\right] d t+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) d \mathbf{w}_t$</li> <li>Currently used SDEs have drift and diffusion coefficients of the symple form: $\boldsymbol{f}\left(\mathbf{x}_t, t\right)=f(t) \mathbf{x}_t$ and $\boldsymbol{G}\left(\mathbf{x}_t, t\right)=g(t) \boldsymbol{I}_d$</li> <li>Generally, setting $p\left(\mathbf{u}<em>0\right)=p</em>{\text {data }}(\mathbf{x})$, $p\left(\mathbf{u}_T\right)= \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}_d)$.</li> <li>If $\boldsymbol{f}$ and $\boldsymbol{G}$ take the simple form above, the denoising score matching objective is: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \sim \mathcal{U}[0, T]} \mathbb{E}<em>{\mathbf{x}_0 \sim p\left(\mathbf{x}_0\right)} \mathbb{E}</em>{\mathbf{x}<em>t \sim p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left|\mathbf{s}</em>{\boldsymbol{\theta}}\left(\mathbf{x}<em>t, t\right)-\nabla</em>{\mathbf{x}_t} \log p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right|_2^2\right]$</li> <li>If $\boldsymbol{f}$ and $\boldsymbol{G}$ are affine, the conditional distribution $p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)$ is Normal and avaiable analytically.</li> </ul> <h1 id="critically-damped-langevin-dynamic">Critically-damped Langevin Dynamic</h1> <ul> <li>We propose to augment the data $\mathbf{x}_t \in \mathbb{R}^d$ and $\mathbf{v}_t \in \mathbb{R}^d$. With $\mathbf{u}_t=\left(\mathbf{x}_t, \mathbf{v}_t\right)^{\top} \in \mathbb{R}^{2 d}$, we set</li> <li>$\boldsymbol{f}\left(\mathbf{u}_t, t\right):=\left(\left(\begin{array}{cc}0 &amp; \beta M^{-1} \ -\beta &amp; -\Gamma \beta M^{-1}\end{array}\right) \otimes \boldsymbol{I}_d\right) \mathbf{u}_t, \quad \boldsymbol{G}\left(\mathbf{u}_t, t\right):=\left(\begin{array}{cc}0 &amp; 0 \ 0 &amp; \sqrt{2 \Gamma \beta}\end{array}\right) \otimes \boldsymbol{I}_d$</li> <li>The coupled SDE that describes the diffusion process: $\left(\begin{array}{l}d \mathbf{x}<em>t \ d \mathbf{v}_t\end{array}\right)=\underbrace{\left(\begin{array}{c}M^{-1} \mathbf{v}_t \ -\mathbf{x}_t\end{array}\right) \beta d t}</em>{\text {Hamiltonian component }=: H}+\underbrace{\left(\begin{array}{c}\mathbf{0}<em>d \ -\Gamma M^{-1} \mathbf{v}_t\end{array}\right) \beta d t+\left(\begin{array}{c}0 \ \sqrt{2 \Gamma \beta}\end{array}\right) d \mathbf{w}_t}</em>{\text {Ornstein-Uhlenbeck process=:O }}$</li> <li>The mass $M \in \mathbb{R}^+$ is a hyperparameter that determines the coupling between the $\mathbf{x}_t$and $\mathbf{v}_t$ variables.</li> <li>$\beta \in \mathbb{R}^+$ is a constant time rescaling chosen such that the diffusion <strong>converges to its equilibrium distribution</strong> (we found constant $\beta$’s to work well).</li> <li>$\Gamma \in \mathbb{R}^+$ is a friction coefficient that determines <strong>the strength of the noise injection</strong> into the velocities.</li> <li>The Hamiltonian component plays a role to <strong>accelerate sampling</strong> and <strong>efﬁciently explore complex probability distributions</strong>.</li> <li>The $O$ term corresponds to an <strong>Ornstein-Uhlenbeck process</strong> in the velocity, which injects noise such that the diffusion dynamics properly converge to equilibrium for any $\Gamma &gt; 0$.</li> <li>It can be shown that the equilibrium distribution of this diffusion is $p_{\mathrm{EQ}}(\mathbf{u})=\mathcal{N}\left(\mathbf{x} ; \mathbf{0}_d, \boldsymbol{I}_d\right) \mathcal{N}\left(\mathbf{v} ; \mathbf{0}_d, M \boldsymbol{I}_d\right)$</li> <li>The balance between $M$ and $\Gamma$ <ul> <li>For $\Gamma^2 &lt; 4M$(underdamped Langevin dynamics): oscillatory dynamics of $\mathbf{x}_t$ and $\boldsymbol{v}_t$ that slow down converge to equilibrium.</li> <li>For $\Gamma^2 &gt; 4M$(overdamped Langevin dynamics): the $O$ term dominates wihci also slows down convergence.</li> <li>For $\Gamma^2 = 4M$(critically-damped Langevin dynamics): an ideal balance is achieved and convergence to $p_{\mathrm{EQ}}(\mathbf{u})$ as fast as possible in a smooth manner without oscillations.</li> </ul> </li> </ul> <h1 id="score-matching-objective">Score Matching Objective</h1> <ul> <li>we initialize the joint $\bar{p}\left(\mathbf{u}<em>0\right)=p\left(\mathbf{x}_0\right) p\left(\mathbf{v}_0\right)=p</em>{\text {data }}\left(\mathbf{x}_0\right) \mathcal{N}\left(\mathbf{v}_0 ; \mathbf{0}_d, \gamma M \boldsymbol{I}_d\right)$ with hyperparameter $\gamma &lt; 1$</li> <li>let the distribution diffuse towards the tractable equilibrium—or prior—distribution $p_{\mathrm{EQ}}(\mathbf{u})$.</li> <li>The score matching (SM) objective: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \sim \mathcal{U}[0, T]} \mathbb{E}<em>{\mathbf{u}_t \sim p_t\left(\mathbf{u}_t\right)}\left[\lambda(t)\left|s</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)-\nabla</em>{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t\right)\right|_2^2\right]$</li> <li>$\nabla_{\mathbf{v}<em>t} \log p_t\left(\mathbf{u}_t\right)=\nabla</em>{\mathbf{v}<em>t}\left[\log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)+\log p_t\left(\mathbf{x}_t\right)\right]=\nabla</em>{\mathbf{v}_t} \log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$</li> <li>Why $p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$ is eaiser to learn <ul> <li>our velocity distribution is initialized from a simple Normal distribution, such that $p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$ is closer to a Normal distribution for all $t \geq 0$ than $p_t\left( \mathbf{x}_t\right)$ itself.</li> <li>empirically verify the reduced complexity <h1 id="hybrid-score-matching">Hybrid score matching</h1> </li> </ul> </li> <li>Objective: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \in[0, T]} \mathbb{E}<em>{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}</em>{\mathbf{u}<em>t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left|s</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)-\nabla</em>{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)\right|_2^2\right]$ <h1 id="score-model-parameterization">Score Model Parameterization</h1> </li> <li>$\mathbf{u}<em>t=\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}</em>{2 d}$, where $\boldsymbol{\Sigma}<em>t=\boldsymbol{L}_t \boldsymbol{L}_t^{\top}$ is the Cholesky decomposition of $p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)$’s covariance matrix, $\boldsymbol{\epsilon}</em>{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}<em>{2 d} ; \mathbf{0}</em>{2 d}, \boldsymbol{I}_{2 d}\right)$, and $\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)$ is $p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)$’s mean.</li> <li>$\nabla_{\mathbf{v}<em>t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)=-\ell_t \boldsymbol{\epsilon}</em>{d: 2 d}$</li> <li>With $\boldsymbol{\Sigma}<em>t=\underbrace{\left(\begin{array}{cc}\Sigma_t^{x x} &amp; \Sigma_t^{x v} \ \Sigma_t^{x v} &amp; \Sigma_t^{v v}\end{array}\right)}</em>{\text {“per-dimension” covariance matrix }} \otimes \boldsymbol{I}_d, \quad$ we have $\quad \ell_t:=\sqrt{\frac{\Sigma_t^{x x}}{\Sigma_t^{x x} \Sigma_t^{v v}-\left(\Sigma_t^{x v}\right)^2}}$</li> <li>We parameterize $s_{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)=-\ell_t \alpha</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)$ with $\alpha</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)=\ell_t^{-1} \mathbf{v}_t / \Sigma_t^{v v}+ \alpha’</em>{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)$, where $\Sigma_t^{vv}$ corresponds to the $v-v$ component of the “per-dimension” covariance matrix of the Normal Distribution $p_t\left(\mathbf{u}_t \mid \mathbf{x}_0=\mathbf{0}_d\right)$</li> <li>$\operatorname{HSM}(\lambda(t))=\mathbb{E}<em>{t \sim \mathcal{U}[0, T], \mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right), \mathbf{u}_t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left(\ell_t^{\mathrm{HSM}}\right)^2\left|\boldsymbol{\epsilon}</em>{d: 2 d}-\alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)\right|_2^2\right]$</li> <li>Training objective: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \sim \mathcal{U}[0, T]} \mathbb{E}<em>{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}</em>{\boldsymbol{\epsilon}<em>{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}</em>{2 d} ; \mathbf{0}<em>{2 d}, \boldsymbol{I}</em>{2 d}\right)}\left[\lambda(t) \ell_t^2\left|\boldsymbol{\epsilon}<em>{d: 2 d}-\alpha</em>{\boldsymbol{\theta}}\left(\boldsymbol{\mu}<em>t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}</em>{2 d}, t\right)\right|_2^2\right]$</li> </ul> </body></html>