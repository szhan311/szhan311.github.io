<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Paper: Where to diffuse, how to diffuse, and how to get back: Automated learning for multivariate diffusions.</p> <h1 id="background">Background</h1> <ul> <li>The choice of this inference process affects both <strong>likelihoods and sample quality</strong>.</li> <li>On different datasets and models, different inference processes work better;</li> <li>A natural question: are there other auxiliary variable diffusions that would lead to improvements like CLD?</li> <li>**Auxiliary variables **have improved other generative models and inferences <ul> <li>such as normalizing ﬂows, neural ordinary differential equations (ODEs), hierarchical variational models, ladder variational autoencoder</li> </ul> </li> </ul> <h1 id="requirement-of-design-a-diffusion-model">Requirement of design a diffusion model</h1> <ul> <li>Selecting an inference and model process pair <ul> <li>such that the inference process converges to the model prior</li> </ul> </li> <li>Deriving the ELBO for this pair</li> <li>Estimating the ELBO and its gradients <ul> <li>by deriving and computing the inference process’ transition kernel <h1 id="work-of-this-paper">Work of this paper</h1> </li> </ul> </li> <li>provide a recipe for training MDMs beyond speciﬁc instantiations to all linear inference processes that have a stationary distribution, <strong>with any number of auxiliary variables</strong>.</li> <li>Using results from gradient-based Markov chain Monte Carlo (MCMC) to construct MDMs</li> <li>derive the MDM ELBO</li> <li>the transition kernel of linear MDMs</li> </ul> <h1 id="elbo-bound">ELBO bound</h1> <p>Reverse process: $d \mathbf{z}=h_\theta(\mathbf{z}, t) d t+\beta_\theta(t) d \mathbf{B}<em>t, \quad t \in[0, T]$ Forward process: $d \mathbf{y}=f</em>\phi(\mathbf{y}, s) d s+g_\phi(s) d \widehat{\mathbf{B}}<em>s, \quad s \in[0, T]$ $\mathbf{z}_T$ approximates the data $x \sim q</em>{\text{data}}$ When the model take the form $d \mathbf{z}=\left[g_\phi^2(T-t) s_\theta(\mathbf{z}, T-t)-f_\phi(\mathbf{z}, T-t)\right] d t+g_\phi(T-t) d \mathbf{B}<em>t$ The ELBO is: $\log p</em>\theta(x) \geq \mathcal{L}^{\mathrm{ism}}(x)=\mathbb{E}<em>{q</em>\phi(\mathbf{y} \mid x)}\left[\log \pi_\theta\left(\mathbf{y}<em>T\right)+\int_0^T-\frac{1}{2}\left|s</em>\theta\right|<em>{g</em>\phi^2}^2-\nabla \cdot\left(g_\phi^2 s_\theta-f_\phi\right) d s\right]$ where $f_{\phi}, g_{\phi}, s_{\theta}$ are evaluated at $(\mathbf{y}<em>s, s)$, $|\mathbf{x}|</em>{\mathbf{A}}^2=\mathbf{x}^{\top} \mathbf{A} \mathbf{x}$ and $g^2 = gg^T$ ISM means Implicit Score Matching loss, which can be re-written as an ELBO $L^{\text{dsm}}$ featuring Denoisng Score Matching (DSM)</p> <h1 id="multivariate-model-and-inference">Multivariate Model and Inference</h1> <p>$\mathbf{u}=\left[\begin{array}{c}\mathbf{z}<em>t \ \mathbf{v}_t\end{array}\right]$ $\mathbf{u}_0 \sim \pi</em>\theta, \quad d \mathbf{u}=h_\theta\left(\mathbf{u}<em>t, t\right) d t+\beta</em>\theta(t) d \mathbf{B}_t$</p> </body></html>