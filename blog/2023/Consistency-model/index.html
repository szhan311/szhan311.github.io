<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="introduction">Introduction</h1> <p>Consistency model can be trained in two ways: <strong>distilling pre-trained diffusion models</strong>, <strong>as standalone generative models</strong></p> <h1 id="preliminaries">Preliminaries</h1> <h2 id="diffusion-models">Diffusion models</h2> <p>Let $p_{\text{data}}(\mathbf{x})$ denote the data distribution. Diffusion models start by diffusing $p_{\text{data}}(x)$ with a stochastic differential equation (SDE) $\mathrm{d} \mathbf{x}<em>t=\boldsymbol{\mu}\left(\mathbf{x}_t, t\right) \mathrm{d} t+\sigma(t) \mathrm{d} \mathbf{w}_t$ where $t \in [0, T], T &gt; 0$ is a fixed constant, $\mu (\cdot, \cdot)$ and $\sigma(\cdot)$ are the drift and diffusion coefficients respectively, and ${ \boldsymbol{\omega}_t}</em>{t \in [0, T]}$ denotes the standard Brownian motion. We denote the distribution of $\mathbf{x}<em>t$ as $p</em>{t}(x)$ and as a result $p_0(\mathbf{x}) \equiv p_{\text {data }}(\mathbf{x})$ <strong>the Probability Flow (PF) ODE</strong>: $\mathrm{d} \mathbf{x}<em>t=\left[\boldsymbol{\mu}\left(\mathbf{x}_t, t\right)-\frac{1}{2} \sigma(t)^2 \nabla \log p_t\left(\mathbf{x}_t\right)\right] \mathrm{d} t$ We adopt the setting: $\boldsymbol{\mu}(\mathbf{x}, t)=\mathbf{0}$ and $\sigma(t)=\sqrt{2 t}$, in this case, $p_t(\mathbf{x})=p</em>{\text {data }}(\mathbf{x}) \otimes \mathcal{N}\left(\mathbf{0}, t^2 \boldsymbol{I}\right)$, where $\otimes$ denotes the convolution operation, and $\pi(\mathbf{x})=\mathcal{N}\left(\mathbf{0}, T^2 \boldsymbol{I}\right)$ For sampling, we first train a score model $\boldsymbol{s}_\phi(\mathbf{x}, t) \approx \nabla \log p_t(\mathbf{x})$ via score matching, then we have the <strong>empirical PF ODE: **$\frac{\mathrm{d} \mathbf{x}<em>t}{\mathrm{~d} t}=-t s</em>\phi\left(\mathbf{x}_t, t\right)$ To solve this ODE, there are two main ways: **numerical ODE solver **and **distillation techniques</strong></p> <h2 id="edm">EDM</h2> <h3 id="denoiser">Denoiser</h3> <h4 id="equation-2-mathbbeboldsymboly-sim-ptext-data--mathbbe_boldsymboln-sim-mathcalnleftmathbf0-sigma2-mathbfirightdboldsymbolyboldsymboln--sigma-boldsymboly_22">Equation 2: $\mathbb{E}<em>{\boldsymbol{y} \sim p</em>{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}|_2^2$</h4> <p>The optimal analytically solution is $D(\boldsymbol{x} ; \sigma) =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}$ <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1701066290736-f039695c-0949-4d4d-aeac-fa80cd5ce23e.png#averageHue=%23dcdad8&amp;clientId=ua25b137d-f497-4&amp;from=paste&amp;height=231&amp;id=ue6a06ef2&amp;originHeight=324&amp;originWidth=1161&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=340528&amp;status=done&amp;style=none&amp;taskId=u953377f2-543e-4db8-9aa4-b0dca7fbfd6&amp;title=&amp;width=829.5" alt="image.png"> Based on Fig. 1, it seems that we cannot get clean image from corrupted image in 1 step.</p> <h3 id="preconditioning-and-training">Preconditioning and training</h3> <p>In practice, instead of parameterize $D_{\theta}$ directly, we train a different network $F_{\theta}$ from which $D_{\theta}$ is derived $D_\theta(\boldsymbol{x} ; \sigma)=c_{\text {skip }}(\sigma) \boldsymbol{x}+c_{\text {out }}(\sigma) F_\theta\left(c_{\text {in }}(\sigma) \boldsymbol{x} ; c_{\text {noise }}(\sigma)\right)$ The loss is: $\mathbb{E}<em>{\sigma, \boldsymbol{y}, \boldsymbol{n}}[\underbrace{\lambda(\sigma) c</em>{\text {out }}(\sigma)^2}<em>{\text {effective weight }}|\underbrace{F</em>\theta\left(c_{\text {in }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n}) ; c_{\text {noise }}(\sigma)\right)}<em>{\text {network output }}-\underbrace{\frac{1}{c</em>{\text {out }}(\sigma)}\left(\boldsymbol{y}-c_{\text {skip }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n})\right)}_{\text {effective training target }}|_2^2]$</p> <h1 id="consistency-model">Consistency Model</h1> <h2 id="what-is-consistency-model">What is consistency model</h2> <p>Definition Given a solution trajectory $\left{\mathbf{x}<em>t\right}</em>{t \in[\epsilon, T]}$ of the PF ODE$\mathrm{d} \mathbf{x}<em>t=\left[\boldsymbol{\mu}\left(\mathbf{x}_t, t\right)-\frac{1}{2} \sigma(t)^2 \nabla \log p_t\left(\mathbf{x}_t\right)\right] \mathrm{d} t$, we define the consistency function as $\boldsymbol{f}:\left(\mathbf{x}_t, t\right) \mapsto \mathbf{x}</em>\epsilon$. A consistency function has the property of self-consistency: it outputs are consistent for arbitrary pairs of $(\mathbf{x}<em>t, t)$ that belong to the same PF ODE trajectory, i.e., $\boldsymbol{f}\left(\mathbf{x}_t, t\right)=\boldsymbol{f}\left(\mathbf{x}</em>{t^{\prime}}, t^{\prime}\right)$ for all $t, t^{\prime} \in[\epsilon, T]$.</p> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1701109739051-d2d30d7b-6403-4661-bd02-246c22cd073e.png#averageHue=%23dfdcdb&amp;clientId=u546aa620-17e3-4&amp;from=paste&amp;height=461&amp;id=u26123346&amp;originHeight=461&amp;originWidth=677&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=251053&amp;status=done&amp;style=none&amp;taskId=u897eceda-657e-4552-b0a6-634403c1b40&amp;title=&amp;width=677.5" alt="image.png"> <strong>self-consistency</strong>: points on the same trajectory map to the same initial point.</p> <h2 id="parameterization">parameterization</h2> <p>parameterize the consistency model using skip connections: $\boldsymbol{f}<em>{\boldsymbol{\theta}}(\mathbf{x}, t)=c</em>{\text {skip }}(t) \mathbf{x}+c_{\text {out }}(t) F_{\boldsymbol{\theta}}(\mathbf{x}, t)$ where $c_{\text{skip}}(t)$ and $c_{\text{out}}(t)$ are differeitiable functions such that $c_{\text{skip}}(\epsilon) = 1$ and $c_{\text{out}}(\epsilon) = 0$</p> <h2 id="the-consistency-ditillation-loss">The consistency ditillation loss</h2> <p>$\begin{aligned} \mathcal{L}<em>{C D}^N\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{-} ; \boldsymbol{\phi}\right) &amp; := \ &amp; \mathbb{E}\left[\lambda\left(t_n\right) d\left(\boldsymbol{f}</em>{\boldsymbol{\theta}}\left(\mathbf{x}<em>{t</em>{n+1}}, t_{n+1}\right), \boldsymbol{f}<em>{\boldsymbol{\theta}^{-}}\left(\hat{\mathbf{x}}</em>{t_n}^{\boldsymbol{\phi}}, t_n\right)\right)\right]\end{aligned}$</p> </body></html>