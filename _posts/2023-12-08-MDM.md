---
layout: post
title:  Where to diffuse, how to diffuse, and how to get back
date: 2023-12-08 20:36:00
description: 
tags: Literature-review
categories: Diffusion-models
tikzjax: true
---

# Background

- The choice of this inference process affects both **likelihoods and sample quality**.

- On different datasets and models, different inference processes work better;

- A natural question: are there other auxiliary variable diffusions that would lead to improvements like CLD?

- **Auxiliary variables **have improved other generative models and inferences
   - such as normalizing ﬂows, neural ordinary differential equations (ODEs), hierarchical variational models, ladder variational autoencoder

# Requirement of design a diffusion model

- Selecting an inference and model process pair
   - such that the inference process converges to the model prior

- Deriving the ELBO for this pair

- Estimating the ELBO and its gradients
   - by deriving and computing the inference process’ transition kernel
# Work of this paper

- provide a recipe for training MDMs beyond speciﬁc instantiations to all linear inference processes that have a stationary distribution, **with any number of auxiliary variables**.

- Using results from gradient-based Markov chain Monte Carlo (MCMC) to construct MDMs.

- derive the MDM ELBO.

- the transition kernel of linear MDMs.

# ELBO bound

Reverse process: $$d \mathbf{z}=h_\theta(\mathbf{z}, t) d t+\beta_\theta(t) d \mathbf{B}_t, \quad t \in[0, T]$$

Forward process: $$d \mathbf{y}=f_\phi(\mathbf{y}, s) d s+g_\phi(s) d \widehat{\mathbf{B}}_s, \quad s \in[0, T]$$
$$\mathbf{z}_T$$ approximates the data $$x \sim q_{\text{data}}$$

When the model take the form

$$d \mathbf{z}=\left[g_\phi^2(T-t) s_\theta(\mathbf{z}, T-t)-f_\phi(\mathbf{z}, T-t)\right] d t+g_\phi(T-t) d \mathbf{B}_t$$

The ELBO is: $$\log p_\theta(x) \geq \mathcal{L}^{\mathrm{ism}}(x)=\mathbb{E}_{q_\phi(\mathbf{y} \mid x)}\left[\log \pi_\theta\left(\mathbf{y}_T\right)+\int_0^T-\frac{1}{2}\left\|s_\theta\right\|_{g_\phi^2}^2-\nabla \cdot\left(g_\phi^2 s_\theta-f_\phi\right) d s\right]$$

where $$f_{\phi}, g_{\phi}, s_{\theta}$$ are evaluated at $$(\mathbf{y}_s, s)$$, $$\|\mathbf{x}\|_{\mathbf{A}}^2=\mathbf{x}^{\top} \mathbf{A} \mathbf{x}$$ and $$g^2 = gg^T$$

ISM means Implicit Score Matching loss, which can be re-written as an ELBO $$L^{\text{dsm}}$$ featuring Denoisng Score Matching (DSM).

# Multivariate Model and Inference

$$\mathbf{u}=\left[\begin{array}{c}\mathbf{z}_t \\ \mathbf{v}_t\end{array}\right]$$
$$\mathbf{u}_0 \sim \pi_\theta, \quad d \mathbf{u}=h_\theta\left(\mathbf{u}_t, t\right) d t+\beta_\theta(t) d \mathbf{B}_t$$



