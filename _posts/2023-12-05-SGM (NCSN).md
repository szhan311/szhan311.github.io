---
layout: post
title:  Generative Modeling by Estimating Gradients of the Data Distribution
date: 2023-12-05 20:36:00
description: 
tags: Literature-review
categories: Diffusion-models
tikzjax: true
---

Gradients can be ill-deﬁned and hard to estimate when the data resides on low-dimensional manifolds.

Two ingredients: score matching and Langevin dynamics

Since Langevin dynamics will often be initialized in low-density regions of the data distribution, inaccurate score estimation in these regions will negatively affect the sampling process.

In practice, this analysis also holds when different modes have approximately disjoint supports—they may share the same support but be connected by regions of small data density.

# Score matching

The diffusion model uses energy-based models: : $$p_\theta(\mathbf{x})=\frac{e^{f_\theta(\mathbf{x})}}{Z(\theta)}$$
Score function: $$s_{\theta}(\textbf{x}) = \nabla_\mathbf{x} \log p_\theta(\mathbf{x})$$

The goal of score-based model:

$$\begin{equation*}
\mathbf{s}_\theta (\mathbf{x}) = \nabla_{\mathbf{x}} \log p_\theta (\mathbf{x} ) = \nabla_{\mathbf{x}}  f_\theta (\mathbf{x}) - \underbrace{\nabla_\mathbf{x} \log Z_\theta}_{=0} = \nabla_\mathbf{x} f_\theta(\mathbf{x}).
\end{equation*}$$

Minimize the Fisher divergence, or simply the MSE: $$\mathcal{L}_{mse} =  \mathbb{E}_{p_{\text {data }}} \left[\frac{1}{2} \left\lVert s_{\theta}(\mathbf{x}) - \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x}) \right\lVert_2^2 \right]$$

However, Fisher divergence is not directly computable, the real $$\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$$ is usually unknown.

To simply discussion, consider 1-D random variables:

$$\begin{aligned} \mathcal{L}_{mse} = & \frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\nabla_x \log p_{\text {data }}(x)-\nabla_x \log p_\theta(x)\right)^2\right] \\ = & \frac{1}{2} \int p_{\text {data }}(x)\left(\nabla_x \log p_{\text {data }}(x)-\nabla_x \log p_\theta(x)\right)^2 \mathrm{~d} x \\ = & \underbrace{\frac{1}{2} \int p_{\text {data }}(x)\left(\nabla_x \log p_{\text {data }}(x)\right)^2 \mathrm{~d} x}_{\text {const }}+\frac{1}{2} \int p_{\text {data }}(x)\left(\nabla_x \log p_\theta(x)\right)^2 \mathrm{~d} x \\ & -\int p_{\text {data }}(x) \nabla_x \log p_\theta(x) \nabla_x \log p_{\text {data }}(x) \mathrm{d} x .\end{aligned}$$

The objective can be written in 3 terms, the first term is a constant, the second term is easy to calculate, next we handle the thirt term by integration by parts:

$$\begin{aligned} &-\int p_{\text {data }}(x) \nabla_x \log p_\theta(x) \nabla_x \log p_{\text {data }}(x) \mathrm{d} x \\ &=-\int \nabla_x \log p_\theta(x) \nabla_x p_{\text {data }}(x) \mathrm{d} x \\ &=-\left.p_{\text {data }}(x) \nabla_x \log p_\theta(x)\right|_{-\infty} ^{\infty}+\int p_{\text {data }}(x) \nabla_x^2 \log p_\theta(x) \mathrm{d} x \\ & \stackrel{(i)}{=} \mathbb{E}_{p_{\text {data }}}\left[\nabla_x^2 \log p_\theta(x)\right]\end{aligned}$$

where $$(i)$$holds if we assume $$p_{\text{data}}(x) \rightarrow 0$$where $$|x|\rightarrow 0$$. Now,
$$\begin{aligned} & \frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\nabla_x \log p_{\text {data }}(x)-\nabla_x \log p_\theta(x)\right)^2\right] \\ = & \mathbb{E}_{p_{\text {data }}}\left[\nabla_x^2 \log p_\theta(x)\right]+\frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\nabla_x \log p_\theta(x)\right)^2\right]+\text { const. }\end{aligned}$$

Generalizing the integration by parts to multi-dimentsional data, the minimum of $$\mathcal{L}_{mse}$$ can be found through a tractable objective:

$$\mathcal{L}_{matching} =  \mathbb{E}_{p_{\text {data }}} \left[ \text{ tr}\left( \nabla_{\mathbf{x}}  s_{\theta}(\mathbf{x})  \right) + \frac{1}{2} \left\Vert s_{\theta}(\mathbf{x}) \right\lVert_2^2 \right]$$
where $$\nabla_{\mathbf{x}} s_{\theta}(\mathbf{x})$$ denotes the Jacobian of $$s_{\theta}(\mathbf{x})$$ with respect to $$\mathbf{x}$$, and $$\text{tr}(\cdot)$$ is the _trace_ operation.

Cons: the computation of$$\text{ tr}\left( \nabla_{\mathbf{x}}  s_{\theta}(\mathbf{x})  \right)$$is a $$O(N^2+N)$$, thus not being suitable for high-dimensional problems.

# Sliced score matching

Idea: **one dimensional data** distribution is much easier to estimate for score matching.

We denote $$\mathbf{v}$$as the random projection direction, and $$p_\mathbf{v}$$as its distribution. The random projected version of Fisher divergence is:

$$\frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\mathbf{v}^{\top} \nabla_{\mathbf{x}} \log p_{\text {data }}(\mathbf{x})-\mathbf{v}^{\top} \nabla_{\mathbf{x}} \log p_\theta(\mathbf{x})\right)^2\right]$$
which we name as the sliced Fisher divergence. Therefore play the same trick of integration by parts to obatain the following tractable form:

- $$\mathbb{E}_{p_{\text {data }}}\left[\mathbf{v}^{\top} \nabla_{\mathbf{x}}^2 \log p_\theta(\mathbf{x}) \mathbf{v}+\frac{1}{2}\left(\mathbf{v}^{\top} \nabla_{\mathbf{x}} \log p_\theta(\mathbf{x})\right)^2\right]+ \text{const}$$ 

- $$\mathbb{E}_{p_{\text {\textbf{v} }}}\mathbb{E}_{p_{\text {data }}} \left[ \mathbf{v}^T \nabla_{\mathbf{x}}  \mathcal{F}_{\theta}(\mathbf{x}) \mathbf{v} + \frac{1}{2} \left\Vert \mathbf{v}^T \mathcal{F}_{\theta}(\mathbf{x}) \right\lVert_2^2 \right]$$

# Denoising score matching

The objective is:

$$E_{q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})} E_{\mathbf{x} \sim p(\mathbf{x})} \left[ \left\Vert s_{\theta}(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x}) \right\lVert_2^2 \right]$$

However, it should be noted that $$s_{\theta}(\mathbf{x}) = \nabla_{\mathbf{x}} \log q_{\sigma}(\mathbf{x}) \approx \nabla_{\mathbf{x}} \log p(\mathbf{x})$$ is only true when the noise is small enough to consider that $$q_{\sigma}(\mathbf{x}) \approx p(\mathbf{x})$$, if we choose the noise distribution to be $$q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})=\mathcal{N}(\tilde{\mathbf{x}}\mid\mathbf{x}, \sigma^{2}\mathbf{I})$$, then we have $$\nabla_{\tilde{\mathbf{x}}} \log q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x}) = \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^{2}}$$. Therefore, the denoising score matching loss simply becomes:

$$\mathcal{l}(\theta;\sigma) = E_{q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})} E_{\mathbf{x} \sim p(\mathbf{x})} \left[ \left\Vert \mathcal{F}_{\theta}(\tilde{\mathbf{x}}) +  \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^{2}} \right\lVert_2^2 \right].$$

# Noise conditional score networks (NCSN)

We consider a positive geometric sequence of noise variances $$\{\sigma_{i}\}_{i=1}^{L}$$, choosing $$\sigma_{1}$$ to be large enough to mitigate manifold issue, and satisfying $$\frac{\sigma_{1}}{\sigma_{2}} = \cdots = \frac{\sigma_{L-1}}{\sigma_{L}} > 1$$. The goal is to train a conditional network to estimate the gradients of all perturbed data distributions, ie.

$$\forall \sigma \in \{\sigma_{i}\}_{i=1}^{L}, s_{\theta}(\tilde{\mathbf{x}}, \sigma) \approx \nabla_{\mathbf{x}} \log q_{\sigma}(\mathbf{x})$$

The objective

$$\mathcal{l}(\theta;\sigma) = E_{q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})} E_{\mathbf{x} \sim p(\mathbf{x})} \left[ \left\Vert s_{\theta}(\tilde{\mathbf{x}}, \sigma) +  \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^{2}} \right\lVert_2^2 \right]$$

this objective can be combined to obtain a single unified objective as:

$$\mathcal{L}(\theta;\{\sigma_{i}\}_{i=1}^{L}) = \frac{1}{L} \sum_{i=1}^{L} \lambda(\sigma_{i})\mathcal{l}(\theta;\sigma_{i})$$

where $$\lambda(\sigma_{i}) > 0$$ is a coefficient function depending on $$\sigma_{i}$$.

# Sampling with Langevin dynamics

- Langevin dynamics can produce samples from a probability density $$p(\mathbf{x})$$using only the score function $$\nabla_{\mathbf{x}} \log p(\mathbf{x})$$.

- $$\tilde{\mathbf{x}}_t = \tilde{\mathbf{x}}_{t-1} + \frac{\epsilon}{2} \nabla_{\mathbf{x}} \log p(\tilde{\mathbf{x}}_{t-1}) + \sqrt{\epsilon} \mathbf{z}_t$$

- a Metropolis-Hastings update is ignored
- Inaccurate score estimation with score matching in regions of low data density
- Annealed Langevin dynamic samples

