---
layout: post
title:  Elucidating the Design Space of Diffusion-Based Generative Models
date: 2023-12-06 20:36:00
description: 
tags: Literature-review
categories: Diffusion-models
tikzjax: true
---

# Original ODE / SDE formulation from previous work

- Song et al. define their forward SDE as: $$dx = f(x,t)dt + g(t) dw_t$$, where $$f(\cdot, t):\mathbb{R}^d \rightarrow \mathbb{R}^d$$and $$g(\cdot): \mathbb{R} \rightarrow \mathbb{R}$$are the drift and diffusion coefficients, repsectively, where $$d$$is the dimensionality of the dataset. $$f(\cdot)$$is always of the form

$$f(x, t) = f(t) x$$, where $$f(\cdot):\mathbb{R} \rightarrow \mathbb{R}.$$

- Thus, the SDE can be equivalently written as$$dx = f(t)x + g(t) d w_t$$.

- The pertubation kernels of this SDE have the general form $$p_{0t}(x(t) \vert x(0)) = \mathcal{N} (x(t); s(t)x(0), s^2(t) \sigma^2(t)\mathbf{I})$$,

- where $$\mathcal{N}(x; \mu, \Sigma)$$denotes the probability density function of $$\mathcal{N}(\mu, \Sigma)$$evaluated at $$x$$,

$$
s(t)=\exp \left(\int_0^t f(\xi) \mathrm{d} \xi\right), \quad \text { and } \quad \sigma(t)=\sqrt{\int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi}
$$

- The marginal distribution of $$p_t(x)$$is

$$
p_t(\boldsymbol{x})=\int_{\mathbb{R}^d} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}_0\right) p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0
$$

- The probability flow ODE

$$
\mathrm{d} \boldsymbol{x}=\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})\right] \mathrm{d} t .
$$

# Understanding the diffusion process

### Idea

$$f$$and $$g$$are of little practical interest, the marginal distribution are of utmost inportance

### The perturbation kernels and marginal distribution

$$p_{0 t}(\boldsymbol{x}(t) \mid \boldsymbol{x}(0))=\mathcal{N}\left(\boldsymbol{x}(t) ; s(t) \boldsymbol{x}(0), s(t)^2 \sigma(t)^2 \mathbf{I}\right)$$

$$p_t(\boldsymbol{x})=\int_{\mathbb{R}^d} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}_0\right) p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0$$

### Convolution form

$$p(\boldsymbol{x} ; \sigma) :=p_{\text {data }} * \mathcal{N}(\mathbf{0}, \sigma(t)^2 \mathbf{I})$$
 $$p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))$$

### ODE

#### $$\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t$$

$$\mathrm{d} \boldsymbol{x}=\left[\frac{\dot{\boldsymbol{s}}(t)}{s(t)} \boldsymbol{x}-s(t)^2 \dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p\left(\frac{\boldsymbol{x}}{s(t)} ; \sigma(t)\right)\right] \mathrm{d} t$$

### Denosing score matching

$$D(\boldsymbol{x}; \sigma)$$is a denoiser function that minimizes the expeced $$L_2$$denosing error for samples drawn from $$p_{\text{data}}$$, 

$$\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}\|_2^2$$, then $$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2$$

### Heat equation
$$\left\{\begin{matrix}

&\frac{\partial q}{\partial t} -\dot{\sigma} \sigma \Delta_{\boldsymbol{x}} q = 0  &\text{in} \quad \mathbb{R}^d \times(0, \, \infty) \\ 
&q= p_{data}(\boldsymbol{x}) & \text{on} \quad \mathbb{R}^d \times \{t=0\}

\end{matrix} \right.$$


### SDE
$$\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t$$

$$\mathrm{d} \boldsymbol{x}_{ \pm}=\underbrace{-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t}_{\text {probability flow ODE}} \pm \underbrace{\beta(t) \sigma(t)^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t}_{\text {deterministic noise decay }}+\underbrace{\sqrt{2 \beta(t)} \sigma(t) \mathrm{d} \omega_t}_{\text {noise injection }}$$

# ODE formulation
#### Equation 1: $$\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t$$

#### Proof

The marginal distrobution

$$
\begin{aligned}
p_t(\boldsymbol{x}) & =\int_{\mathbb{R}^d} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}_0\right) p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0 \\
& =\int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right)\left[\mathcal{N}\left(\boldsymbol{x} ; s(t) \boldsymbol{x}_0, s(t)^2 \sigma(t)^2 \mathbf{I}\right)\right] \mathrm{d} \boldsymbol{x}_0 \\
& =\int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right)\left[s(t)^{-d} \mathcal{N}\left(\boldsymbol{x} / s(t) ; \boldsymbol{x}_0, \sigma(t)^2 \mathbf{I}\right)\right] \mathrm{d} \boldsymbol{x}_0 \\
& =s(t)^{-d} \int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathcal{N}\left(\boldsymbol{x} / s(t) ; \boldsymbol{x}_0, \sigma(t)^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \\
& =s(t)^{-d}\left[p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)\right](\boldsymbol{x} / s(t)),
\end{aligned}
$$

where $$p_a * p_b$$denotes the convolution of probability density functions $$p_a$$and $$p_b$$

Let denote:

$$
p(\boldsymbol{x} ; \sigma)=p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right) \quad \text { and } \quad p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))
$$

the probability ﬂow ODE

$$
\begin{aligned}
\mathrm{d} \boldsymbol{x} & =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log \left[p_t(\boldsymbol{x})\right]\right] \mathrm{d} t \\
& =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log \left[s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))\right]\right] \mathrm{d} t \\
& =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2\left[\nabla_{\boldsymbol{x}} \log s(t)^{-d}+\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right]\right] \mathrm{d} t \\
& =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t .
\end{aligned}
$$

Next, $$f(t) = {\dot s(t)}/{s(t)}$$, $$g(t) = s(t) \sqrt{2 \dot{\sigma}(t) \sigma(t)}$$

$$
\begin{aligned}
\exp \left(\int_0^t f(\xi) \mathrm{d} \xi\right) & =s(t) \\
\int_0^t f(\xi) \mathrm{d} \xi & =\log s(t) \\
\mathrm{d}\left[\int_0^t f(\xi) \mathrm{d} \xi\right] / \mathrm{d} t & =\mathrm{d}[\log s(t)] / \mathrm{d} t \\
f(t) & =\dot{s}(t) / s(t) .
\end{aligned}
$$

$$
\begin{aligned}
\sqrt{\int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi} & =\sigma(t) \\
\int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi & =\sigma(t)^2 \\
\mathrm{~d}\left[\int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi\right] / \mathrm{d} t & =\mathrm{d}\left[\sigma(t)^2\right] / \mathrm{d} t \\
g(t)^2 / s(t)^2 & =2 \dot{\sigma}(t) \sigma(t) \\
g(t) / s(t) & =\sqrt{2 \dot{\sigma}(t) \sigma(t)} \\
g(t) & =s(t) \sqrt{2 \dot{\sigma}(t) \sigma(t)} .
\end{aligned}
$$

Finally

$$
\begin{aligned}
\mathrm{d} \boldsymbol{x} & =\left[[f(t)] \boldsymbol{x}-\frac{1}{2}[g(t)]^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t \\
& =\left[[\dot{s}(t) / s(t)] \boldsymbol{x}-\frac{1}{2}[s(t) \sqrt{2 \dot{\sigma}(t) \sigma(t)}]^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t \\
& =\left[[\dot{s}(t) / s(t)] \boldsymbol{x}-\frac{1}{2}\left[2 s(t)^2 \dot{\sigma}(t) \sigma(t)\right] \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t \\
& =\left[\frac{\dot{s}(t)}{s(t)} \boldsymbol{x}-s(t)^2 \dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p\left(\frac{\boldsymbol{x}}{s(t)} ; \sigma(t)\right)\right] \mathrm{d} t
\end{aligned}
$$

By setting $$s(t) = 1$$:

$$
\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t
$$

# Denoising score matching
#### Equation 2 & 3

$$\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}\|_2^2$$, then $$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2$$

#### Proof

Finite number of samples $$\{\mathbf{y}_1, \cdots, \mathbf{y}_Y \}$$,$$p_{data}(x)$$is represented by a mixture of Dirac delta distributions:$$p_{\text{data}}(x) = \frac{1}{Y} \sum_{i=1}^{Y} \delta(\mathbf{x} - \mathbf{y}_i)$$

$$\begin{aligned} p(\boldsymbol{x} ; \sigma) & =p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right) \\ & =\int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \\ & =\int_{\mathbb{R}^d}\left[\frac{1}{Y} \sum_{i=1}^Y \delta\left(\boldsymbol{x}_0-\boldsymbol{y}_i\right)\right] \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \\ & =\frac{1}{Y} \sum_{i=1}^Y \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \delta\left(\boldsymbol{x}_0-\boldsymbol{y}_i\right) \mathrm{d} \boldsymbol{x}_0 \\ & =\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\end{aligned}$$

Considering the Eq. 2. by expanding the expections:

$$\begin{aligned} \mathcal{L}(D ; \sigma) & =\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}\|_2^2 \\ & =\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{x} \sim \mathcal{N}\left(\boldsymbol{y}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}\|_2^2 \\ & =\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}, \sigma^2 \mathbf{I}\right)\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}\|_2^2 \mathrm{~d} \boldsymbol{x} \\ & =\frac{1}{Y} \sum_{i=1}^Y \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2 \mathrm{~d} \boldsymbol{x} \\ & =\int_{\mathbb{R}^d} \underbrace{\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2}_{=: \mathcal{L}(D ; \boldsymbol{x}, \sigma)} \mathrm{d} \boldsymbol{x} .\end{aligned}$$

we can minimize $$\mathcal{L}(D ; \sigma)$$by minimizing $$\mathcal{L}(D ; \boldsymbol{x}, \sigma)

$$independently for each $$\boldsymbol{x}$$:

$$D(\boldsymbol{x} ; \sigma)=\arg \min _{D(\boldsymbol{x} ; \sigma)} \mathcal{L}(D ; \boldsymbol{x}, \sigma)$$
This is a convex optimization problem; its solution is uniquely identiﬁed by setting the gradient w.r.t. $$D(\boldsymbol{x}; \sigma)$$to zero:

$$\begin{aligned} \mathbf{0} & =\nabla_{D(\boldsymbol{x} ; \sigma)}[\mathcal{L}(D ; \boldsymbol{x}, \sigma)] \\ \mathbf{0} & =\nabla_{D(\boldsymbol{x} ; \sigma)}\left[\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2\right] \\ \mathbf{0} & =\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \nabla_{D(\boldsymbol{x} ; \sigma)}\left[\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2\right] \\ \mathbf{0} & =\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[2 D(\boldsymbol{x} ; \sigma)-2 \boldsymbol{y}_i\right] \\ \mathbf{0} & =\left[\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\right] D(\boldsymbol{x} ; \sigma)-\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i \\ D(\boldsymbol{x} ; \sigma) & =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\end{aligned}$$

which gives a closed-form solution for the ideal denoiser $$D(\boldsymbol{x}; \sigma)$$.

$$D(\boldsymbol{x} ; \sigma) =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}$$

Next, let us consider the score of the distribution

$$\begin{aligned} \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma) & =\frac{\nabla_{\boldsymbol{x}} p(\boldsymbol{x} ; \sigma)}{p(\boldsymbol{x} ; \sigma)} \\ & =\frac{\nabla_{\boldsymbol{x}}\left[\frac{1}{Y} \sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\right]}{\left[\frac{1}{Y} \sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\right]} \\ & =\frac{\sum_i \nabla_{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\end{aligned}$$

Then

$$\begin{aligned} \nabla_{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) & =\nabla_{\boldsymbol{x}}\left[\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \exp \frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ & =\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \nabla_{\boldsymbol{x}}\left[\exp \frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ & =\left[\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \exp \frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \nabla_{\boldsymbol{x}}\left[\frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ & =\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \nabla_{\boldsymbol{x}}\left[\frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ & =\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[\frac{\boldsymbol{y}_i-\boldsymbol{x}}{\sigma^2}\right] .\end{aligned}$$

Next

$$\begin{aligned} \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma) & =\frac{\sum_i \nabla_{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)} \\ & =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[\frac{\boldsymbol{y}_i-\boldsymbol{x}}{\sigma^2}\right]}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)} \\ & =\left(\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}-\boldsymbol{x}\right) / \sigma^2 .\end{aligned}$$

So $$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2$$

# Proposed SDE
### Task 1: Find a PDE given initial value $$q(\boldsymbol{x}, 0):= p_{data}(\boldsymbol{x})$$, and solution $$q(\boldsymbol{x}, t) = p(\boldsymbol{x}, \sigma(t))$$

#### The solution is a heat equation

$$\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$$

#### Proof

$$p(\boldsymbol{x} ; \sigma)=p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)$$ and $$p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))$$

the density evolves according to a heat diffusion PDE with time-varying diffusivity. As a ﬁrst step, we ﬁnd this PDE.

The solution can be generated by the heat equation with time-varying diffusivity $$\kappa(t)$$. 

The heat equation PDE:

$$\frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t}=-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)$$

Take Fourier transform along the $$\boldsymbol{x}-\text{dimension}$$

$$\frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t}=-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)$$

Since $$q(\boldsymbol{x}, t)=p(\boldsymbol{x} ; \sigma(t))=p_{\text {data }}(\boldsymbol{x}) * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)$$,
$$\hat{q}(\boldsymbol{\nu}, t)=\hat{p}_{\text {data }}(\boldsymbol{\nu}) \exp \left(-\frac{1}{2}|\boldsymbol{\nu}|^2 \sigma(t)^2\right)$$

Differentiating the target solution along the time axis, we have

$$\begin{aligned} \frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t} & =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{p}_{\text {data }}(\boldsymbol{\nu}) \exp \left(-\frac{1}{2}|\boldsymbol{\nu}|^2 \sigma(t)^2\right) \\ & =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)\end{aligned}$$
Then we have 

$$\begin{aligned}-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t) & =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t) \\ \kappa(t) & =\dot{\sigma}(t) \sigma(t) .\end{aligned}$$

To summzrize

$$\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$$

### Task 2: Seek an SDE whose solution density is described by the PDE $$\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$$

#### Solution: $$\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t$$

#### Proof

Given an SDE,$$\mathrm{d} \boldsymbol{x}=\boldsymbol{f}(\boldsymbol{x}, t) \mathrm{d} t+\boldsymbol{g}(\boldsymbol{x}, t) \mathrm{d} \omega_t$$, The Fokker–Planck PDE describes the time evolution of its solution probability density $$r(\boldsymbol{x},t)$$ as

$$\frac{\partial r(\boldsymbol{x}, t)}{\partial t}=-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))+\frac{1}{2} \nabla_{\boldsymbol{x}} \nabla_{\boldsymbol{x}}:(\mathbf{D}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))$$

where $$\mathbf{D}_{i j}=\sum_k \boldsymbol{g}_{i k} \boldsymbol{g}_{j k}$$ is diffusion tensor. We consider a special case $$\boldsymbol{g}(\boldsymbol{x}, t) = g(t) \boldsymbol{I}$$

$$\frac{\partial r(\boldsymbol{x}, t)}{\partial t}=-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))+\frac{1}{2} g(t)^2 \Delta_{\boldsymbol{x}} r(\boldsymbol{x}, t)$$

We can find a sufﬁcient condition that sastify the PDE $$\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$$

$$-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) q(\boldsymbol{x}, t))+\frac{1}{2} g(t)^2 \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) = \dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$$

The key is given by the identity $$\nabla_{\boldsymbol{x}} \cdot \nabla_{\boldsymbol{x}}=\Delta_{\boldsymbol{x}}$$. We set $$\boldsymbol{f}(\boldsymbol{x}, t) q(\boldsymbol{x}, t)=v(t) \nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)$$, then

$$\begin{aligned} \nabla_{\boldsymbol{x}} \cdot\left(v(t) \nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)\right) & =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) \\ v(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) & =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) \\ v(t) & =\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t) .\end{aligned}$$

$$\boldsymbol{f}(\boldsymbol{x}, t)$$is in fact proportional to the score function:

$$\begin{aligned} \boldsymbol{f}(\boldsymbol{x}, t) & =v(t) \frac{\nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)}{q(\boldsymbol{x}, t)} \\ & =v(t) \nabla_{\boldsymbol{x}} \log q(\boldsymbol{x}, t) \\ & =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log q(\boldsymbol{x}, t)\end{aligned}$$

we recover a family of SDEs whose solution densities have the desired marginals with noise levels $$\sigma(t)$$ for any choice of $$g(t)$$:

$$\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t$$



