---
layout: post
title:  Score-based generative modeling with critically-damped Langevin diffusion
date: 2023-12-07 19:23:00
description: 
tags: Literature-notes
categories: Diffusion-models
tikzjax: true
---


# Ideas

- The diffusion process is the key to improve **synthesis quality** or** sampling speed**.

- Inspired by **statistical mechanics.**

- The score of the conditional distribution $$p_t(\boldsymbol{v}_t \vert \boldsymbol{x}_t)$$ is an arguably easier task than learning the score of $$p_t(\boldsymbol{x}_t)$$.

# Background

- The forward process: $$d \mathbf{u}_t=\boldsymbol{f}\left(\mathbf{u}_t, t\right) d t+\boldsymbol{G}\left(\mathbf{u}_t, t\right) d \mathbf{w}_t, \quad t \in[0, T]$$

- The backward process: $$d \overline{\mathbf{u}}_t=\left[-\boldsymbol{f}\left(\overline{\mathbf{u}}_t, T-t\right)+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) \boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right)^{\top} \nabla_{\overline{\mathbf{u}}_t} \log p_{T-t}\left(\overline{\mathbf{u}}_t\right)\right] d t+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) d \mathbf{w}_t$$

- Currently used SDEs have drift and diffusion coefficients of the symple form: $$\boldsymbol{f}\left(\mathbf{x}_t, t\right)=f(t) \mathbf{x}_t$$ and $$\boldsymbol{G}\left(\mathbf{x}_t, t\right)=g(t) \boldsymbol{I}_d$$

- Generally, setting $$p\left(\mathbf{u}_0\right)=p_{\text {data }}(\mathbf{x})$$, $$p\left(\mathbf{u}_T\right)= \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}_d)$$. 

- If $$\boldsymbol{f}$$ and $$\boldsymbol{G}$$ take the simple form above, the denoising score matching objective is: $$\min _{\boldsymbol{\theta}} \mathbb{E}_{t \sim \mathcal{U}[0, T]} \mathbb{E}_{\mathbf{x}_0 \sim p\left(\mathbf{x}_0\right)} \mathbb{E}_{\mathbf{x}_t \sim p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left\|\mathbf{s}_{\boldsymbol{\theta}}\left(\mathbf{x}_t, t\right)-\nabla_{\mathbf{x}_t} \log p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right\|_2^2\right]$$

- If $$\boldsymbol{f}$$ and $$\boldsymbol{G}$$ are affine, the conditional distribution $$p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)$$ is Normal and avaiable analytically.

# Critically-damped Langevin Dynamic

- We propose to augment the data $$\mathbf{x}_t \in \mathbb{R}^d$$ and $$\mathbf{v}_t \in \mathbb{R}^d$$. With $$\mathbf{u}_t=\left(\mathbf{x}_t, \mathbf{v}_t\right)^{\top} \in \mathbb{R}^{2 d}$$, we set
- $$\boldsymbol{f}\left(\mathbf{u}_t, t\right):=\left(\left(\begin{array}{cc}0 & \beta M^{-1} \\ -\beta & -\Gamma \beta M^{-1}\end{array}\right) \otimes \boldsymbol{I}_d\right) \mathbf{u}_t, \quad \boldsymbol{G}\left(\mathbf{u}_t, t\right):=\left(\begin{array}{cc}0 & 0 \\ 0 & \sqrt{2 \Gamma \beta}\end{array}\right) \otimes \boldsymbol{I}_d$$

- The coupled SDE that describes the diffusion process: $$\left(\begin{array}{l}d \mathbf{x}_t \\ d \mathbf{v}_t\end{array}\right)=\underbrace{\left(\begin{array}{c}M^{-1} \mathbf{v}_t \\ -\mathbf{x}_t\end{array}\right) \beta d t}_{\text {Hamiltonian component }=: H}+\underbrace{\left(\begin{array}{c}\mathbf{0}_d \\ -\Gamma M^{-1} \mathbf{v}_t\end{array}\right) \beta d t+\left(\begin{array}{c}0 \\ \sqrt{2 \Gamma \beta}\end{array}\right) d \mathbf{w}_t}_{\text {Ornstein-Uhlenbeck process=:O }}$$

- The mass $$M \in \mathbb{R}^+$$ is a hyperparameter that determines the coupling between the $$\mathbf{x}_t$$and $$\mathbf{v}_t$$ variables.

-  $$\beta \in \mathbb{R}^+$$ is a constant time rescaling chosen such that the diffusion **converges to its equilibrium distribution** (we found constant $$\beta$$'s to work well).

- $$\Gamma \in \mathbb{R}^+$$ is a friction coefficient that determines **the strength of the noise injection** into the velocities.

- The Hamiltonian component plays a role to **accelerate sampling** and **efﬁciently explore complex probability distributions**.

- The $$O$$ term corresponds to an **Ornstein-Uhlenbeck process** in the velocity, which injects noise such that the diffusion dynamics properly converge to equilibrium for any $$\Gamma > 0$$.

- It can be shown that the equilibrium distribution of this diffusion is $$p_{\mathrm{EQ}}(\mathbf{u})=\mathcal{N}\left(\mathbf{x} ; \mathbf{0}_d, \boldsymbol{I}_d\right) \mathcal{N}\left(\mathbf{v} ; \mathbf{0}_d, M \boldsymbol{I}_d\right)$$

- The balance between $$M$$ and $$\Gamma$$

   - For $$\Gamma^2 < 4M$$(underdamped Langevin dynamics): oscillatory dynamics of $$\mathbf{x}_t$$ and $$\boldsymbol{v}_t$$ that slow down converge to equilibrium.

   - For $$\Gamma^2 > 4M$$(overdamped Langevin dynamics): the $$O$$ term dominates wihci also slows down convergence.

   -  For $$\Gamma^2 = 4M$$(critically-damped Langevin dynamics): an ideal balance is achieved and convergence to $$p_{\mathrm{EQ}}(\mathbf{u})$$ as fast as possible in a smooth manner without oscillations.

# Score Matching Objective

- we initialize the joint $$\bar{p}\left(\mathbf{u}_0\right)=p\left(\mathbf{x}_0\right) p\left(\mathbf{v}_0\right)=p_{\text {data }}\left(\mathbf{x}_0\right) \mathcal{N}\left(\mathbf{v}_0 ; \mathbf{0}_d, \gamma M \boldsymbol{I}_d\right)$$ with hyperparameter $$\gamma < 1$$ 

- let the distribution diffuse towards the tractable equilibrium—or prior—distribution $$p_{\mathrm{EQ}}(\mathbf{u})$$.

- The score matching (SM) objective: $$\min _{\boldsymbol{\theta}} \mathbb{E}_{t \sim \mathcal{U}[0, T]} \mathbb{E}_{\mathbf{u}_t \sim p_t\left(\mathbf{u}_t\right)}\left[\lambda(t)\left\|s_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)-\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t\right)\right\|_2^2\right]$$

- $$\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t\right)=\nabla_{\mathbf{v}_t}\left[\log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)+\log p_t\left(\mathbf{x}_t\right)\right]=\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$$

- Why $$p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$$ is eaiser to learn

   - our velocity distribution is initialized from a simple Normal distribution, such that $$p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$$ is closer to a Normal distribution for all $$t \geq 0$$ than $$p_t\left( \mathbf{x}_t\right)$$ itself.

   - empirically verify the reduced complexity

# Hybrid score matching

- Objective: $$\min _{\boldsymbol{\theta}} \mathbb{E}_{t \in[0, T]} \mathbb{E}_{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}_{\mathbf{u}_t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left\|s_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)-\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)\right\|_2^2\right]$$

# Score Model Parameterization

- $$\mathbf{u}_t=\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}_{2 d}$$, where $$\boldsymbol{\Sigma}_t=\boldsymbol{L}_t \boldsymbol{L}_t^{\top}$$ is the Cholesky decomposition of $$p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)$$'s covariance matrix, $$\boldsymbol{\epsilon}_{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}_{2 d} ; \mathbf{0}_{2 d}, \boldsymbol{I}_{2 d}\right)$$, and $$\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)$$ is $$p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)$$'s mean.
- $$\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)=-\ell_t \boldsymbol{\epsilon}_{d: 2 d}$$

- With $$\boldsymbol{\Sigma}_t=\underbrace{\left(\begin{array}{cc}\Sigma_t^{x x} & \Sigma_t^{x v} \\ \Sigma_t^{x v} & \Sigma_t^{v v}\end{array}\right)}_{\text {"per-dimension" covariance matrix }} \otimes \boldsymbol{I}_d, \quad$$ we have $$\quad \ell_t:=\sqrt{\frac{\Sigma_t^{x x}}{\Sigma_t^{x x} \Sigma_t^{v v}-\left(\Sigma_t^{x v}\right)^2}}$$

- We parameterize $$s_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)=-\ell_t \alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)$$ with $$\alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)=\ell_t^{-1} \mathbf{v}_t / \Sigma_t^{v v}+ \alpha'_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)$$, where $$\Sigma_t^{vv}$$ corresponds to the $$v-v$$ component of the "per-dimension" covariance matrix of the Normal Distribution $$p_t\left(\mathbf{u}_t \mid \mathbf{x}_0=\mathbf{0}_d\right)$$

- $$\operatorname{HSM}(\lambda(t))=\mathbb{E}_{t \sim \mathcal{U}[0, T], \mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right), \mathbf{u}_t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left(\ell_t^{\mathrm{HSM}}\right)^2\left\|\boldsymbol{\epsilon}_{d: 2 d}-\alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)\right\|_2^2\right]$$ 

- Training objective: $$\min _{\boldsymbol{\theta}} \mathbb{E}_{t \sim \mathcal{U}[0, T]} \mathbb{E}_{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}_{\boldsymbol{\epsilon}_{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}_{2 d} ; \mathbf{0}_{2 d}, \boldsymbol{I}_{2 d}\right)}\left[\lambda(t) \ell_t^2\left\|\boldsymbol{\epsilon}_{d: 2 d}-\alpha_{\boldsymbol{\theta}}\left(\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}_{2 d}, t\right)\right\|_2^2\right]$$

