<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://szhan311.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://szhan311.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-15T03:00:01+00:00</updated><id>https://szhan311.github.io//feed.xml</id><title type="html">Shaorong Zhang</title><subtitle></subtitle><entry><title type="html">Action maching</title><link href="https://szhan311.github.io//blog/2023/Action-Maching/" rel="alternate" type="text/html" title="Action maching"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Action%20Maching</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Action-Maching/"><![CDATA[<p>Action matching: a method for learning population dynamics from samples of their temporal marginals The proposed method can learn dynamics which simulate an arbitrary path of marginal distributions, it can also be applied in the context of generative modeling.</p> <h1 id="continuity-equation">Continuity Equation</h1> <p>any <strong>continuous dynamics</strong> can be modeled by the <strong>continuity equation</strong>, and moreover any <strong>continuity equation</strong> results in a <strong>continuous dynamics</strong>.</p> <h2 id="state-space-to-density-space">State space to density space</h2> <ul> <li>Suppose we have a set of particles in space $\mathcal{X} \subset \mathbb{R}^d$, initially distributed as $q_{t=0}$. Let each particle follow a timedependent ODE (continuous flow) with the velocity field $v:[0,1] \times \mathcal{X} \rightarrow \mathbb{R}^d$ as follows <ul> <li>$\frac{d}{d t} x(t)=v_t(x(t)), \quad x(t=0)=x$</li> <li>The continuity equation describes how the density of the particles $q_t$ evolves in time $t$, i.e.,</li> <li>$\frac{\partial}{\partial t} q_t=-\nabla \cdot\left(q_t v_t\right)$ <h2 id="density-space-to-state-space">Density space to state space</h2> </li> </ul> </li> <li><strong>Theorem 2.1</strong>. Consider a continuous dynamic with the density evolution of $q_t$, which satisfies mild conditions (absolute continuty in the 2-Wassertein space of distributions $\mathcal{P}_2(\mathcal{X})$. Then there exists a unique (up to a constant) function $s_t^<em>(x)$, called the “action”, such that vector field $v_t^</em>(x) = \nabla s_t^<em>(x)$ and $q_t$ satisfies the continuity equation $\frac{\partial}{\partial t} q_t=-\nabla \cdot\left(q_t \nabla s_t^</em>(x)\right)$.</li> <li>In other words, the ODE $\frac{d}{d t} x(t)=\nabla s_t^*(x)$ can be used to move samples in time such that the marginals are $q_t$.</li> <li>Using Theorem 2.1, the problem of learning the dynamics can be boiled down to learning the unique vector field $\nabla s_t^*$</li> <li>Based on Theorem 2.1, given density dynamic, we can learn a function $s_t(x, \theta)$ to approximate $s_t^*$. Then Given $s_t(x, \theta)$, we can calculate the dynamic in the state space.</li> </ul> <h1 id="action-matching">Action matching</h1> <h2 id="objective">Objective</h2> <p>Recover the true action $s_t^<em>$ while having access only to samples from $q_t$. The objective is to minimize the “ACTION-GAP” objective $\operatorname{ACTION-GAP}\left(s, s^</em>\right):=\frac{1}{2} \int_0^1 \mathbb{E}_{q_t(x)}\left|\nabla s_t(x)-\nabla s_t^*(x)\right|^2 d t$</p> <h2 id="tractable-objective">Tractable objective</h2> <p>Theorem 2.2. For an arbitrary variational action $s$, the ACTION-GAP $\left(s, s^<em>\right)$ can be decomposed as the sum of an intractable constant $\mathcal{K}$, and a tractable term $\mathcal{L}_{A M}(s)$ $\operatorname{ACTION-GAP}\left(s_t, s_t^</em>\right)=\mathcal{K}+\mathcal{L}<em>{\mathrm{AM}}\left(s_t\right) .$ where $\mathcal{L}</em>{\mathrm{AM}}(s)$ is the Action Matching objective, which we minimize $\begin{aligned} \mathcal{L}<em>{\mathrm{AM}}(s) &amp; :=\mathbb{E}</em>{q_0(x)}\left[s_0(x)\right]-\mathbb{E}<em>{q_1(x)}\left[s_1(x)\right] <br/> &amp; +\int_0^1 \mathbb{E}</em>{q_t(x)}\left[\frac{1}{2}\left|\nabla s_t(x)\right|^2+\frac{\partial s_t}{\partial t}(x)\right] d t \end{aligned}$ The term $\mathcal{L}_{\mathrm{AM}}$ is tractable</p> <h2 id="connection-with-optimal-transport">Connection with Optimal Transport</h2> <p>The optimal dynamics of AM along the curve is also optimal in the sense of optimal transport with the 2-Wasserstein cost. the optimal vector field in the AM objective defines a mapping between two infinitesimally close distributions $q_t$ and $q_{t+h}$, which is of the form $x \mapsto x+h \nabla s_t^*(x)$. This mapping is indeed the same as the Brenier map in optimal transport, which is of the form $x \mapsto x+h \nabla \varphi_t(x)$, where $\varphi_t$ is the (c-convex) Kantorovish potential</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Action matching: a method for learning population dynamics from samples of their temporal marginals The proposed method can learn dynamics which simulate an arbitrary path of marginal distributions, it can also be applied in the context of generative modeling.]]></summary></entry><entry><title type="html">Bridge</title><link href="https://szhan311.github.io//blog/2023/Bridge/" rel="alternate" type="text/html" title="Bridge"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Bridge</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Bridge/"><![CDATA[<h1 id="prelimineries">Prelimineries</h1> <p>$\mathbf{x}<em>0 \sim p_0(\mathbf{x}):=q</em>{\text {data }}(\mathbf{x})$ and $\mathbf{x}<em>T \sim p_T(\mathbf{x}):=p</em>{\text {prior }}(\mathbf{x})$ Forward SDE: $d \mathbf{x}<em>t=\mathbf{f}\left(\mathbf{x}_t, t\right) d t+g(t) d \mathbf{w}_t$ Reverse SDE: $\left.d \mathbf{x}_t=\mathbf{f}\left(\mathbf{x}_t, t\right)-g(t)^2 \nabla</em>{\mathbf{x}<em>t} \log p\left(\mathbf{x}_t\right)\right) d t+g(t) d \mathbf{w}_t$ Reverse ODE: $d \mathbf{x}_t=\left[\mathbf{f}\left(\mathbf{x}_t, t\right)-\frac{1}{g} g(t)^2 \nabla</em>{\mathbf{x}<em>t} \log p\left(\mathbf{x}_t\right)\right] d t$ The denoising score matching: $\mathcal{L}(\theta)=\mathbb{E}</em>{\mathbf{x}<em>t \sim p\left(\mathbf{x}_t \mid \mathbf{x}_0\right), \mathbf{x}_0 \sim q</em>{\text {data }}(\mathbf{x}), t \sim \mathcal{U}(0, T)}\left[\left|\mathbf{s}<em>\theta\left(\mathbf{x}_t, t\right)-\nabla</em>{\mathbf{x}_t} \log p\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right|^2\right]$</p> <h1 id="diffusion-process-with-fixed-endpoints">Diffusion Process with Fixed Endpoints</h1> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1699414745464-756ca5dd-db55-42e6-b77a-99ac99ead44b.png#averageHue=%23e7e3de&amp;clientId=u22a16a20-57e3-4&amp;from=paste&amp;height=291&amp;id=uad45a1bb&amp;originHeight=460&amp;originWidth=1152&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=456796&amp;status=done&amp;style=none&amp;taskId=u766a4e25-9162-4aae-8ed2-ab3c4a610c9&amp;title=&amp;width=729" alt="image.png"/></p> <ul> <li>a diffusion process $d \mathbf{x}<em>t=\mathbf{f}\left(\mathbf{x}_t, t\right) d t+g(t) d \mathbf{w}_t$ can be driven to arrive at a particular point of intetest $y \in \mathbb{R}^d$ almost surely via Doob’s $h$-transform: $d \mathbf{x}_t= [\mathbf{f}\left(\mathbf{x}_t, t\right) +g(t)^2 \mathbf{h}\left(\mathbf{x}_t, t, y, T\right)]dt+g(t) d \mathbf{w}_t, \quad \mathbf{x}_0 \sim q</em>{\text {data }}(\mathbf{x}), \quad \mathbf{x}_T=y$</li> <li> <table> <tbody> <tr> <td>where $\mathbf{h}(x, t, y, T)=\left.\nabla_{\mathbf{x}_t} \log p\left(\mathbf{x}_T \mid \mathbf{x}_t\right)\right</td> <td>_{\mathbf{x}_t=x, \mathbf{x}_T=y}$ is the gradient if the log transition kernel of from $t$ to $T$ generated by the original SDE, evaluated at points $\mathbf{x}_t = \mathbf{x}$ and $\mathbf{x}_T = y$, and each $\mathbf{x}_t$ now explicitly depends on $y$ at time $T$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>**Theorem 1. **The evolution of conditional probability $q(\mathbf{x}_t</td> <td>\mathbf{x}_T)$ has a time-reversed SDE of the form$d \mathbf{x}_t=\left[\mathbf{f}\left(\mathbf{x}_t, t\right)-g^2(t)\left(\mathbf{s}\left(\mathbf{x}_t, t, y, T\right)-\mathbf{h}\left(\mathbf{x}_t, t, y, T\right)\right)\right] d t+g(t) d \hat{\mathbf{w}}_t, \quad \mathbf{x}_T=y$</td> </tr> </tbody> </table> </li> <li>with an associated probability flow ODE $d \mathbf{x}_t=\left[\mathbf{f}\left(\mathbf{x}_t, t\right)-g^2(t)\left(\frac{1}{2} \mathbf{s}\left(\mathbf{x}_t, t, y, T\right)-\mathbf{h}\left(\mathbf{x}_t, t, y, T\right)\right)\right] d t, \quad \mathbf{x}_T=y$</li> <li><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1699414789423-80fe4fb2-d4b2-4480-b81c-89dd37d211db.png#averageHue=%23e9e9e9&amp;clientId=u22a16a20-57e3-4&amp;from=paste&amp;height=145&amp;id=u06bdc0aa&amp;originHeight=248&amp;originWidth=1243&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=65385&amp;status=done&amp;style=none&amp;taskId=u9ceb338f-e97b-4912-b911-e93f3f129e3&amp;title=&amp;width=728.5" alt="image.png"/></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Prelimineries $\mathbf{x}0 \sim p_0(\mathbf{x}):=q{\text {data }}(\mathbf{x})$ and $\mathbf{x}T \sim p_T(\mathbf{x}):=p{\text {prior }}(\mathbf{x})$ Forward SDE: $d \mathbf{x}t=\mathbf{f}\left(\mathbf{x}_t, t\right) d t+g(t) d \mathbf{w}_t$ Reverse SDE: $\left.d \mathbf{x}_t=\mathbf{f}\left(\mathbf{x}_t, t\right)-g(t)^2 \nabla{\mathbf{x}t} \log p\left(\mathbf{x}_t\right)\right) d t+g(t) d \mathbf{w}_t$ Reverse ODE: $d \mathbf{x}_t=\left[\mathbf{f}\left(\mathbf{x}_t, t\right)-\frac{1}{g} g(t)^2 \nabla{\mathbf{x}t} \log p\left(\mathbf{x}_t\right)\right] d t$ The denoising score matching: $\mathcal{L}(\theta)=\mathbb{E}{\mathbf{x}t \sim p\left(\mathbf{x}_t \mid \mathbf{x}_0\right), \mathbf{x}_0 \sim q{\text {data }}(\mathbf{x}), t \sim \mathcal{U}(0, T)}\left[\left|\mathbf{s}\theta\left(\mathbf{x}_t, t\right)-\nabla{\mathbf{x}_t} \log p\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right|^2\right]$]]></summary></entry><entry><title type="html">Cld</title><link href="https://szhan311.github.io//blog/2023/CLD/" rel="alternate" type="text/html" title="Cld"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/CLD</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/CLD/"><![CDATA[<p>Score-based generative modeling with critically-damped Langevin diffusion</p> <h1 id="ideas">Ideas</h1> <ul> <li>The diffusion process is the key to improve <strong>synthesis quality</strong> or** sampling speed**.</li> <li>Inspired by <strong>statistical mechanics.</strong></li> <li> <table> <tbody> <tr> <td>The score of the conditional distribution $p_t(\boldsymbol{v}_t</td> <td>\boldsymbol{x}_t)$ is an arguably easier task than learning the score of $p_t(\boldsymbol{x}_t)$.</td> </tr> </tbody> </table> </li> </ul> <h1 id="background">Background</h1> <ul> <li>The forward process: $d \mathbf{u}_t=\boldsymbol{f}\left(\mathbf{u}_t, t\right) d t+\boldsymbol{G}\left(\mathbf{u}_t, t\right) d \mathbf{w}_t, \quad t \in[0, T]$</li> <li>The backward process: $d \overline{\mathbf{u}}<em>t=\left[-\boldsymbol{f}\left(\overline{\mathbf{u}}_t, T-t\right)+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) \boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right)^{\top} \nabla</em>{\overline{\mathbf{u}}<em>t} \log p</em>{T-t}\left(\overline{\mathbf{u}}_t\right)\right] d t+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) d \mathbf{w}_t$</li> <li>Currently used SDEs have drift and diffusion coefficients of the symple form: $\boldsymbol{f}\left(\mathbf{x}_t, t\right)=f(t) \mathbf{x}_t$ and $\boldsymbol{G}\left(\mathbf{x}_t, t\right)=g(t) \boldsymbol{I}_d$</li> <li>Generally, setting $p\left(\mathbf{u}<em>0\right)=p</em>{\text {data }}(\mathbf{x})$, $p\left(\mathbf{u}_T\right)= \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}_d)$.</li> <li>If $\boldsymbol{f}$ and $\boldsymbol{G}$ take the simple form above, the denoising score matching objective is: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \sim \mathcal{U}[0, T]} \mathbb{E}<em>{\mathbf{x}_0 \sim p\left(\mathbf{x}_0\right)} \mathbb{E}</em>{\mathbf{x}<em>t \sim p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left|\mathbf{s}</em>{\boldsymbol{\theta}}\left(\mathbf{x}<em>t, t\right)-\nabla</em>{\mathbf{x}_t} \log p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right|_2^2\right]$</li> <li>If $\boldsymbol{f}$ and $\boldsymbol{G}$ are affine, the conditional distribution $p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)$ is Normal and avaiable analytically.</li> </ul> <h1 id="critically-damped-langevin-dynamic">Critically-damped Langevin Dynamic</h1> <ul> <li>We propose to augment the data $\mathbf{x}_t \in \mathbb{R}^d$ and $\mathbf{v}_t \in \mathbb{R}^d$. With $\mathbf{u}_t=\left(\mathbf{x}_t, \mathbf{v}_t\right)^{\top} \in \mathbb{R}^{2 d}$, we set</li> <li>$\boldsymbol{f}\left(\mathbf{u}_t, t\right):=\left(\left(\begin{array}{cc}0 &amp; \beta M^{-1} \ -\beta &amp; -\Gamma \beta M^{-1}\end{array}\right) \otimes \boldsymbol{I}_d\right) \mathbf{u}_t, \quad \boldsymbol{G}\left(\mathbf{u}_t, t\right):=\left(\begin{array}{cc}0 &amp; 0 \ 0 &amp; \sqrt{2 \Gamma \beta}\end{array}\right) \otimes \boldsymbol{I}_d$</li> <li>The coupled SDE that describes the diffusion process: $\left(\begin{array}{l}d \mathbf{x}<em>t \ d \mathbf{v}_t\end{array}\right)=\underbrace{\left(\begin{array}{c}M^{-1} \mathbf{v}_t \ -\mathbf{x}_t\end{array}\right) \beta d t}</em>{\text {Hamiltonian component }=: H}+\underbrace{\left(\begin{array}{c}\mathbf{0}<em>d \ -\Gamma M^{-1} \mathbf{v}_t\end{array}\right) \beta d t+\left(\begin{array}{c}0 \ \sqrt{2 \Gamma \beta}\end{array}\right) d \mathbf{w}_t}</em>{\text {Ornstein-Uhlenbeck process=:O }}$</li> <li>The mass $M \in \mathbb{R}^+$ is a hyperparameter that determines the coupling between the $\mathbf{x}_t$and $\mathbf{v}_t$ variables.</li> <li>$\beta \in \mathbb{R}^+$ is a constant time rescaling chosen such that the diffusion <strong>converges to its equilibrium distribution</strong> (we found constant $\beta$’s to work well).</li> <li>$\Gamma \in \mathbb{R}^+$ is a friction coefficient that determines <strong>the strength of the noise injection</strong> into the velocities.</li> <li>The Hamiltonian component plays a role to <strong>accelerate sampling</strong> and <strong>efﬁciently explore complex probability distributions</strong>.</li> <li>The $O$ term corresponds to an <strong>Ornstein-Uhlenbeck process</strong> in the velocity, which injects noise such that the diffusion dynamics properly converge to equilibrium for any $\Gamma &gt; 0$.</li> <li>It can be shown that the equilibrium distribution of this diffusion is $p_{\mathrm{EQ}}(\mathbf{u})=\mathcal{N}\left(\mathbf{x} ; \mathbf{0}_d, \boldsymbol{I}_d\right) \mathcal{N}\left(\mathbf{v} ; \mathbf{0}_d, M \boldsymbol{I}_d\right)$</li> <li>The balance between $M$ and $\Gamma$ <ul> <li>For $\Gamma^2 &lt; 4M$(underdamped Langevin dynamics): oscillatory dynamics of $\mathbf{x}_t$ and $\boldsymbol{v}_t$ that slow down converge to equilibrium.</li> <li>For $\Gamma^2 &gt; 4M$(overdamped Langevin dynamics): the $O$ term dominates wihci also slows down convergence.</li> <li>For $\Gamma^2 = 4M$(critically-damped Langevin dynamics): an ideal balance is achieved and convergence to $p_{\mathrm{EQ}}(\mathbf{u})$ as fast as possible in a smooth manner without oscillations.</li> </ul> </li> </ul> <h1 id="score-matching-objective">Score Matching Objective</h1> <ul> <li>we initialize the joint $\bar{p}\left(\mathbf{u}<em>0\right)=p\left(\mathbf{x}_0\right) p\left(\mathbf{v}_0\right)=p</em>{\text {data }}\left(\mathbf{x}_0\right) \mathcal{N}\left(\mathbf{v}_0 ; \mathbf{0}_d, \gamma M \boldsymbol{I}_d\right)$ with hyperparameter $\gamma &lt; 1$</li> <li>let the distribution diffuse towards the tractable equilibrium—or prior—distribution $p_{\mathrm{EQ}}(\mathbf{u})$.</li> <li>The score matching (SM) objective: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \sim \mathcal{U}[0, T]} \mathbb{E}<em>{\mathbf{u}_t \sim p_t\left(\mathbf{u}_t\right)}\left[\lambda(t)\left|s</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)-\nabla</em>{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t\right)\right|_2^2\right]$</li> <li>$\nabla_{\mathbf{v}<em>t} \log p_t\left(\mathbf{u}_t\right)=\nabla</em>{\mathbf{v}<em>t}\left[\log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)+\log p_t\left(\mathbf{x}_t\right)\right]=\nabla</em>{\mathbf{v}_t} \log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$</li> <li>Why $p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$ is eaiser to learn <ul> <li>our velocity distribution is initialized from a simple Normal distribution, such that $p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)$ is closer to a Normal distribution for all $t \geq 0$ than $p_t\left( \mathbf{x}_t\right)$ itself.</li> <li>empirically verify the reduced complexity <h1 id="hybrid-score-matching">Hybrid score matching</h1> </li> </ul> </li> <li>Objective: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \in[0, T]} \mathbb{E}<em>{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}</em>{\mathbf{u}<em>t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left|s</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)-\nabla</em>{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)\right|_2^2\right]$ <h1 id="score-model-parameterization">Score Model Parameterization</h1> </li> <li>$\mathbf{u}<em>t=\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}</em>{2 d}$, where $\boldsymbol{\Sigma}<em>t=\boldsymbol{L}_t \boldsymbol{L}_t^{\top}$ is the Cholesky decomposition of $p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)$’s covariance matrix, $\boldsymbol{\epsilon}</em>{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}<em>{2 d} ; \mathbf{0}</em>{2 d}, \boldsymbol{I}_{2 d}\right)$, and $\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)$ is $p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)$’s mean.</li> <li>$\nabla_{\mathbf{v}<em>t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)=-\ell_t \boldsymbol{\epsilon}</em>{d: 2 d}$</li> <li>With $\boldsymbol{\Sigma}<em>t=\underbrace{\left(\begin{array}{cc}\Sigma_t^{x x} &amp; \Sigma_t^{x v} \ \Sigma_t^{x v} &amp; \Sigma_t^{v v}\end{array}\right)}</em>{\text {“per-dimension” covariance matrix }} \otimes \boldsymbol{I}_d, \quad$ we have $\quad \ell_t:=\sqrt{\frac{\Sigma_t^{x x}}{\Sigma_t^{x x} \Sigma_t^{v v}-\left(\Sigma_t^{x v}\right)^2}}$</li> <li>We parameterize $s_{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)=-\ell_t \alpha</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)$ with $\alpha</em>{\boldsymbol{\theta}}\left(\mathbf{u}<em>t, t\right)=\ell_t^{-1} \mathbf{v}_t / \Sigma_t^{v v}+ \alpha’</em>{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)$, where $\Sigma_t^{vv}$ corresponds to the $v-v$ component of the “per-dimension” covariance matrix of the Normal Distribution $p_t\left(\mathbf{u}_t \mid \mathbf{x}_0=\mathbf{0}_d\right)$</li> <li>$\operatorname{HSM}(\lambda(t))=\mathbb{E}<em>{t \sim \mathcal{U}[0, T], \mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right), \mathbf{u}_t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left(\ell_t^{\mathrm{HSM}}\right)^2\left|\boldsymbol{\epsilon}</em>{d: 2 d}-\alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)\right|_2^2\right]$</li> <li>Training objective: $\min <em>{\boldsymbol{\theta}} \mathbb{E}</em>{t \sim \mathcal{U}[0, T]} \mathbb{E}<em>{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}</em>{\boldsymbol{\epsilon}<em>{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}</em>{2 d} ; \mathbf{0}<em>{2 d}, \boldsymbol{I}</em>{2 d}\right)}\left[\lambda(t) \ell_t^2\left|\boldsymbol{\epsilon}<em>{d: 2 d}-\alpha</em>{\boldsymbol{\theta}}\left(\boldsymbol{\mu}<em>t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}</em>{2 d}, t\right)\right|_2^2\right]$</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Score-based generative modeling with critically-damped Langevin diffusion Ideas]]></summary></entry><entry><title type="html">Consistency model</title><link href="https://szhan311.github.io//blog/2023/Consistency-model/" rel="alternate" type="text/html" title="Consistency model"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Consistency%20model</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Consistency-model/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Consistency model can be trained in two ways: <strong>distilling pre-trained diffusion models</strong>, <strong>as standalone generative models</strong></p> <h1 id="preliminaries">Preliminaries</h1> <h2 id="diffusion-models">Diffusion models</h2> <p>Let $p_{\text{data}}(\mathbf{x})$ denote the data distribution. Diffusion models start by diffusing $p_{\text{data}}(x)$ with a stochastic differential equation (SDE) $\mathrm{d} \mathbf{x}<em>t=\boldsymbol{\mu}\left(\mathbf{x}_t, t\right) \mathrm{d} t+\sigma(t) \mathrm{d} \mathbf{w}_t$ where $t \in [0, T], T &gt; 0$ is a fixed constant, $\mu (\cdot, \cdot)$ and $\sigma(\cdot)$ are the drift and diffusion coefficients respectively, and ${ \boldsymbol{\omega}_t}</em>{t \in [0, T]}$ denotes the standard Brownian motion. We denote the distribution of $\mathbf{x}<em>t$ as $p</em>{t}(x)$ and as a result $p_0(\mathbf{x}) \equiv p_{\text {data }}(\mathbf{x})$ <strong>the Probability Flow (PF) ODE</strong>: $\mathrm{d} \mathbf{x}<em>t=\left[\boldsymbol{\mu}\left(\mathbf{x}_t, t\right)-\frac{1}{2} \sigma(t)^2 \nabla \log p_t\left(\mathbf{x}_t\right)\right] \mathrm{d} t$ We adopt the setting: $\boldsymbol{\mu}(\mathbf{x}, t)=\mathbf{0}$ and $\sigma(t)=\sqrt{2 t}$, in this case, $p_t(\mathbf{x})=p</em>{\text {data }}(\mathbf{x}) \otimes \mathcal{N}\left(\mathbf{0}, t^2 \boldsymbol{I}\right)$, where $\otimes$ denotes the convolution operation, and $\pi(\mathbf{x})=\mathcal{N}\left(\mathbf{0}, T^2 \boldsymbol{I}\right)$ For sampling, we first train a score model $\boldsymbol{s}_\phi(\mathbf{x}, t) \approx \nabla \log p_t(\mathbf{x})$ via score matching, then we have the <strong>empirical PF ODE: **$\frac{\mathrm{d} \mathbf{x}<em>t}{\mathrm{~d} t}=-t s</em>\phi\left(\mathbf{x}_t, t\right)$ To solve this ODE, there are two main ways: **numerical ODE solver **and **distillation techniques</strong></p> <h2 id="edm">EDM</h2> <h3 id="denoiser">Denoiser</h3> <h4 id="equation-2-mathbbeboldsymboly-sim-ptext-data--mathbbe_boldsymboln-sim-mathcalnleftmathbf0-sigma2-mathbfirightdboldsymbolyboldsymboln--sigma-boldsymboly_22">Equation 2: $\mathbb{E}<em>{\boldsymbol{y} \sim p</em>{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}|_2^2$</h4> <p>The optimal analytically solution is $D(\boldsymbol{x} ; \sigma) =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}$ <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1701066290736-f039695c-0949-4d4d-aeac-fa80cd5ce23e.png#averageHue=%23dcdad8&amp;clientId=ua25b137d-f497-4&amp;from=paste&amp;height=231&amp;id=ue6a06ef2&amp;originHeight=324&amp;originWidth=1161&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=340528&amp;status=done&amp;style=none&amp;taskId=u953377f2-543e-4db8-9aa4-b0dca7fbfd6&amp;title=&amp;width=829.5" alt="image.png"/> Based on Fig. 1, it seems that we cannot get clean image from corrupted image in 1 step.</p> <h3 id="preconditioning-and-training">Preconditioning and training</h3> <p>In practice, instead of parameterize $D_{\theta}$ directly, we train a different network $F_{\theta}$ from which $D_{\theta}$ is derived $D_\theta(\boldsymbol{x} ; \sigma)=c_{\text {skip }}(\sigma) \boldsymbol{x}+c_{\text {out }}(\sigma) F_\theta\left(c_{\text {in }}(\sigma) \boldsymbol{x} ; c_{\text {noise }}(\sigma)\right)$ The loss is: $\mathbb{E}<em>{\sigma, \boldsymbol{y}, \boldsymbol{n}}[\underbrace{\lambda(\sigma) c</em>{\text {out }}(\sigma)^2}<em>{\text {effective weight }}|\underbrace{F</em>\theta\left(c_{\text {in }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n}) ; c_{\text {noise }}(\sigma)\right)}<em>{\text {network output }}-\underbrace{\frac{1}{c</em>{\text {out }}(\sigma)}\left(\boldsymbol{y}-c_{\text {skip }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n})\right)}_{\text {effective training target }}|_2^2]$</p> <h1 id="consistency-model">Consistency Model</h1> <h2 id="what-is-consistency-model">What is consistency model</h2> <p>Definition Given a solution trajectory $\left{\mathbf{x}<em>t\right}</em>{t \in[\epsilon, T]}$ of the PF ODE$\mathrm{d} \mathbf{x}<em>t=\left[\boldsymbol{\mu}\left(\mathbf{x}_t, t\right)-\frac{1}{2} \sigma(t)^2 \nabla \log p_t\left(\mathbf{x}_t\right)\right] \mathrm{d} t$, we define the consistency function as $\boldsymbol{f}:\left(\mathbf{x}_t, t\right) \mapsto \mathbf{x}</em>\epsilon$. A consistency function has the property of self-consistency: it outputs are consistent for arbitrary pairs of $(\mathbf{x}<em>t, t)$ that belong to the same PF ODE trajectory, i.e., $\boldsymbol{f}\left(\mathbf{x}_t, t\right)=\boldsymbol{f}\left(\mathbf{x}</em>{t^{\prime}}, t^{\prime}\right)$ for all $t, t^{\prime} \in[\epsilon, T]$.</p> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1701109739051-d2d30d7b-6403-4661-bd02-246c22cd073e.png#averageHue=%23dfdcdb&amp;clientId=u546aa620-17e3-4&amp;from=paste&amp;height=461&amp;id=u26123346&amp;originHeight=461&amp;originWidth=677&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=251053&amp;status=done&amp;style=none&amp;taskId=u897eceda-657e-4552-b0a6-634403c1b40&amp;title=&amp;width=677.5" alt="image.png"/> <strong>self-consistency</strong>: points on the same trajectory map to the same initial point.</p> <h2 id="parameterization">parameterization</h2> <p>parameterize the consistency model using skip connections: $\boldsymbol{f}<em>{\boldsymbol{\theta}}(\mathbf{x}, t)=c</em>{\text {skip }}(t) \mathbf{x}+c_{\text {out }}(t) F_{\boldsymbol{\theta}}(\mathbf{x}, t)$ where $c_{\text{skip}}(t)$ and $c_{\text{out}}(t)$ are differeitiable functions such that $c_{\text{skip}}(\epsilon) = 1$ and $c_{\text{out}}(\epsilon) = 0$</p> <h2 id="the-consistency-ditillation-loss">The consistency ditillation loss</h2> <p>$\begin{aligned} \mathcal{L}<em>{C D}^N\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{-} ; \boldsymbol{\phi}\right) &amp; := \ &amp; \mathbb{E}\left[\lambda\left(t_n\right) d\left(\boldsymbol{f}</em>{\boldsymbol{\theta}}\left(\mathbf{x}<em>{t</em>{n+1}}, t_{n+1}\right), \boldsymbol{f}<em>{\boldsymbol{\theta}^{-}}\left(\hat{\mathbf{x}}</em>{t_n}^{\boldsymbol{\phi}}, t_n\right)\right)\right]\end{aligned}$</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction Consistency model can be trained in two ways: distilling pre-trained diffusion models, as standalone generative models Preliminaries Diffusion models Let $p_{\text{data}}(\mathbf{x})$ denote the data distribution. Diffusion models start by diffusing $p_{\text{data}}(x)$ with a stochastic differential equation (SDE) $\mathrm{d} \mathbf{x}t=\boldsymbol{\mu}\left(\mathbf{x}_t, t\right) \mathrm{d} t+\sigma(t) \mathrm{d} \mathbf{w}_t$ where $t \in [0, T], T &gt; 0$ is a fixed constant, $\mu (\cdot, \cdot)$ and $\sigma(\cdot)$ are the drift and diffusion coefficients respectively, and ${ \boldsymbol{\omega}_t}{t \in [0, T]}$ denotes the standard Brownian motion. We denote the distribution of $\mathbf{x}t$ as $p{t}(x)$ and as a result $p_0(\mathbf{x}) \equiv p_{\text {data }}(\mathbf{x})$ the Probability Flow (PF) ODE: $\mathrm{d} \mathbf{x}t=\left[\boldsymbol{\mu}\left(\mathbf{x}_t, t\right)-\frac{1}{2} \sigma(t)^2 \nabla \log p_t\left(\mathbf{x}_t\right)\right] \mathrm{d} t$ We adopt the setting: $\boldsymbol{\mu}(\mathbf{x}, t)=\mathbf{0}$ and $\sigma(t)=\sqrt{2 t}$, in this case, $p_t(\mathbf{x})=p{\text {data }}(\mathbf{x}) \otimes \mathcal{N}\left(\mathbf{0}, t^2 \boldsymbol{I}\right)$, where $\otimes$ denotes the convolution operation, and $\pi(\mathbf{x})=\mathcal{N}\left(\mathbf{0}, T^2 \boldsymbol{I}\right)$ For sampling, we first train a score model $\boldsymbol{s}_\phi(\mathbf{x}, t) \approx \nabla \log p_t(\mathbf{x})$ via score matching, then we have the empirical PF ODE: **$\frac{\mathrm{d} \mathbf{x}t}{\mathrm{~d} t}=-t s\phi\left(\mathbf{x}_t, t\right)$ To solve this ODE, there are two main ways: **numerical ODE solver **and **distillation techniques EDM Denoiser Equation 2: $\mathbb{E}{\boldsymbol{y} \sim p{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}|_2^2$ The optimal analytically solution is $D(\boldsymbol{x} ; \sigma) =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}$ Based on Fig. 1, it seems that we cannot get clean image from corrupted image in 1 step. Preconditioning and training In practice, instead of parameterize $D_{\theta}$ directly, we train a different network $F_{\theta}$ from which $D_{\theta}$ is derived $D_\theta(\boldsymbol{x} ; \sigma)=c_{\text {skip }}(\sigma) \boldsymbol{x}+c_{\text {out }}(\sigma) F_\theta\left(c_{\text {in }}(\sigma) \boldsymbol{x} ; c_{\text {noise }}(\sigma)\right)$ The loss is: $\mathbb{E}{\sigma, \boldsymbol{y}, \boldsymbol{n}}[\underbrace{\lambda(\sigma) c{\text {out }}(\sigma)^2}{\text {effective weight }}|\underbrace{F\theta\left(c_{\text {in }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n}) ; c_{\text {noise }}(\sigma)\right)}{\text {network output }}-\underbrace{\frac{1}{c{\text {out }}(\sigma)}\left(\boldsymbol{y}-c_{\text {skip }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n})\right)}_{\text {effective training target }}|_2^2]$]]></summary></entry><entry><title type="html">Ddpm</title><link href="https://szhan311.github.io//blog/2023/DDPM/" rel="alternate" type="text/html" title="Ddpm"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/DDPM</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/DDPM/"><![CDATA[<h1 id="background">Background</h1> <h3 id="vlb-bound">VLB Bound</h3> <p>$\begin{align} &amp; \mathbb{E}<em>{q(\mathbf{x}_0)}[-\log p</em>{\theta}(\mathbf{x}<em>0)] \ &amp;= -\mathbb{E}</em>{q(\mathbf{x}<em>0)}[\log (p</em>{\theta}(\mathbf{x}<em>0) \int p</em>{\theta} (\mathbf{x}<em>{1:T})d\mathbf{x}</em>{1:T})] <br/> &amp;= -\mathbb{E}<em>{q(\mathbf{x}_0)}[ \log(\int p</em>{\theta} (\mathbf{x}<em>{0:T})d\mathbf{x}</em>{1:T})] <br/> &amp;= -\mathbb{E}<em>{q(\mathbf{x}_0)}[\log (\int q (\mathbf{x}</em>{1:T}|\mathbf{x}<em>0) \frac{ p</em>{\theta} (\mathbf{x}<em>{0:T}) }{q(\mathbf{x}</em>{1:T}|\mathbf{x}<em>0) }d\mathbf{x}</em>{1:T}] <br/> &amp;= -\mathbb{E}<em>{q(\mathbf{x}_0)}[\log (E</em>{ q (\mathbf{x}<em>{1:T}|\mathbf{x}_0)} \frac{ p</em>{\theta} (\mathbf{x}<em>{0:T}) }{q(\mathbf{x}</em>{1:T}|\mathbf{x}<em>0) })] <br/> &amp;\leq -\mathbb{E}</em>{q(\mathbf{x}<em>{0:T})}[\log \frac{ p</em>{\theta} (\mathbf{x}<em>{0:T}) }{q(\mathbf{x}</em>{1:T}|\mathbf{x}<em>0) })] = L</em>{VLB} \end{align}$ $\begin{align} L_{VLB} &amp;= \mathbb{E_q}[-\log \frac{p_{\theta}(\mathbf{x}<em>{0:T})}{q(\mathbf{x}</em>{1:T}|\mathbf{x}<em>0)})] \ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum</em>{t \geq 1}\log \frac{p_{\theta}(\mathbf{x}<em>{t-1}|\mathbf{x}_t)}{q(\mathbf{x}</em>{t}|\mathbf{x}<em>{t-1})})] \ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum</em>{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}<em>{t-1}|\mathbf{x}_t)}{q(\mathbf{x}</em>{t}|\mathbf{x}<em>{t-1})}) - \log \frac{p</em>{\theta}(\mathbf{x}<em>{0}|\mathbf{x}_1)}{q(\mathbf{x}</em>{1}|\mathbf{x}<em>{0})}] \ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum</em>{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}<em>{t-1}|\mathbf{x}_t)}{q(\mathbf{x}</em>{t-1}|\mathbf{x}<em>{t}, \mathbf{x}_0)}\frac{q(\mathbf{x}</em>{t-1}|\mathbf{x}<em>0)}{q(\mathbf{x}_t|\mathbf{x}_0)}) - \log \frac{p</em>{\theta}(\mathbf{x}<em>{0}|\mathbf{x}_1)}{q(\mathbf{x}</em>{1}|\mathbf{x}<em>{0})}] \ &amp;= \mathbb{E_q}[-\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_0)}- \sum</em>{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}<em>{t-1}|\mathbf{x}_t)}{q(\mathbf{x}</em>{t-1}|\mathbf{x}<em>{t}, \mathbf{x}_0)} - \log p</em>{\theta}(\mathbf{x}<em>{0}|\mathbf{x}_1)] \ &amp;= \mathbb{E_q}[ D</em>{\text{KL}}(q(\mathbf{x}<em>T|\mathbf{x}_0)||p</em>(\mathbf{x}<em>T)) + \sum</em>{t&gt;1} D_{\text{KL}}(q(\mathbf{x}<em>{t-1}|\mathbf{x}_t, \mathbf{x}_0)||p</em>{\theta}(\mathbf{x}<em>{t-1}|\mathbf{x}_t))-\log p</em>{\theta}(\mathbf{x}_{0}|\mathbf{x}_1)] \end{align}$</p> <h3 id="some-distributions">Some Distributions</h3> <ul> <li> <table> <tbody> <tr> <td>$q(x_t</td> <td>x_{t-1}):= \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \sqrt{\beta_t}\mathbf{I})$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$q(x_t</td> <td>x_0) \sim \mathcal{N} (x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t) \mathbf{I})$</td> </tr> </tbody> </table> <ul> <li>Let $\alpha_t = 1 - \beta_t$, $\bar{\alpha}_t = \alpha_1 \alpha_2 \cdots \alpha_t$</li> <li>$\begin{align} x_t &amp;= \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_1 <br/> &amp;= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1-\alpha_t} \epsilon_t + \sqrt{\alpha_t - \alpha_t \alpha_{t-1}} \epsilon_{2} <br/> &amp;=\sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}} \bar{\epsilon}<em>{2} <br/> &amp;= \sqrt{\alpha_t \alpha</em>{t-1} \cdots \alpha_1}x_{0} + \sqrt{1-\alpha_t \alpha_{t-1}\cdots \alpha_0} \bar{\epsilon}_{t} <br/> &amp;:= \sqrt{\bar{\alpha_t}} x_0 + \sqrt{1-\bar{\alpha_t}} \bar{\epsilon}_t \end{align}$</li> </ul> </li> <li> <table> <tbody> <tr> <td>$q(x_{t-1}</td> <td>x_t, x_0) \sim \mathcal{N} (x_{t-1}; \tilde{\mu}_t (x_t, x_0), \tilde{\beta}_t \mathbf{I})$</td> </tr> </tbody> </table> <ul> <li><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697757475267-c1edbc41-caec-4570-8222-e847dd71ba0e.png#averageHue=%23f3f3f3&amp;clientId=u961053e4-f864-4&amp;from=paste&amp;height=76&amp;id=ud33762e7&amp;originHeight=151&amp;originWidth=1186&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=37498&amp;status=done&amp;style=none&amp;taskId=u289fa311-77ef-4b37-b8a5-13700311a7d&amp;title=&amp;width=593" alt="image.png"/></li> </ul> </li> <li><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697761841777-375e6f5a-04a6-4647-bf0e-780553b06691.png#averageHue=%23f4f4f4&amp;clientId=u2c57025a-ea68-4&amp;from=paste&amp;height=482&amp;id=ua2c47993&amp;originHeight=963&amp;originWidth=1087&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=201395&amp;status=done&amp;style=none&amp;taskId=ufa5fa872-e83b-49ef-8aa1-1a830216d41&amp;title=&amp;width=543.5" alt="image.png"/></li> <li>KL divergence of two Gaussian: $D_{KL}(P | Q) = \log\left(\frac{\sigma_Q}{\sigma_P}\right) + \frac{\sigma_P^2 + (\mu_P - \mu_Q)^2}{2\sigma_Q^2} - \frac{1}{2}$ <h1 id="reverse-process">Reverse process</h1> </li> <li> <table> <tbody> <tr> <td>$p_{\theta}(\mathbf{x}_{t-1}</td> <td>\mathbf{x}<em>t) = \mathcal{N}(\mathbf{x}</em>{t-1}; \boldsymbol{\mu}<em>{\theta}(\mathbf{x}_t, t), \boldsymbol{\Sigma}</em>{\theta}(\mathbf{x}_t, t))$for $1 \leq t \leq T$</td> </tr> </tbody> </table> </li> <li>We set$\boldsymbol{\Sigma}<em>{\theta}(x_t, t) = \sigma_t^2 \mathbf{I}$. Experimentally, both $\sigma_t^2 = \beta_t$and $\sigma_t^2 = \tilde{\beta}_t = \frac{1 - \bar{\alpha}</em>{t-1}}{1 - \bar{\alpha}_t} \beta_t$ had similar results <ul> <li>Q: In the reverse, why we set$\boldsymbol{\Sigma}_{\theta}(x_t, t)$to be independent of $x_t$</li> </ul> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Background VLB Bound $\begin{align} &amp; \mathbb{E}{q(\mathbf{x}_0)}[-\log p{\theta}(\mathbf{x}0)] \ &amp;= -\mathbb{E}{q(\mathbf{x}0)}[\log (p{\theta}(\mathbf{x}0) \int p{\theta} (\mathbf{x}{1:T})d\mathbf{x}{1:T})] &amp;= -\mathbb{E}{q(\mathbf{x}_0)}[ \log(\int p{\theta} (\mathbf{x}{0:T})d\mathbf{x}{1:T})] &amp;= -\mathbb{E}{q(\mathbf{x}_0)}[\log (\int q (\mathbf{x}{1:T}|\mathbf{x}0) \frac{ p{\theta} (\mathbf{x}{0:T}) }{q(\mathbf{x}{1:T}|\mathbf{x}0) }d\mathbf{x}{1:T}] &amp;= -\mathbb{E}{q(\mathbf{x}_0)}[\log (E{ q (\mathbf{x}{1:T}|\mathbf{x}_0)} \frac{ p{\theta} (\mathbf{x}{0:T}) }{q(\mathbf{x}{1:T}|\mathbf{x}0) })] &amp;\leq -\mathbb{E}{q(\mathbf{x}{0:T})}[\log \frac{ p{\theta} (\mathbf{x}{0:T}) }{q(\mathbf{x}{1:T}|\mathbf{x}0) })] = L{VLB} \end{align}$ $\begin{align} L_{VLB} &amp;= \mathbb{E_q}[-\log \frac{p_{\theta}(\mathbf{x}{0:T})}{q(\mathbf{x}{1:T}|\mathbf{x}0)})] \ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum{t \geq 1}\log \frac{p_{\theta}(\mathbf{x}{t-1}|\mathbf{x}_t)}{q(\mathbf{x}{t}|\mathbf{x}{t-1})})] \ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}{t-1}|\mathbf{x}_t)}{q(\mathbf{x}{t}|\mathbf{x}{t-1})}) - \log \frac{p{\theta}(\mathbf{x}{0}|\mathbf{x}_1)}{q(\mathbf{x}{1}|\mathbf{x}{0})}] \ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}{t-1}|\mathbf{x}_t)}{q(\mathbf{x}{t-1}|\mathbf{x}{t}, \mathbf{x}_0)}\frac{q(\mathbf{x}{t-1}|\mathbf{x}0)}{q(\mathbf{x}_t|\mathbf{x}_0)}) - \log \frac{p{\theta}(\mathbf{x}{0}|\mathbf{x}_1)}{q(\mathbf{x}{1}|\mathbf{x}{0})}] \ &amp;= \mathbb{E_q}[-\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_0)}- \sum{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}{t-1}|\mathbf{x}_t)}{q(\mathbf{x}{t-1}|\mathbf{x}{t}, \mathbf{x}_0)} - \log p{\theta}(\mathbf{x}{0}|\mathbf{x}_1)] \ &amp;= \mathbb{E_q}[ D{\text{KL}}(q(\mathbf{x}T|\mathbf{x}_0)||p(\mathbf{x}T)) + \sum{t&gt;1} D_{\text{KL}}(q(\mathbf{x}{t-1}|\mathbf{x}_t, \mathbf{x}_0)||p{\theta}(\mathbf{x}{t-1}|\mathbf{x}_t))-\log p{\theta}(\mathbf{x}_{0}|\mathbf{x}_1)] \end{align}$ Some Distributions]]></summary></entry><entry><title type="html">Dpm</title><link href="https://szhan311.github.io//blog/2023/DPM/" rel="alternate" type="text/html" title="Dpm"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/DPM</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/DPM/"><![CDATA[<h1 id="preliminiaries">Preliminiaries</h1> <ul> <li>use a Markov chain to gradually convert one distribution into another</li> <li>Ideas: Estimating small perturbations is more tractable than explicitly describing the full distribution with a single, non-analytically-normalizable, potential function</li> <li>Ideas from quasi-static processes, and <strong>annealed</strong> <strong>importance sampling</strong></li> <li>**Jarzynski equality (Annealed Importance Sampling (AIS)): **use a Markov chain which slowly converts one distribution into another to compute a ratio of normalizing constants.</li> <li>Diffusion in the context of statistics refers to transforming a complex distribution $p_{\text{complex}}$on $\mathbb{R}^{d}$to a simple distribution $p_{\text{prior}}$on the same domain. $\mathbf{x}<em>0 \sim p</em>{\text{complex}} \Rightarrow \mathcal{T} (\mathbf{x}<em>0) \sim p</em>{\text{prior}}$</li> <li> <table> <tbody> <tr> <td>By repeated application of a transition kernel $q(\mathbf{x}</td> <td>\mathbf{x}’)$on the samples of any distribution would lead to samples from $p_{\text{prior}}(\mathbf{x})$if the following holds: $p_{\text{prior}}(\textbf{x}) = \int q \mathbf{(x</td> <td>x’} )p_{\text{prior}}\mathbf{x}’d \mathbf{x}’$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Define $\mathcal{T}$ to be repeated application of the transition kernel $\mathbf{q}(\mathbf{x}</td> <td>\mathbf{x}’)$over discrete $t$,$\mathbf{x}_t \sim q(\mathbf{x}</td> <td>\mathbf{x}’ = \mathbf{x}<em>{t-1}), \, \forall t&gt;0$ we have $\mathbf{x}</em>{\infty} \sim p_{\text{prior}}$</td> </tr> </tbody> </table> </li> <li>One attractive choice of ${ q, p_{\text{prior}} }$pair due to its simiplicity and tractability <ul> <li> <table> <tbody> <tr> <td>$q(\mathbf{x}_t</td> <td>\mathbf{x}<em>{t-1}) = \mathcal{N} (\mathbf{x}_t; \sqrt{1- \beta_t}\mathbf{x}</em>{t-1}, \beta_t \mathbf{I})$</td> </tr> </tbody> </table> </li> <li>$q(\mathbf{x}<em>T) = p</em>{prior}(\mathbf{x}_T) = \mathcal{N} (\mathbf{x}_T; \mathbf{0, I})$</li> <li>It is known as Gaussian Diffusion</li> <li>Notice: the diffusion process does not depend on the initial density</li> <li>Forward diffusion process: $\mathbf{x}<em>0 \sim p</em>{\text{data}} \Rightarrow \mathcal{T} (\mathbf{x}_0) \sim \mathcal{N} (\mathbf{0, I})$</li> <li>Reverse diffusion proccess: $\mathbf{x}<em>T \sim \mathcal{N} (\mathbf{0, I}) \Rightarrow \mathcal{T}^{-1} (\mathbf{x}_T) \sim p</em>{\text{data}}$</li> <li> <table> <tbody> <tr> <td>$q(\mathbf{x}_t</td> <td>\mathbf{x}<em>{t-1})$is a Gaussian $\Rightarrow$$q(\mathbf{x}</em>{t-1}</td> <td>\mathbf{x}<em>{t})$is a Gaussian, so we only need to know $\mathbf{f}</em>{\mu}(\mathbf{x}<em>{t}, t)$and $\mathbf{f}</em>{\Sigma}(\mathbf{x}_{t}, t)$for the reverse Markov transitions</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <h1 id="dpm">DPM</h1> <p>$\text{forward (diffusion)} : q(\mathbf{x}<em>{t}\mid\mathbf{x}</em>{t-1})$ $\text{reverse (parametric)} : p_{\theta}(\mathbf{x}<em>{t-1}\mid\mathbf{x}</em>{t})$</p> <h2 id="parameterize-the-mean-and-convariance">Parameterize the mean and convariance</h2> <p>In the DPM, we need to parameterize the mean $\mathbf{\mu}<em>{\theta}(\mathbf{x}</em>{t},t)$ and covariance $\mathbf{\Sigma}<em>{\theta}(\mathbf{x}</em>{t},t)$ Starting from Gaussian noise to gradually remove local perturbations. Therefore the reverse process starts with our given tractable distribution $p(\mathbf{x}<em>{T})=\pi(\mathbf{x}</em>{T})$ and is described as $p_{\theta}(\mathbf{x}<em>{0:T}) = p(\mathbf{x}</em>{T}) \prod_{t=1}^{T} p_{\theta}(\mathbf{x}<em>{t-1}\mid\mathbf{x}</em>{t})$ During learning, only the mean and covariancce for a Gaussian diffusion kernel needs to be trained $p_{\theta}(\mathbf{x}<em>{t-1}\mid\mathbf{x}</em>{t}) = \mathcal{N}(\mathbf{x}<em>{t-1} ; \mathbf{\mu}</em>{\theta}(\mathbf{x}<em>{t},t),\mathbf{\Sigma}</em>{\theta}(\mathbf{x}<em>{t},t))$ The two functions defining the mean $\mathbf{\mu}</em>{\theta}(\mathbf{x}<em>{t},t)$ and covariance $\mathbf{\Sigma}</em>{\theta}(\mathbf{x}_{t},t)$ can be parametrized by deep neural networks.</p> <h2 id="loss-function">Loss function</h2> <p>$\begin{align} \mathbb{E}<em>q\left[-\log p</em>{\theta}(\mathbf{x}<em>{0}) \right] &amp; \leq \mathbb{E}</em>{q}\left[-\log \frac{p_{\theta}(\mathbf{x}<em>{0:T})}{q(\mathbf{x}</em>{1:T} \mid \mathbf{x}<em>{0})} \right] \end{align}$ Define: $\mathcal{L} = \mathbb{E}</em>{q}\left[ -\log p(\mathbf{x}<em>{T}) - \sum</em>{t\geq 1} \log \frac{p_{\theta}(\mathbf{x}<em>{t-1}\mid\mathbf{x}</em>{t})}{q(\mathbf{x}<em>{t}\mid\mathbf{x}</em>{t-1})} \right]$ this loss is shown to be reducible to $\begin{align} K = -\mathbb{E}<em>{q}[ &amp;D</em>{KL}(q(\mathbf{x}<em>{t-1}\mid\mathbf{x}</em>{t},\mathbf{x}<em>{0}) \Vert p</em>{\theta}(\mathbf{x}<em>{t-1}\mid\mathbf{x}</em>{t})) <br/> &amp;+ H_{q}(\mathbf{X}<em>{T}\vert\mathbf{X}</em>{0}) - H_{q}(\mathbf{X}<em>{1}\vert\mathbf{X}</em>{0}) - H_{p}(\mathbf{X}_{T})] \end{align}$</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Preliminiaries use a Markov chain to gradually convert one distribution into another Ideas: Estimating small perturbations is more tractable than explicitly describing the full distribution with a single, non-analytically-normalizable, potential function Ideas from quasi-static processes, and annealed importance sampling **Jarzynski equality (Annealed Importance Sampling (AIS)): **use a Markov chain which slowly converts one distribution into another to compute a ratio of normalizing constants. Diffusion in the context of statistics refers to transforming a complex distribution $p_{\text{complex}}$on $\mathbb{R}^{d}$to a simple distribution $p_{\text{prior}}$on the same domain. $\mathbf{x}0 \sim p{\text{complex}} \Rightarrow \mathcal{T} (\mathbf{x}0) \sim p{\text{prior}}$ By repeated application of a transition kernel $q(\mathbf{x} \mathbf{x}’)$on the samples of any distribution would lead to samples from $p_{\text{prior}}(\mathbf{x})$if the following holds: $p_{\text{prior}}(\textbf{x}) = \int q \mathbf{(x x’} )p_{\text{prior}}\mathbf{x}’d \mathbf{x}’$ Define $\mathcal{T}$ to be repeated application of the transition kernel $\mathbf{q}(\mathbf{x} \mathbf{x}’)$over discrete $t$,$\mathbf{x}_t \sim q(\mathbf{x} \mathbf{x}’ = \mathbf{x}{t-1}), \, \forall t&gt;0$ we have $\mathbf{x}{\infty} \sim p_{\text{prior}}$ One attractive choice of ${ q, p_{\text{prior}} }$pair due to its simiplicity and tractability $q(\mathbf{x}_t \mathbf{x}{t-1}) = \mathcal{N} (\mathbf{x}_t; \sqrt{1- \beta_t}\mathbf{x}{t-1}, \beta_t \mathbf{I})$ $q(\mathbf{x}T) = p{prior}(\mathbf{x}_T) = \mathcal{N} (\mathbf{x}_T; \mathbf{0, I})$ It is known as Gaussian Diffusion Notice: the diffusion process does not depend on the initial density Forward diffusion process: $\mathbf{x}0 \sim p{\text{data}} \Rightarrow \mathcal{T} (\mathbf{x}_0) \sim \mathcal{N} (\mathbf{0, I})$ Reverse diffusion proccess: $\mathbf{x}T \sim \mathcal{N} (\mathbf{0, I}) \Rightarrow \mathcal{T}^{-1} (\mathbf{x}_T) \sim p{\text{data}}$ $q(\mathbf{x}_t \mathbf{x}{t-1})$is a Gaussian $\Rightarrow$$q(\mathbf{x}{t-1} \mathbf{x}{t})$is a Gaussian, so we only need to know $\mathbf{f}{\mu}(\mathbf{x}{t}, t)$and $\mathbf{f}{\Sigma}(\mathbf{x}_{t}, t)$for the reverse Markov transitions]]></summary></entry><entry><title type="html">Edm</title><link href="https://szhan311.github.io//blog/2023/EDM/" rel="alternate" type="text/html" title="Edm"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/EDM</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/EDM/"><![CDATA[<h1 id="original-ode--sde-formulation-from-previous-work">Original ODE / SDE formulation from previous work</h1> <ul> <li>Song et al. define their forward SDE as: $dx = f(x,t)dt + g(t) dw_t$, where $f(\cdot, t):\mathbb{R}^d \rightarrow \mathbb{R}^d$and $g(\cdot): \mathbb{R} \rightarrow \mathbb{R}$are the drift and diffusion coefficients, repsectively, where $d$is the dimensionality of the dataset. $f(\cdot)$is always of the form $f(x, t) = f(t) x$, where $f(\cdot):\mathbb{R} \rightarrow \mathbb{R}$.</li> <li>Thus, the SDE can be equivalently written as$dx = f(t)x + g(t) d w_t$.</li> <li> <table> <tbody> <tr> <td>The pertubation kernels of this SDE have the general form $p_{0t}(x(t)</td> <td>x(0)) = \mathcal{N} (x(t); s(t)x(0), s^2(t) \sigma^2(t)\mathbf{I})$,</td> </tr> </tbody> </table> </li> <li>where $\mathcal{N}(x; \mu, \Sigma)$denotes the probability density function of $\mathcal{N}(\mu, \Sigma)$evaluated at $x$,</li> </ul> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697930172374-2c15fd2a-68e7-47ad-8167-3b372e6cc27a.png#averageHue=%23f5f5f5&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=70&amp;id=u4fa8c5c6&amp;originHeight=140&amp;originWidth=853&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=25020&amp;status=done&amp;style=none&amp;taskId=ua55e5685-8108-45a8-80fc-7c2438ea088&amp;title=&amp;width=426.5" alt="image.png"/></p> <ul> <li>The marginal distribution of $p_t(x)$is</li> </ul> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697930257381-db952a3d-05e4-4996-9006-4d07fa59a526.png#averageHue=%23f1f1f1&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=43&amp;id=u4421d568&amp;originHeight=85&amp;originWidth=536&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=13425&amp;status=done&amp;style=none&amp;taskId=u24e7dd04-d765-402f-8eba-3133d0dcb50&amp;title=&amp;width=268" alt="image.png"/></p> <ul> <li>The probability flow ODE</li> </ul> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697930289525-9c84f26a-65e1-4569-88ec-c4a12499f5ff.png#averageHue=%23ececec&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=34&amp;id=u0ee0774b&amp;originHeight=61&amp;originWidth=559&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=13896&amp;status=done&amp;style=none&amp;taskId=udf9fe673-3e69-48f0-b8e8-beb67581336&amp;title=&amp;width=310.5" alt="image.png"/></p> <h1 id="understanding-the-diffusion-process">Understanding the diffusion process</h1> <h3 id="idea">Idea</h3> <p>$f$and $g$are of little practical interest, the marginal distribution are of utmost inportance</p> <h3 id="the-perturbation-kernels-and-marginal-distribution">The perturbation kernels and marginal distribution</h3> <p>$p_{0 t}(\boldsymbol{x}(t) \mid \boldsymbol{x}(0))=\mathcal{N}\left(\boldsymbol{x}(t) ; s(t) \boldsymbol{x}(0), s(t)^2 \sigma(t)^2 \mathbf{I}\right)$ $p_t(\boldsymbol{x})=\int_{\mathbb{R}^d} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}<em>0\right) p</em>{\text {data }}\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0$</p> <h3 id="convolution-form">Convolution form</h3> <p>$p(\boldsymbol{x} ; \sigma) :=p_{\text {data }} * \mathcal{N}(\mathbf{0}, \sigma(t)^2 \mathbf{I})$ $p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))$</p> <h3 id="ode">ODE</h3> <h4 id="mathrmd-boldsymbolx-dotsigmat-sigmat-nabla_boldsymbolx-log-pboldsymbolx--sigmat-mathrmd-t">$\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t$</h4> <p>$\mathrm{d} \boldsymbol{x}=\left[\frac{\dot{\boldsymbol{s}}(t)}{s(t)} \boldsymbol{x}-s(t)^2 \dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p\left(\frac{\boldsymbol{x}}{s(t)} ; \sigma(t)\right)\right] \mathrm{d} t$</p> <h3 id="denosing-score-matching">Denosing score matching</h3> <p>$D(\boldsymbol{x}; \sigma)$is a denoiser function that minimizes the expeced $L_2$denosing error for samples drawn from $p_{\text{data}}$, $\mathbb{E}<em>{\boldsymbol{y} \sim p</em>{\text {data }}} \mathbb{E}<em>{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}|_2^2$, then $\nabla</em>{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2$</p> <h3 id="heat-equation">Heat equation</h3> <p>$\left{\begin{matrix}</p> <p>&amp;\frac{\partial q}{\partial t} -\dot{\sigma} \sigma \Delta_{\boldsymbol{x}} q = 0 &amp;\text{in} \quad \mathbb{R}^d \times(0, \, \infty) \ &amp;q= p_{data}(\boldsymbol{x}) &amp; \text{on} \quad \mathbb{R}^d \times {t=0}</p> <p>\end{matrix} \right.$</p> <h3 id="sde">SDE</h3> <p>$\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t$ $\mathrm{d} \boldsymbol{x}<em>{ \pm}=\underbrace{-\dot{\sigma}(t) \sigma(t) \nabla</em>{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t}<em>{\text {probability flow ODE}} \pm \underbrace{\beta(t) \sigma(t)^2 \nabla</em>{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t}<em>{\text {deterministic noise decay }}+\underbrace{\sqrt{2 \beta(t)} \sigma(t) \mathrm{d} \omega_t}</em>{\text {noise injection }}$</p> <h1 id="ode-formulation">ODE formulation</h1> <h4 id="equation-1-mathrmd-boldsymbolx-dotsigmat-sigmat-nabla_boldsymbolx-log-pboldsymbolx--sigmat-mathrmd-t">Equation 1: $\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t$</h4> <h4 id="proof">Proof</h4> <p>The marginal distrobution <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697930569096-d5a257f3-6450-484f-afd6-041f31e6a10f.png#averageHue=%23f5f5f5&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=206&amp;id=u638b3b63&amp;originHeight=411&amp;originWidth=1089&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=79908&amp;status=done&amp;style=none&amp;taskId=u313df4ce-60f6-4073-8c19-a436b95fc7c&amp;title=&amp;width=544.5" alt="image.png"/> where $p_a * p_b$denotes the convolution of probability density functions $p_a$and $p_b$ Let denote: <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697930701606-9f6d0ebd-be76-4eb4-bbbf-d53b65041430.png#averageHue=%23ededed&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=30&amp;id=u983cd5f5&amp;originHeight=59&amp;originWidth=969&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=18831&amp;status=done&amp;style=none&amp;taskId=ub805e725-d7ff-48bc-85fb-06c5e181f8f&amp;title=&amp;width=484.5" alt="image.png"/> the probability ﬂow ODE <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697930831219-82ef998f-97eb-494c-bd79-a4bd260357a6.png#averageHue=%23f0f0f0&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=106&amp;id=uad3225bf&amp;originHeight=211&amp;originWidth=1159&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=62671&amp;status=done&amp;style=none&amp;taskId=u1dd8658d-ccec-4ad9-90a4-88582f1b435&amp;title=&amp;width=579.5" alt="image.png"/> Next, $f(t) = {\dot s(t)}/{s(t)}$, $g(t) = s(t) \sqrt{2 \dot{\sigma}(t) \sigma(t)}$ <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697930954904-0a5b9b1a-97f1-4647-a688-d820c439d120.png#averageHue=%23f8f8f8&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=170&amp;id=u75a1a75b&amp;originHeight=340&amp;originWidth=964&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=50358&amp;status=done&amp;style=none&amp;taskId=u69cf4036-dd99-4d84-8263-067f88ba2a0&amp;title=&amp;width=482" alt="image.png"/> <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697931014823-264dcafd-b5a4-4d78-8a1f-4edea0ec0779.png#averageHue=%23f7f7f7&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=227&amp;id=u5e5bb92b&amp;originHeight=453&amp;originWidth=980&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=77385&amp;status=done&amp;style=none&amp;taskId=u22ea01df-7d17-4abf-a48c-4743f3c12f9&amp;title=&amp;width=490" alt="image.png"/> Finally <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697931039779-bb7dc83a-0142-42e3-a26e-0110a91f4103.png#averageHue=%23f3f3f3&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=175&amp;id=udfc3e8bb&amp;originHeight=349&amp;originWidth=1012&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=69591&amp;status=done&amp;style=none&amp;taskId=u8d189c60-063b-400a-b206-37fbf84b84a&amp;title=&amp;width=506" alt="image.png"/> By setting $s(t) = 1$: <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697931108682-eb182f7a-83c3-4c1e-853b-0b5f5d11b881.png#averageHue=%23ececec&amp;clientId=u312744c2-edfb-4&amp;from=paste&amp;height=32&amp;id=u87895a30&amp;originHeight=60&amp;originWidth=514&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=13035&amp;status=done&amp;style=none&amp;taskId=u9227af1f-b801-4cdf-9ef4-91404a710b6&amp;title=&amp;width=275" alt="image.png"/></p> <h1 id="denoising-score-matching">Denoising score matching</h1> <h4 id="equation-2--3">Equation 2 &amp; 3</h4> <p>$\mathbb{E}<em>{\boldsymbol{y} \sim p</em>{\text {data }}} \mathbb{E}<em>{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}|_2^2$, then $\nabla</em>{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2$</p> <h4 id="proof-1">Proof</h4> <p>Finite number of samples ${\mathbf{y}<em>1, \cdots, \mathbf{y}_Y }$,$p</em>{data}(x)$is represented by a mixture of Dirac delta distributions:$p_{\text{data}}(x) = \frac{1}{Y} \sum_{i=1}^{Y} \delta(\mathbf{x} - \mathbf{y}<em>i)$ $\begin{aligned} p(\boldsymbol{x} ; \sigma) &amp; =p</em>{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right) \ &amp; =\int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}<em>0\right) \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \ &amp; =\int</em>{\mathbb{R}^d}\left[\frac{1}{Y} \sum_{i=1}^Y \delta\left(\boldsymbol{x}<em>0-\boldsymbol{y}_i\right)\right] \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \ &amp; =\frac{1}{Y} \sum</em>{i=1}^Y \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}<em>0, \sigma^2 \mathbf{I}\right) \delta\left(\boldsymbol{x}_0-\boldsymbol{y}_i\right) \mathrm{d} \boldsymbol{x}_0 \ &amp; =\frac{1}{Y} \sum</em>{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)\end{aligned}$<br/> Considering the Eq. 2. by expanding the expections: $\begin{aligned} \mathcal{L}(D ; \sigma) &amp; =\mathbb{E}</em>{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}<em>{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}|_2^2 \ &amp; =\mathbb{E}</em>{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}<em>{\boldsymbol{x} \sim \mathcal{N}\left(\boldsymbol{y}, \sigma^2 \mathbf{I}\right)}|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}|_2^2 \ &amp; =\mathbb{E}</em>{\boldsymbol{y} \sim p_{\text {data }}} \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}, \sigma^2 \mathbf{I}\right)|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}|<em>2^2 \mathrm{~d} \boldsymbol{x} \ &amp; =\frac{1}{Y} \sum</em>{i=1}^Y \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)\left|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right|_2^2 \mathrm{~d} \boldsymbol{x} \ &amp; =\int</em>{\mathbb{R}^d} \underbrace{\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)\left|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right|_2^2}</em>{=: \mathcal{L}(D ; \boldsymbol{x}, \sigma)} \mathrm{d} \boldsymbol{x} .\end{aligned}$ we can minimize $\mathcal{L}(D ; \sigma)$by minimizing $\mathcal{L}(D ; \boldsymbol{x}, \sigma)$independently for each $\boldsymbol{x}$: $D(\boldsymbol{x} ; \sigma)=\arg \min <em>{D(\boldsymbol{x} ; \sigma)} \mathcal{L}(D ; \boldsymbol{x}, \sigma)$ This is a convex optimization problem; its solution is uniquely identiﬁed by setting the gradient w.r.t. $D(\boldsymbol{x}; \sigma)$to zero: $\begin{aligned} \mathbf{0} &amp; =\nabla</em>{D(\boldsymbol{x} ; \sigma)}[\mathcal{L}(D ; \boldsymbol{x}, \sigma)] \ \mathbf{0} &amp; =\nabla_{D(\boldsymbol{x} ; \sigma)}\left[\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)\left|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right|_2^2\right] \ \mathbf{0} &amp; =\sum</em>{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right) \nabla</em>{D(\boldsymbol{x} ; \sigma)}\left[\left|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}<em>i\right|_2^2\right] \ \mathbf{0} &amp; =\sum</em>{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)\left[2 D(\boldsymbol{x} ; \sigma)-2 \boldsymbol{y}_i\right] \ \mathbf{0} &amp; =\left[\sum</em>{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)\right] D(\boldsymbol{x} ; \sigma)-\sum</em>{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i \ D(\boldsymbol{x} ; \sigma) &amp; =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\end{aligned}$ which gives a closed-form solution for the ideal denoiser $D(\boldsymbol{x}; \sigma)$. $D(\boldsymbol{x} ; \sigma) =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}$ Next, let us consider the score of the distribution $\begin{aligned} \nabla</em>{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma) &amp; =\frac{\nabla_{\boldsymbol{x}} p(\boldsymbol{x} ; \sigma)}{p(\boldsymbol{x} ; \sigma)} \ &amp; =\frac{\nabla_{\boldsymbol{x}}\left[\frac{1}{Y} \sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)\right]}{\left[\frac{1}{Y} \sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\right]} \ &amp; =\frac{\sum_i \nabla</em>{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\end{aligned}$ Then $\begin{aligned} \nabla</em>{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right) &amp; =\nabla</em>{\boldsymbol{x}}\left[\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \exp \frac{\left|\boldsymbol{x}-\boldsymbol{y}<em>i\right|_2^2}{-2 \sigma^2}\right] \ &amp; =\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \nabla</em>{\boldsymbol{x}}\left[\exp \frac{\left|\boldsymbol{x}-\boldsymbol{y}<em>i\right|_2^2}{-2 \sigma^2}\right] \ &amp; =\left[\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \exp \frac{\left|\boldsymbol{x}-\boldsymbol{y}_i\right|_2^2}{-2 \sigma^2}\right] \nabla</em>{\boldsymbol{x}}\left[\frac{\left|\boldsymbol{x}-\boldsymbol{y}<em>i\right|_2^2}{-2 \sigma^2}\right] \ &amp; =\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \nabla</em>{\boldsymbol{x}}\left[\frac{\left|\boldsymbol{x}-\boldsymbol{y}<em>i\right|_2^2}{-2 \sigma^2}\right] \ &amp; =\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[\frac{\boldsymbol{y}_i-\boldsymbol{x}}{\sigma^2}\right] .\end{aligned}$ Next $\begin{aligned} \nabla</em>{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma) &amp; =\frac{\sum_i \nabla_{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}<em>i, \sigma^2 \mathbf{I}\right)}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)} \ &amp; =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[\frac{\boldsymbol{y}_i-\boldsymbol{x}}{\sigma^2}\right]}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)} \ &amp; =\left(\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}-\boldsymbol{x}\right) / \sigma^2 .\end{aligned}$ So $\nabla</em>{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2$</p> <h1 id="proposed-sde">Proposed SDE</h1> <h3 id="task-1-find-a-pde-given-initial-value-qboldsymbolx-0-p_databoldsymbolx-and-solution-qboldsymbolx-t--pboldsymbolx-sigmat">Task 1: Find a PDE given initial value $q(\boldsymbol{x}, 0):= p_{data}(\boldsymbol{x})$, and solution $q(\boldsymbol{x}, t) = p(\boldsymbol{x}, \sigma(t))$</h3> <h4 id="the-solution-is-a-heat-equation">The solution is a heat equation</h4> <p>$\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$</p> <h4 id="proof-2">Proof</h4> <p>$p(\boldsymbol{x} ; \sigma)=p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)$ and $p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))$ the density evolves according to a heat diffusion PDE with time-varying diffusivity. As a ﬁrst step, we ﬁnd this PDE. The solution can be generated by the heat equation with time-varying diffusivity $\kappa(t)$. The heat equation PDE: $\frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t}=-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)$ Take Fourier transform along the $\boldsymbol{x}-\text{dimension}$ $\frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t}=-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)$ Since $q(\boldsymbol{x}, t)=p(\boldsymbol{x} ; \sigma(t))=p_{\text {data }}(\boldsymbol{x}) * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)$, $\hat{q}(\boldsymbol{\nu}, t)=\hat{p}<em>{\text {data }}(\boldsymbol{\nu}) \exp \left(-\frac{1}{2}|\boldsymbol{\nu}|^2 \sigma(t)^2\right)$ Differentiating the target solution along the time axis, we have $\begin{aligned} \frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t} &amp; =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{p}</em>{\text {data }}(\boldsymbol{\nu}) \exp \left(-\frac{1}{2}|\boldsymbol{\nu}|^2 \sigma(t)^2\right) \ &amp; =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)\end{aligned}$ Then we have $\begin{aligned}-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t) &amp; =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t) \ \kappa(t) &amp; =\dot{\sigma}(t) \sigma(t) .\end{aligned}$ To summzrize $\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$</p> <h3 id="task-2-seek-an-sde-whose-solution-density-is-described-by-the-pde-fracpartial-qboldsymbolx-tpartial-tdotsigmat-sigmat-delta_boldsymbolx-qboldsymbolx-t">Task 2: Seek an SDE whose solution density is described by the PDE $\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$</h3> <h4 id="solution-mathrmd-boldsymbolxleftfrac12-gt2-dotsigmat-sigmatright-nabla_boldsymbolx-log-pboldsymbolx--sigmat-mathrmd-tgt-mathrmd-omega_t">Solution: $\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t$</h4> <h4 id="proof-3">Proof</h4> <p>Given an SDE,$\mathrm{d} \boldsymbol{x}=\boldsymbol{f}(\boldsymbol{x}, t) \mathrm{d} t+\boldsymbol{g}(\boldsymbol{x}, t) \mathrm{d} \omega_t$, The Fokker–Planck PDE describes the time evolution of its solution probability density $r(\boldsymbol{x},t)$as $\frac{\partial r(\boldsymbol{x}, t)}{\partial t}=-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))+\frac{1}{2} \nabla_{\boldsymbol{x}} \nabla_{\boldsymbol{x}}:(\mathbf{D}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))$ where $\mathbf{D}<em>{i j}=\sum_k \boldsymbol{g}</em>{i k} \boldsymbol{g}<em>{j k}$is diffusion tensor. We consider a special case $\boldsymbol{g}(\boldsymbol{x}, t) = g(t) \boldsymbol{I}$ $\frac{\partial r(\boldsymbol{x}, t)}{\partial t}=-\nabla</em>{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))+\frac{1}{2} g(t)^2 \Delta_{\boldsymbol{x}} r(\boldsymbol{x}, t)$ We can find a sufﬁcient condition that sastify the PDE $\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$ $-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) q(\boldsymbol{x}, t))+\frac{1}{2} g(t)^2 \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) = \dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)$ The key is given by the identity $\nabla_{\boldsymbol{x}} \cdot \nabla_{\boldsymbol{x}}=\Delta_{\boldsymbol{x}}$. We set $\boldsymbol{f}(\boldsymbol{x}, t) q(\boldsymbol{x}, t)=v(t) \nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)$, then $\begin{aligned} \nabla_{\boldsymbol{x}} \cdot\left(v(t) \nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)\right) &amp; =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) \ v(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) &amp; =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) \ v(t) &amp; =\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t) .\end{aligned}$ $\boldsymbol{f}(\boldsymbol{x}, t)$is in fact proportional to the score function: $\begin{aligned} \boldsymbol{f}(\boldsymbol{x}, t) &amp; =v(t) \frac{\nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)}{q(\boldsymbol{x}, t)} \ &amp; =v(t) \nabla_{\boldsymbol{x}} \log q(\boldsymbol{x}, t) \ &amp; =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log q(\boldsymbol{x}, t)\end{aligned}$ we recover a family of SDEs whose solution densities have the desired marginals with noise levels $\sigma(t)$for any choice of $g(t)$: $\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t$</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Original ODE / SDE formulation from previous work Song et al. define their forward SDE as: $dx = f(x,t)dt + g(t) dw_t$, where $f(\cdot, t):\mathbb{R}^d \rightarrow \mathbb{R}^d$and $g(\cdot): \mathbb{R} \rightarrow \mathbb{R}$are the drift and diffusion coefficients, repsectively, where $d$is the dimensionality of the dataset. $f(\cdot)$is always of the form $f(x, t) = f(t) x$, where $f(\cdot):\mathbb{R} \rightarrow \mathbb{R}$. Thus, the SDE can be equivalently written as$dx = f(t)x + g(t) d w_t$. The pertubation kernels of this SDE have the general form $p_{0t}(x(t) x(0)) = \mathcal{N} (x(t); s(t)x(0), s^2(t) \sigma^2(t)\mathbf{I})$, where $\mathcal{N}(x; \mu, \Sigma)$denotes the probability density function of $\mathcal{N}(\mu, \Sigma)$evaluated at $x$,]]></summary></entry><entry><title type="html">Mdm</title><link href="https://szhan311.github.io//blog/2023/MDM/" rel="alternate" type="text/html" title="Mdm"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/MDM</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/MDM/"><![CDATA[<p>Paper: Where to diffuse, how to diffuse, and how to get back: Automated learning for multivariate diffusions.</p> <h1 id="background">Background</h1> <ul> <li>The choice of this inference process affects both <strong>likelihoods and sample quality</strong>.</li> <li>On different datasets and models, different inference processes work better;</li> <li>A natural question: are there other auxiliary variable diffusions that would lead to improvements like CLD?</li> <li>**Auxiliary variables **have improved other generative models and inferences <ul> <li>such as normalizing ﬂows, neural ordinary differential equations (ODEs), hierarchical variational models, ladder variational autoencoder</li> </ul> </li> </ul> <h1 id="requirement-of-design-a-diffusion-model">Requirement of design a diffusion model</h1> <ul> <li>Selecting an inference and model process pair <ul> <li>such that the inference process converges to the model prior</li> </ul> </li> <li>Deriving the ELBO for this pair</li> <li>Estimating the ELBO and its gradients <ul> <li>by deriving and computing the inference process’ transition kernel <h1 id="work-of-this-paper">Work of this paper</h1> </li> </ul> </li> <li>provide a recipe for training MDMs beyond speciﬁc instantiations to all linear inference processes that have a stationary distribution, <strong>with any number of auxiliary variables</strong>.</li> <li>Using results from gradient-based Markov chain Monte Carlo (MCMC) to construct MDMs</li> <li>derive the MDM ELBO</li> <li>the transition kernel of linear MDMs</li> </ul> <h1 id="elbo-bound">ELBO bound</h1> <p>Reverse process: $d \mathbf{z}=h_\theta(\mathbf{z}, t) d t+\beta_\theta(t) d \mathbf{B}<em>t, \quad t \in[0, T]$ Forward process: $d \mathbf{y}=f</em>\phi(\mathbf{y}, s) d s+g_\phi(s) d \widehat{\mathbf{B}}<em>s, \quad s \in[0, T]$ $\mathbf{z}_T$ approximates the data $x \sim q</em>{\text{data}}$ When the model take the form $d \mathbf{z}=\left[g_\phi^2(T-t) s_\theta(\mathbf{z}, T-t)-f_\phi(\mathbf{z}, T-t)\right] d t+g_\phi(T-t) d \mathbf{B}<em>t$ The ELBO is: $\log p</em>\theta(x) \geq \mathcal{L}^{\mathrm{ism}}(x)=\mathbb{E}<em>{q</em>\phi(\mathbf{y} \mid x)}\left[\log \pi_\theta\left(\mathbf{y}<em>T\right)+\int_0^T-\frac{1}{2}\left|s</em>\theta\right|<em>{g</em>\phi^2}^2-\nabla \cdot\left(g_\phi^2 s_\theta-f_\phi\right) d s\right]$ where $f_{\phi}, g_{\phi}, s_{\theta}$ are evaluated at $(\mathbf{y}<em>s, s)$, $|\mathbf{x}|</em>{\mathbf{A}}^2=\mathbf{x}^{\top} \mathbf{A} \mathbf{x}$ and $g^2 = gg^T$ ISM means Implicit Score Matching loss, which can be re-written as an ELBO $L^{\text{dsm}}$ featuring Denoisng Score Matching (DSM)</p> <h1 id="multivariate-model-and-inference">Multivariate Model and Inference</h1> <p>$\mathbf{u}=\left[\begin{array}{c}\mathbf{z}<em>t \ \mathbf{v}_t\end{array}\right]$ $\mathbf{u}_0 \sim \pi</em>\theta, \quad d \mathbf{u}=h_\theta\left(\mathbf{u}<em>t, t\right) d t+\beta</em>\theta(t) d \mathbf{B}_t$</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Paper: Where to diffuse, how to diffuse, and how to get back: Automated learning for multivariate diffusions. Background]]></summary></entry><entry><title type="html">Psld</title><link href="https://szhan311.github.io//blog/2023/PSLD/" rel="alternate" type="text/html" title="Psld"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/PSLD</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/PSLD/"><![CDATA[<p>Phase Space Langevin Diffusion</p> <h1 id="preliminaries">Preliminaries</h1> <h3 id="sgm">SGM</h3> <p>Forward process SDE for $\mathbf{x}<em>t \in \mathbb{R}^d$: $d\mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t) dt + \mathbf{G}(t) d \mathbf{w}_t$ The reverse SDE is given by: $d\mathbf{x}_t = (\mathbf{f} (\mathbf{x}_t, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla</em>{\mathbf{x}<em>t} \log \mathbf{p}_t(\mathbf{x}_t, t))dt + \mathbf{G}(t) d \bar {\mathbf{w}}_t$ The denoising score matching: $\mathcal{L}(\theta)=\mathbb{E}</em>{\mathbf{x}<em>t \sim p\left(\mathbf{x}_t \mid \mathbf{x}_0\right), \mathbf{x}_0 \sim q</em>{\text {data }}(\mathbf{x}), t \sim \mathcal{U}(0, T)}\left[\left|\mathbf{s}<em>\theta\left(\mathbf{x}_t, t\right)-\nabla</em>{\mathbf{x}_t} \log p\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right|^2\right]$</p> <h3 id="sampling">Sampling</h3> <p>Given $p_s(\mathbf{z}) \propto \exp(- H(\mathbf{z}))$, sampling SDE:</p> <ul> <li><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697932448678-33b2275d-45a3-4f85-bf17-9ce9feeaaf30.png#averageHue=%23e7e7e7&amp;clientId=uc38b8078-9b77-4&amp;from=paste&amp;height=29&amp;id=uf2a3f6e8&amp;originHeight=54&amp;originWidth=403&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=10340&amp;status=done&amp;style=none&amp;taskId=u885c7c35-8be7-4800-bc1e-84c6b9c1430&amp;title=&amp;width=215.5" alt="image.png"/><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697932488641-0e67a9f2-1016-4929-8107-5c5361613828.png#averageHue=%23ededed&amp;clientId=uc38b8078-9b77-4&amp;from=paste&amp;height=35&amp;id=uc65a40f5&amp;originHeight=70&amp;originWidth=546&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=13666&amp;status=done&amp;style=none&amp;taskId=u2c1cc1a3-67fa-48a6-a6aa-c0620f2cda2&amp;title=&amp;width=273" alt="image.png"/></li> <li>$\mathbf{f}(\mathbf{z})=-[\mathbf{D}(\mathbf{z})+\mathbf{Q}(\mathbf{z})] \nabla H(\mathbf{z})+\Gamma(\mathbf{z}), \quad \Gamma_i(\mathbf{z})=\sum_{j=1}^d \frac{\partial}{\partial \mathbf{z}<em>j}\left(\mathbf{D}</em>{i j}(\mathbf{z})+\mathbf{Q}_{i j}(\mathbf{z})\right)$ <ul> <li>$\mathbf{D(z)}$: a <strong>positive semideﬁnite</strong> diffusion matrix; the strength of the Wienerprocess-driven diffusion.</li> <li>$\mathbf{Q(z)}$ is a <strong>skew-symmetric</strong> curl matrix; the deterministic traversing effects.</li> <li>Adjuct $\mathbf{D(z)}$and $\mathbf{Q(z)}$to attain faster convergenve</li> </ul> </li> </ul> <h1 id="psld">PSLD</h1> <p>$p_s(\mathbf{z})=\mathcal{N}\left(\mathbf{x} ; \mathbf{0}<em>{d_x}, \boldsymbol{I}</em>{d_x}\right) \mathcal{N}\left(\mathbf{0}<em>{d_m}, M \boldsymbol{I}</em>{d_m}\right)$</p> <h3 id="forward-sde">Forward SDE</h3> <p>The choice of momentum $\mathbf{m}_t\in \mathbb{R}^d$ The choice of $\mathbf{D}$and $\mathbf{Q}$: <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697931713095-3ff44b0c-d3ee-4d04-9305-0a03f2b0c984.png#averageHue=%23f3f0f0&amp;clientId=ufee8c02a-34b5-4&amp;from=paste&amp;height=66&amp;id=ue96fc47d&amp;originHeight=131&amp;originWidth=770&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=27174&amp;status=done&amp;style=none&amp;taskId=ub7b8b5de-3911-48c6-b684-9f391e774be&amp;title=&amp;width=385" alt="image.png"/> Above $\Gamma, \, M, \, v, \, \beta$are positive scalars. The forward process is given by: <img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697931924336-917cd57f-a7ef-4812-9ec5-6a10134a9503.png#averageHue=%23f2f2f2&amp;clientId=ufee8c02a-34b5-4&amp;from=paste&amp;height=133&amp;id=u0728707f&amp;originHeight=265&amp;originWidth=597&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=40389&amp;status=done&amp;style=none&amp;taskId=uff0786fd-f9d7-4f84-a4e3-39f0e22cfa1&amp;title=&amp;width=298.5" alt="image.png"/></p> <h3 id="reverse-sde">Reverse SDE</h3> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697932640475-29cb8b10-7f24-403d-a8aa-2498c4350bc5.png#averageHue=%23f3f3f3&amp;clientId=ub27ee31f-3ecd-4&amp;from=paste&amp;height=142&amp;id=ub48eb5b5&amp;originHeight=283&amp;originWidth=755&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=50084&amp;status=done&amp;style=none&amp;taskId=u7d9f9daf-73fb-43fb-bb04-3e663eb30b4&amp;title=&amp;width=377.5" alt="image.png"/></p> <h3 id="sde-solver-sscs">SDE solver: SSCS</h3> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1697932782337-65ee40e1-410e-42ac-8d5a-c59fba1e9dca.png#averageHue=%23f3f3f3&amp;clientId=ub27ee31f-3ecd-4&amp;from=paste&amp;height=140&amp;id=ue1a71e05&amp;originHeight=279&amp;originWidth=689&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=47567&amp;status=done&amp;style=none&amp;taskId=uf142f8ab-000f-48ef-b3c2-5d0deb1b237&amp;title=&amp;width=344.5" alt="image.png"/></p> <h3 id="training-objectives">Training Objectives</h3> <p>$\min <em>{\boldsymbol{\theta}} \mathbb{E}_t \mathbb{E}</em>{p\left(\mathbf{z}<em>0\right)} \mathbb{E}</em>{p_t\left(\mathbf{z}<em>t \mid \mathbf{z}_0\right)}\left[\mathcal{L}_x\left(\boldsymbol{\theta}, \mathbf{z}_t, \mathbf{z}_0\right)+\mathcal{L}_m\left(\boldsymbol{\theta}, \mathbf{z}_t, \mathbf{z}_0\right)\right]$ $\begin{aligned} \mathcal{L}_x &amp; =\Gamma \beta\left|\left.\mathbf{s}</em>{\boldsymbol{\theta}}\left(\mathbf{z}<em>t, t\right)\right|</em>{0: d}-\nabla_{\mathbf{x}<em>t} \log p_t\left(\mathbf{z}_t \mid \mathbf{z}_0\right)\right|_2^2, \ \mathcal{L}_m &amp; =M \nu \beta\left|\left.\mathbf{s}</em>{\boldsymbol{\theta}}\left(\mathbf{z}<em>t, t\right)\right|</em>{d: 2 d}-\nabla_{\mathbf{m}_t} \log p_t\left(\mathbf{z}_t \mid \mathbf{z}_0\right)\right|_2^2,\end{aligned}$</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Phase Space Langevin Diffusion Preliminaries SGM Forward process SDE for $\mathbf{x}t \in \mathbb{R}^d$: $d\mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t) dt + \mathbf{G}(t) d \mathbf{w}_t$ The reverse SDE is given by: $d\mathbf{x}_t = (\mathbf{f} (\mathbf{x}_t, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla{\mathbf{x}t} \log \mathbf{p}_t(\mathbf{x}_t, t))dt + \mathbf{G}(t) d \bar {\mathbf{w}}_t$ The denoising score matching: $\mathcal{L}(\theta)=\mathbb{E}{\mathbf{x}t \sim p\left(\mathbf{x}_t \mid \mathbf{x}_0\right), \mathbf{x}_0 \sim q{\text {data }}(\mathbf{x}), t \sim \mathcal{U}(0, T)}\left[\left|\mathbf{s}\theta\left(\mathbf{x}_t, t\right)-\nabla{\mathbf{x}_t} \log p\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right|^2\right]$ Sampling Given $p_s(\mathbf{z}) \propto \exp(- H(\mathbf{z}))$, sampling SDE:]]></summary></entry><entry><title type="html">Reverse Time diffusion equation models</title><link href="https://szhan311.github.io//blog/2023/Reverse-time-Diffusion-Equation-Models/" rel="alternate" type="text/html" title="Reverse Time diffusion equation models"/><published>2023-12-14T00:00:00+00:00</published><updated>2023-12-14T00:00:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Reverse-time%20Diffusion%20Equation%20Models</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Reverse-time-Diffusion-Equation-Models/"><![CDATA[<h1 id="what-is-the-reversal-of-a-diffusion-process">What is the reversal of a diffusion process?</h1> <p>Suppose you have an initial distribution $p(x;0)$and some drift and diffusion coefficients $f$and $g$. Together, they describe a distribution over the trajectories of your system through time Suppose at time $T$, the marginal distributionover the trajectories of your system through time. Suppose at time $T$, the margianl distribution of the states is $p(x; T)$. Can you describe a diffusion process such that it starts with a distribution $q(x; 0): = p(x; T)$and evolves such that the distribution of the trajectories is the same as in the original process but with the tiem reversed? Of course, in such a case we shall have $q(x; T) = p(x; 0)$.</p> <h1 id="non-linear-case">Non-linear case</h1> <h3 id="ito-sde-and-reverse-time-sde">Ito SDE and reverse time SDE</h3> <ul> <li>Given a SDE: $d \boldsymbol{x} = \boldsymbol{f} (\boldsymbol{x}, t)dt + \boldsymbol{G}(\boldsymbol{X}, t)d\boldsymbol{w}$, where $\mathbf{f}(\cdot, t): \mathbb{R}^d \rightarrow \mathbb{R}^d$ and $\mathbf{G}(\cdot, t): \mathbb{R}^d \rightarrow \mathbb{R}^{d \times d}$</li> </ul> <p>The reverse time SDE is given by $\mathrm{d} \mathbf{x}=\left{\mathbf{f}(\mathbf{x}, t)-\nabla \cdot\left[\mathbf{G}(\mathbf{x}, t) \mathbf{G}(\mathbf{x}, t)^{\top}\right]-\mathbf{G}(\mathbf{x}, t) \mathbf{G}(\mathbf{x}, t)^{\top} \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right} \mathrm{d} t+\mathbf{G}(\mathbf{x}, t) \mathrm{d} \overline{\mathbf{w}}$ where we define $\nabla \cdot \mathbf{F}(\mathbf{x}):=\left(\nabla \cdot \mathbf{f}^1(\mathbf{x}), \nabla \cdot \mathbf{f}^2(\mathbf{x}), \cdots, \nabla \cdot \mathbf{f}^d(\mathbf{x})\right)^{\top}$ for a matrix-valued function $\mathbf{F}(\mathbf{x}):=\left(\mathbf{f}^1(\mathbf{x}), \mathbf{f}^2(\mathbf{x}), \cdots, \mathbf{f}^d(\mathbf{x})\right)^{\top}$</p> <h3 id="a-special-case-diffusion-model-as-sdes">A Special case: Diffusion model as SDEs</h3> <p>$dX = f(X, t)dt + g(t) dw$ The reverse time SDE is: $\begin{align} dX &amp;= (f (X, t) - \frac{1}{p(X, t)}\frac{\partial}{\partial X} g^2(t)p(X, t))dt + g(t)dw \ &amp;= (f (X, t) -g^2(t) \frac{\partial}{\partial X} \log p(X, t))dt + g(t)dw</p> <p>\end{align}$</p> <h1 id="the-linear-problem">The linear Problem</h1> <ul> <li>Gaussian-Markov processes: $dx = Ax dt + B dw$, here $A$and $B$are constant matrices, $Re[\lambda_i(A)] &lt; 0$for all $i$and $w(\cdot)$is a vector Wiener process such that $x(t)$is independent of future increments of $w$, but not of past ones.</li> <li>Solution: $x(t) = \int_{-\infty}^{t} e^{A(t-s)}Bdw(s)$</li> <li>A reverse time model: $dx = \bar A dt + \bar B d \bar w$, where now $Re[\lambda_i(\bar A)] &gt;0$for all $i$.</li> <li>Let $P = E[x(t) x^T(t)]$, the matrix $P$is the solution of the linear matrix equation $PA^T + AP = -BB^T$, and id nonsingular recisely when rank $[B AB \cdots A^{n-1}B] = n$ <ul> <li>Here $P$is the covariance matrix</li> <li>Proof: <ul> <li>take the differential of $x(t) x^T(t)$: $d(xx^T) = dxx^T + xdx^T + dxdx’$;</li> <li>Substitude $dx = Ax dt + B dw$into the above equation: $d(xx^T) = (Axdt + Bdw)dx^T + x(A^T x^T dt + B^T dw^T) + (Axdt + Bdw)(A^Tx^T dt + B^T dw^T )$</li> <li>Taking expection on both sides and using the fact that the expected value of the increments of the Wiener process $dw$are zero: $\frac{d}{dt} E(xx^T) = AE[xx^T] + E[x x^T] A^T + BB^T$</li> <li>Solving the above matrix differential equations for $E[xx^T]$ gives $PA^T + AP = -BB^T$</li> </ul> </li> </ul> </li> <li>Suppose $P$is nonsingular, and define $d \bar w = dw - B^TP^{-1}xdt, \bar w(0) = 0$</li> <li>$dx = (A + BB^TP^{-1})dt + Bd\bar{w}$</li> </ul> <h1 id="the-kolmogorov-equations">The Kolmogorov Equations</h1> <ul> <li>The Forward Kolmogorov Equation: $\frac{\partial}{\partial t} p(x ; t \mid y ; s)=-\frac{\partial}{\partial x}(f(x, t) p(x ; t \mid y ; s))+\frac{1}{2} \frac{\partial^2}{\partial x^2}\left(g^2(x ; t) p(x ; t \mid y ; s)\right)$</li> <li>The Backward Kolmogorov Equation: $-\frac{\partial}{\partial s} p(x ; t \mid y ; s)=f(y, s) \frac{\partial}{\partial y} p(x ; t \mid y ; s)+\frac{g^2(y ; s)}{2} \frac{\partial^2}{\partial y^2} p(x ; t \mid y ; s)$</li> <li>$E[dX] = f(X, t)dt$, $Var(dX) = g^2(X, t)dt - O(dt^2) \approx g^2(X, t) dt$ <h2 id="the-forward-kolmogorov-equation">The Forward Kolmogorov Equation</h2> </li> <li> <table> <tbody> <tr> <td>$p(x;t</td> <td>y,s) = \int_{-\infty}^{\infty} p(x; t</td> <td>k; m) p(k;m</td> <td>y;s)dk$, where $p(x; t</td> <td>y, s)$represents the probability $p(X_t = t</td> <td>X_s = y)$with $t \geq s$always.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$p(x;t) = \int_{-\infty}^{\infty} p(x; t</td> <td>k; m) p(k;m)dk$</td> </tr> </tbody> </table> </li> <li>The initial marginal density $p(x;0)$determine the entire diffusion process.</li> </ul> <p>$p(x;t + dt|y,s) = \int_{-\infty}^{\infty} p(x; t + dt | m;t) p(m;t |y;s)dm$ Define: $\phi_t(\Delta; z) = p(z+\Delta; t+dt|z;t)$, then $m = x = \Delta \Rightarrow dm = -d \Delta$; $m = \pm \infty \Rightarrow \Delta = \mp \infty$ $\begin{aligned} m &amp; =x-\Delta \ \Rightarrow \mathrm{d} m &amp; =-\mathrm{d} \Delta \ m= \pm \infty &amp; \Rightarrow \Delta=\mp \infty \ \Rightarrow p(x ; t+\mathrm{d} t \mid y ; s) &amp; =-\int_{\infty}^{-\infty} \phi_t(\Delta ; m) p(m ; t \mid y ; s) \mathrm{d} \Delta \ &amp; =\int_{-\infty}^{+\infty} \phi_t(\Delta ; m) p(m ; t \mid y ; s) \mathrm{d} \Delta\end{aligned}$ Next Taylor expand the entir function within the integral with respect to $m$around $x$: $\begin{aligned} p(x ; t+\mathrm{d} t \mid y ; s) &amp; =\int_{-\infty}^{+\infty} \phi_t(\Delta ; x) p(x ; t \mid y ; s) \mathrm{d} \Delta \ &amp; -\int_{-\infty}^{+\infty} \Delta \frac{\partial}{\partial x} \phi_t(\Delta ; x) p(x ; t \mid y ; s) \mathrm{d} \Delta \ &amp; +\int_{-\infty}^{+\infty} \frac{\Delta^2}{2} \frac{\partial^2}{\partial x^2} \phi_t(\Delta ; x) p(x ; t \mid y ; s) \mathrm{d} \Delta\end{aligned}$ Using the fact that $\phi_t$integrates to 1 over $\Delta$: $\begin{aligned} p(x ; t+\mathrm{d} t \mid y ; s)-p(x ; t \mid y ; s) &amp; =-\frac{\partial}{\partial x}\left(\mathbb{E}<em>{\Delta \sim \phi_t(; x)}[\Delta] p(x ; t \mid y ; s)\right) \ &amp; +\frac{1}{2} \frac{\partial^2}{\partial x^2}\left(\mathbb{E}</em>{\Delta \sim \phi_t(; x)}\left[\Delta^2\right] p(x ; t \mid y ; s)\right) \ &amp; \vdots\end{aligned}$ Define $\begin{aligned} \mathbb{E}<em>{\Delta \sim \phi_t(; x)}[\Delta] &amp; :=f(x, t) \mathrm{d} t \ \mathbb{E}</em>{\Delta \sim \phi_t(; x)}\left[\Delta^2\right] &amp; :=g^2(x, t) \mathrm{d} t\end{aligned}$ Dividing by $dt$on both the sides we get the partial differential equation: $\frac{\partial}{\partial t} p(x ; t \mid y ; s)=-\frac{\partial}{\partial x}(f(x, t) p(x ; t \mid y ; s))+\frac{1}{2} \frac{\partial^2}{\partial x^2}\left(g^2(x ; t) p(x ; t \mid y ; s)\right)$ $dX \sim \phi_t(;X)$, $E[dX] = f(X, t)dt$, $Var(dX) = g^2(X, t)dt - O(dt^2) \approx g^2(X, t) dt$ The function $f$is called drift coefficient of our diffusion coefficient, and $g$is our diffusion coefficient. $dX = f(X, t)dt + g(X, t) dw$, where $dw \sim D$such that $D$is sime distribution with a varience $dt$and a mean of $0$. This diffusion is called Ito diffusion SDE. We can always consider $D$to be Gaussian distribution, $\int_0^{\frac{1}{N}} dw \sim \mathcal{N}(0, \frac{1}{N})$ The unconditioned forward equation: $\frac{\partial}{\partial t} p(x ; t)=-\frac{\partial}{\partial x}(f(x, t) p(x ; t))+\frac{1}{2} \frac{\partial^2}{\partial x^2}\left(g^2(x ; t) p(x ; t)\right)$</p> <h2 id="the-backward-kolmogorov-equation">The Backward Kolmogorov Equation</h2> <p>$\begin{aligned} p(x ; t \mid y ; s-\mathrm{d} s) &amp; =\int_{-\infty}^{+\infty} \phi_{s-\mathrm{d} s}(\Delta ; y) p(x ; t \mid y+\Delta ; s) \mathrm{d} \Delta \ &amp; =\int_{-\infty}^{+\infty} \phi_{s-\mathrm{d} s}(\Delta ; y) p(x ; t \mid y ; s) \mathrm{d} \Delta \ &amp; +\int_{-\infty}^{+\infty} \phi_{s-\mathrm{d} s}(\Delta ; y) \Delta \frac{\partial}{\partial y} p(x ; t \mid y ; s) \mathrm{d} \Delta \ &amp; +\int_{-\infty}^{+\infty} \phi_{s-\mathrm{d} s}(\Delta ; y) \frac{\Delta^2}{2} \frac{\partial^2}{\partial y^2} p(x ; t \mid y ; s) \mathrm{d} \Delta \ &amp; \vdots\end{aligned}$ $-\frac{\partial}{\partial s} p(x ; t \mid y ; s)=f(y, s) \frac{\partial}{\partial y} p(x ; t \mid y ; s)+\frac{g^2(y ; s)}{2} \frac{\partial^2}{\partial y^2} p(x ; t \mid y ; s)$ Then the reverse SDE: $\mathrm{d} X=\left(f(X, t)-\frac{1}{p(X, t)} \frac{\partial}{\partial X} g^2(X, t) p(X, t)\right) \mathrm{d} t+g(X, t) \mathrm{d} w$ Proof see the original paper</p> <p><a href="https://www.vanillabug.com/posts/sde/">https://www.vanillabug.com/posts/sde/</a> <a href="https://ludwigwinkler.github.io/blog/FokkerPlanck/">https://ludwigwinkler.github.io/blog/FokkerPlanck/</a> <a href="https://ludwigwinkler.github.io/blog/Kramers/">https://ludwigwinkler.github.io/blog/Kramers/</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[What is the reversal of a diffusion process? Suppose you have an initial distribution $p(x;0)$and some drift and diffusion coefficients $f$and $g$. Together, they describe a distribution over the trajectories of your system through time Suppose at time $T$, the marginal distributionover the trajectories of your system through time. Suppose at time $T$, the margianl distribution of the states is $p(x; T)$. Can you describe a diffusion process such that it starts with a distribution $q(x; 0): = p(x; T)$and evolves such that the distribution of the trajectories is the same as in the original process but with the tiem reversed? Of course, in such a case we shall have $q(x; T) = p(x; 0)$. Non-linear case Ito SDE and reverse time SDE]]></summary></entry></feed>