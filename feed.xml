<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://szhan311.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://szhan311.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-15T06:49:55+00:00</updated><id>https://szhan311.github.io//feed.xml</id><title type="html">Shaorong Zhang</title><subtitle></subtitle><entry><title type="html">A Computational Framework For Solving Wassertein Lagrangian Flows</title><link href="https://szhan311.github.io//blog/2023/Wasserstein-Lagrangian-Flows/" rel="alternate" type="text/html" title="A Computational Framework For Solving Wassertein Lagrangian Flows"/><published>2023-12-13T20:36:00+00:00</published><updated>2023-12-13T20:36:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Wasserstein%20Lagrangian%20Flows</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Wasserstein-Lagrangian-Flows/"><![CDATA[<h1 id="preliminaries">Preliminaries</h1> <h2 id="machanics">Machanics</h2> <p>Curves: \(\gamma(t): [0:1] \rightarrow \mathcal{X}\); Velocity \(\dot{\gamma_t} \in \mathcal{T}_{\gamma(t)} \mathcal{X}\); Lagrangian function: \(L(\gamma_t \dot{\gamma_t}, t)\).</p> <p>Given two endpoints \(x_0, x_1 \in \mathcal{X}\), define the <em><strong>action</strong></em> as the time integral \(\mathcal{A}(\gamma)=\int_0^1 L\left(\gamma_t, \dot{\gamma}_t, t\right) d t\).</p> <p>Minimize the action along all curves: \(c\left(x_0, x_1\right)=\inf _{\gamma \in \Pi\left(x_0, x_1\right)} \mathcal{A}(\gamma)=\inf _{\gamma_t} \int_0^1 L\left(\gamma_t, \dot{\gamma}_t, t\right) d t \quad\).</p> <p>Lagrangian flows: we refer to the optimizing curves \(\gamma^*(x_0, x_0)\) as Lagrange flows, which satisfy the Euler-Lagrange equation \(\frac{d}{d t} \frac{\partial}{\partial \dot{\gamma}_t} L\left(\gamma_t, \dot{\gamma}_t, t\right)=\frac{d}{d \gamma_t} L\left(\gamma_t, \dot{\gamma}_t, t\right)\).</p> <p>Assume that \(L(\gamma_t, \dot{\gamma}_t, t)\) is strictly convex in the velocity \(\dot{\gamma}_t\), then we can obtain an equivalent, Hamiltonian perspective. Define the Hamiltonian \(H\left(\gamma_t, p_t, t\right)\) as the Legendre transform of \(L\) with respect to \(\dot{\gamma}_t\),</p> \[H\left(\gamma_t, p_t, t\right)=\sup _{\dot{\gamma}_t}\left\langle\dot{\gamma}_t, p_t\right\rangle-L\left(\gamma_t, \dot{\gamma}_t, t\right)\] <p>The Euler-Lagrange equations can be written as Hamiltion’s equation in phase space \(\dot{\gamma}_t=\frac{\partial}{\partial p_t} H\left(\gamma_t, p_t, t\right) \quad \dot{p}_t=-\frac{\partial}{\partial \gamma_t} H\left(\gamma_t, p_t, t\right)\)</p> <h2 id="basic-of-math">Basic of math</h2> <p>**Deﬁnition (Image measure or push forward measure): **take a map \(T:X \rightarrow Y\) and a probability measure \(\mu \in \mathcal{P} (Y)\) as: \(\left(T_{\#} \mu\right)(A):=\mu\left(T^{-1}(A)\right)\) for any \(A \in \mathcal{B}(Y)\)</p> <p><strong>Deﬁnition (Tangent space)</strong>: Given a point \(p \in M\), the tangent space \(T_p M \subset \mathbb{R}^D\) of \(M\) at \(p\) is defined as \(T_p M:=\{\dot{\gamma}(0) \mid \gamma:(-1,1) \rightarrow M, \gamma(0)=p\}\). Intuitively, the tangent space contains all the directions tangent to \(M\) at \(p\).</p> <p><strong>Deﬁnition (Riemannian distance)</strong>: Given two points \(x, y \in M\), their Riemannian distance \(d_{M}(x, y)\) is defined as \(d_M(x, y):=\inf \left\{\int_a^b \vert \dot{\gamma}(t) \vert d t \mid \gamma:[a, b] \rightarrow M, \gamma(a)=x, \gamma(b)=y\right\}\).</p> <p>\(X\) and \(Y\) are locally compact, separable, and complete metric spaces.</p> <p><strong>Deﬁnition (Minimizing geodesic)</strong>: A curve \(\gamma : [a,b] \rightarrow M\) with constant speed such that \(\gamma(a) = x, \gamma(b) = y\), and whose length is equal to \(d_M (x, y)\), is called a minimizing geodesic.</p> <h2 id="transport-maps--coupling">Transport maps &amp; coupling</h2> <p>**Definition (Transport map): **Given \(\mu \in \mathcal{P}(X)\) and \(v \in \mathcal{P}(Y)\), a map \(T: X \rightarrow Y\) is called a transport map from \(\mu\) to \(v\) if \(T_{\#} \mu = v\)</p> <p><strong>Definition (Coupling or transport plan):</strong> We call \(\gamma \in \mathcal{P}(X \times Y)\) a coupling of \(\mu\) and \(v\) if</p> <p>\(\left(\pi_X\right)_{\#} \gamma=\mu \quad \text { and } \quad\left(\pi_Y\right)_{\#} \gamma=v \text {, }\) where \(\pi_X(x, y)=x, \quad \pi_Y(x, y)=y \quad \forall(x, y) \in X \times Y\). We denote by \(\Gamma(\mu, v)\) the set of couplings of \(\mu\) and \(v\).</p> <p>**Remark: **Given \(\mu\) and \(v\), the set \(\Gamma(\mu, v)\) is always nonempty, indeed the product measure \(\gamma = \mu \otimes v\) (defined by \(\int \phi(x, y) d \gamma(x, y)=\int \phi(x, y) d \mu(x) d \nu(y)\)) for every \(\phi: X \times Y \rightarrow \mathbb{R}\) is a coupling.</p> <p><strong>Remark (Transport map vs. coupling)</strong>. Let \(T: X \rightarrow Y\) satisfy \(T_{\#} \mu=v\). Consider the map\(\operatorname{Id} \times T: X \rightarrow X \times Y\), i.e., \(x \mapsto(x, T(x))\), and define\(\gamma_T:=(\operatorname{Id} \times T)_{\#} \mu \in \mathcal{P}(X \times Y)\), we claim that \(\gamma_T \in \Gamma(\mu, v)\). This proves that any transport map \(T\) includes a coupling \(\gamma_T\).</p> <h2 id="optimal-transport">Optimal Transport</h2> <p>Two kinds of formulation: <strong>Monge</strong> formulation and <strong>Kantorovich</strong> formulation</p> <h1 id="sketch">Sketch</h1> <h2 id="how-to-calculate-the-action-of-rho_t">How to calculate the action of \(\rho_t\)</h2> <p>Given Lagrangian function \(\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]\), the action is \(A_L[\rho_t]=\int_0^1 \mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right] d t\)</p> <p>Recall the definition of the Legendre transform for \(\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]\) strictly convex in \(\dot{\rho}_t\),</p> \[\begin{aligned} &amp; \mathcal{H}\left[\rho_t, s_t, t\right]=\sup _{\dot{\rho}_t \in \mathcal{T}_{\rho_t} \mathcal{P}} \int \dot{\rho}_t s_t d x_t-\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right] \\ &amp; \mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]=\sup _{s_t \in \mathcal{T}_{\rho_t}^* \mathcal{P}} \int \dot{\rho}_t s_t d x_t-\mathcal{H}\left[\rho_t, s_t, t\right]\end{aligned}\] <p>The action \(A_L[\rho_t]\) is the solution to the inneroptimization</p> \[\mathcal{A}_{\mathcal{L}}\left[\rho_t\right]=\sup _{s_t} \int s_1 \mu_1 d x_1-\int s_0 \mu_0 d x_0-\int_0^1\left(\int \frac{\partial s_t}{\partial t} \rho_t d x_t+\mathcal{H}\left[\rho_t, s_t, t\right]\right) d t\] <h2 id="how-to-minimize-the-action">How to minimize the action</h2> <p>The optimization\(\mathcal{S}=\inf _{\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)} \mathcal{A}_{\mathcal{L}}\left[\rho_t\right]=\inf _{\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)} \int_0^1 \mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right] d t\) is equivalent to the following dual</p> \[\mathcal{S}=\inf _{\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)} \sup _{s_t} \int s_1 \mu_1 d x_1-\int s_0 \mu_0 d x_0-\int_0^1\left(\int \frac{\partial s_t}{\partial t} \rho_t d x_t+\mathcal{H}\left[\rho_t, s_t, t\right]\right) d t\] <p>So we have two goals</p> <ul> <li> <p><strong>Inner optimization:</strong> evaluating the action functional \(\mathcal{A}_{\mathcal{L}}\left[\rho_t\right]\) for a given curve \(\rho_t\)</p> </li> <li> <p>**Outer optimization: **optimizing the action over curves \(\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)\) satisfying the desired constraints</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/optimal%20transport/wlf-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/optimal%20transport/wlf-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/optimal%20transport/wlf-1400.webp"/> <img src="/assets/img/optimal%20transport/wlf.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hamiltonian-optimality-conditions">Hamiltonian optimality conditions</h2> <p>\(\frac{\partial \rho_t}{\partial t}=\frac{\delta}{\delta s_t} \mathcal{H}\left[\rho_t, s_t, t\right]\) and \(\frac{\partial s_t}{\partial t}=-\frac{\delta}{\delta \rho_t} \mathcal{H}\left[\rho_t, s_t, t\right]\)</p> <h1 id="a-special-class-of-lagrangians">A special class of Lagrangians</h1> <p>Definition 3.1 ((Dual) Linearizability). A Lagrangian \(\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]\) is dual linearizable if the corresponding Hamiltonian \(\mathcal{H}\left[\rho_t, s_t, t\right]\) can be written as a linear functional of the density \(\rho_t\). In other words, \(\mathcal{H}\left[\rho_t, s_t, t\right]\) is linearizable if there exist functions \(K^*\left(x_t, s_t, t\right)\), and \(U\left(x_t, s_t, t\right)\) such that \(\mathcal{H}\left[\rho_t, s_t, t\right]=\int\left(K^*\left(x_t, s_t, t\right)+U\left(x_t, s_t, t\right)\right) \rho_t d x_t\)</p> <h1 id="parameterization">Parameterization</h1> <p>We parameterize \(s_t\) as a neural network \(s_t(x, \theta)\), we parameterize the distribution path \(\rho_t(x, \eta)\) as a generative model \(x_t=(1-t) x_0+t x_1+t(1-t) \mathrm{NNET}\left(t, x_0, x_1, \eta\right), \quad x_0 \sim \mu_0, \quad x_1 \sim \mu_1\).</p> <h2 id="reparameterization-trick">Reparameterization trick</h2> <p>For linearizable dual objectives, we optimize</p> \[\begin{aligned} \operatorname{LOSS}(\theta, \eta)=\min _\eta \max _\theta \int s_1(x, \theta) \mu_1 d x_1-\int s_0(x, \theta) \mu_0 d x_0 \\ \quad-\int_0^1 \int\left(\frac{\partial s_t}{\partial t}(x, \theta)+K^*\left(s_t(x, \theta), t\right)+U\left(s_t(x, \theta), t\right)\right) \rho_t(x, \eta) d x d t\end{aligned}\] <p>Given this objective, we see that minimizing \(\eta\) and maximizing \(\theta\) are seperatable.</p> <p>An alternative to parametrizing the distribution path \(\rho_t\) is perform minimization of the LOSS via the Wasserstein gradient flow</p> \[x_t^{\prime}=x_t+\alpha \cdot t(1-t) \nabla_x\left[\frac{\partial s_t}{\partial t}\left(x_t, \theta\right)+K^*\left(s_t\left(x_t, \theta\right), t\right)+U\left(s_t\left(x_t, \theta\right), t\right)\right]\] <h2 id="algorithm">Algorithm</h2> <p><img src="https://cdn.nlark.com/yuque/0/2023/png/27584564/1699855255705-28c77fbc-0f50-4140-9ebe-fa296b5e2965.png#averageHue=%23eeeeee&amp;clientId=u65838f34-583e-4&amp;from=paste&amp;height=318&amp;id=J95gc&amp;originHeight=504&amp;originWidth=1177&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=143483&amp;status=done&amp;style=none&amp;taskId=u0e294a81-e410-487f-bb00-11fb2e0db96&amp;title=&amp;width=742.5" alt="image.png"/></p> <h1 id="wassertein-2-geometry">Wassertein-2 Geometry</h1> <h2 id="definition">Definition</h2> <p>Given two densities with finite second moments \(\mu_0, \mu_1 \in \mathcal{P}_2(\mathcal{X})\), the Wassertein-2 OT problem is defined , in the Kantorovich formulation, as a cost-minimization problem over joint distributions \(\pi \in \Pi\left(\mu_0, \mu_1\right)=\left\{\pi\left(x_0, x_1\right) \mid \int \pi\left(x_0, x_1\right) d x_1=\mu_0, \int \pi\left(x_0, x_1\right) d x_0=\mu_1\right\}\), i.e.</p> \[W_2\left(\mu_0, \mu_1\right)^2:=\inf _{\pi \in \Pi\left(\mu_0, \mu_1\right)} \int\left \| x_0-x_1\right \|^2 \pi\left(x_0, x_1\right) d x_0 d x_1\] <h2 id="an-alternative-perspective-dynamical-formulation">An alternative perspective (dynamical formulation)</h2> <p>An alternative perspective on the \(W_2\) OT problem as an optimization over a vector field \(v_t\) that transports samples according to an ODE \(\dot x_t = v_t\).</p> <p>The evolution of samples’ density \(\rho_t\), under transport by \(v_t\), is governed by the continuity equation \(\dot{\rho}_t = - \nabla \cdot (\rho_t v_t)\), and we have</p> <p>\(W_2\left(\mu_0, \mu_1\right)^2=\inf _{\rho_t} \inf _{v_t} \int_0^1 \int \frac{1}{2}\left\| v_t\right \|^2 \rho_t d x_t d t \quad\) s.t. \(\dot{\rho}_t=-\nabla \cdot\left(\rho_t v_t\right), \quad \rho_0=\mu_0, \rho_1=\mu_1\) where \(\nabla \cdot ()\) is the divergence operator.</p> <p>The \(W_2\) transport cost can be viewed as providing a Riemannian mannifold structure on the space of densities \(\mathcal{P}_2(\mathcal{X})\)</p> <h1 id="wassertein-fisher-rao-geometry">Wassertein Fisher-Rao Geometry</h1> <ul> <li>consider additional terms allowing for birth and death of particles</li> </ul> <p>Extending the continuity equation to include a ‘growth term’ \(g_t: \mathcal{X}, \rightarrow \mathbb{R}\), the objective is</p> \[W F R_\lambda\left(\mu_0, \mu_1\right)^2=\inf _{\rho_t} \inf _{v_t, g_t} \int_0^1 \int\left(\frac{1}{2}\left\|v_t\right\|^2+\frac{\lambda}{2} g_t^2\right) \rho_t d x_t d t\] <p>subject to \(\dot{\rho}_t=-\nabla \cdot\left(\rho_t v_t\right)+\lambda \rho_t g_t, \rho_0=\mu_0, \rho_1=\mu_1\) We call this the <em><strong>Wasserstein Fisher-Rao (WFR)</strong></em> distance, this problem is reffered as <strong>unbalanced OT problem</strong></p> <h2 id="a-riemanian-structure-on-mathcalmmathcalx">A Riemanian structure on \(\mathcal{M}(\mathcal{X})\)</h2> <p>A Riemanian structure on \(\mathcal{M}(\mathcal{X})\) via the WFR distance. Introducting Lagrange multipliers \(s_t\) and eliminating \(v_t\), \(g_t\) yields the optimality conditions \(v_t = \nabla s_t\) and \(g_t = s_t\). This suggests characterizing the tangent space via the tuple \(\left(s_t, \nabla s_t\right)\) and define the metric as \(\begin{aligned} T_\rho^{W F R_\lambda} \mathcal{M}(\mathcal{X}) &amp; =\{\dot{\rho} \mid \dot{\rho}=-\nabla \cdot(\rho \nabla s)+\lambda \rho s\} \\ \left\langle\dot{\mu}_t, \dot{\rho}_t\right\rangle_{T_\rho}^{W F R_\lambda} &amp; =\left\langle s_{\dot{\mu}_t}, s_{\dot{\rho}_t}\right\rangle_{T_\rho^*}^{W F R_\lambda}=\int\left(\left\langle\nabla s_{\dot{\mu}_t}, \nabla s_{\dot{\rho}_t}\right\rangle+\lambda s_{\dot{\mu}_t} s_{\dot{\rho}_t}\right) \rho d x .\end{aligned}\)</p> <h1 id="action-matching">Action Matching</h1> <p>The objective is \(\mathcal{A}\left[\mu_t\right]=\sup _{s_t} \int s_1 \mu_1 d x_1-\int s_0 \mu_0 d x_0-\int_0^1 \int\left(\frac{\partial s_t}{\partial t}+\frac{1}{2}\left\|\nabla s_t\right\|^2\right) \mu_t d x_t d t\) \(s_t: \mathcal{X}\times [0, 1] \rightarrow \mathbb{R}\) is parameterized by a neural network, with similar objective for \(WFR_{\lambda}\) we view Action Matching as maximizing a lower bound on the action \(\mathcal{A}[\mu_t]\) or kinetic energy of the curve \(\mu_t : [0, 1] \rightarrow \mathcal{P}_2(\mathcal{X})\) of densities. In particular, at those optimal \(s_{\dot{\mu_t}}\) satisfying \(\dot{\mu}_t=-\nabla \cdot\left(\mu_t \nabla s_{\dot{\mu}_t}\right)\), \(\mathcal{A}\left[\mu_t\right]=\int_0^1 \frac{1}{2}\left\langle\dot{\mu}_t, \dot{\mu}_t\right\rangle_{T_{\mu_t}}^{W_2} d t=\int_0^1 \frac{1}{2}\left\langle s_{\dot{\mu}_t}, s_{\dot{\mu}_t}\right\rangle_{T_{\mu_t}^*}^{W_2} d t=\int_0^1 \int \frac{1}{2}\left\|\nabla s_t\right\|^2 \mu_t d x_t d t\)</p> <h1 id="wassertein-lagrangian-flows">Wassertein Lagrangian Flows</h1> <h2 id="wassertein-lagrangian-and-hamiltonian-flows">Wassertein Lagrangian and Hamiltonian Flows</h2> \[\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]=\mathcal{K}\left[\rho_t, \dot{\rho}_t, t\right]-\mathcal{U}\left[\rho_t, t\right]\] <p>define the action of a curve: \(\mathcal{A}(\rho_t)=\int_0^1 L\left(\rho_t, \dot{\rho}_t, t\right) d t\).</p> <p>seek the action-minimizing curve subject to the constraints:</p> \[\begin{aligned} \mathcal{S}_{\mathcal{L}}\left(\left\{\mu_{t_i}\right\}_{i=0}^{M-1}\right) &amp; :=\inf _{\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)} \mathcal{A}_{\mathcal{L}}\left[\rho_t\right] \\ &amp; :=\inf _{\rho_t} \int_0^1 \mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right] d t \quad \text { s.t. } \rho_{t_i}=\mu_{t_i} \quad \forall 0 \leq i \leq M-1\end{aligned}\] <p>where \(\Gamma\left(\left\{\mu_{t_i}\right\}\right)=\left\{\rho_t:[0,1] \rightarrow \mathcal{P}(\mathcal{X}) \mid \rho_0=\mu_0, \rho_1=\mu_1, \rho_{t_i}=\mu_{t_i} \quad(\forall 1 \leq i \leq M-2)\right\}\)</p> <p>Define the Hamiltonian via Legendre transform:</p> \[\mathcal{H}\left[\rho_t, s_t, t\right]=\sup _{\dot{\rho}_t \in T_{\rho_t} \mathcal{P}} \int \dot{\rho}_t s_t d x_t-\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]=\mathcal{K}^*\left[\rho_t, s_t, t\right]+\mathcal{U}\left[\rho_t, t\right]\] <p>Finally, \(\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]\) can be written using the Legendre transform \(\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]=\sup _{s_t \in T_{\rho_t}^* \mathcal{P}} \int \dot{\rho}_t s_t d x_t-\mathcal{H}\left[\rho_t, s_t, t\right]\)</p> <p>The following theorem forms the basis for our computational approach</p> <p>Theorem 1. For a Lagrangian \(\mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right]\) which is lsc and strictly convex in \(\dot{\rho}_t\), the optimization</p> \[\mathcal{S}=\inf _{\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)} \mathcal{A}_{\mathcal{L}}\left[\rho_t\right]=\inf _{\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)} \int_0^1 \mathcal{L}\left[\rho_t, \dot{\rho}_t, t\right] d t\] <p>is equivalent to the following dual</p> \[\mathcal{S}=\inf _{\rho_t \in \Gamma\left(\left\{\mu_{t_i}\right\}\right)} \sup _{s_t} \int s_1 \mu_1 d x_1-\int s_0 \mu_0 d x_0-\int_0^1\left(\int \frac{\partial s_t}{\partial t} \rho_t d x_t+\mathcal{H}\left[\rho_t, s_t, t\right]\right) d t\] <p>Next, we have Wasserstein Hamiltonian flows satisfy the optimality conditions</p> <p>\(\frac{\partial \rho_t}{\partial t}=\frac{\delta}{\delta s_t} \mathcal{H}\left[\rho_t, s_t, t\right]\)and \(\frac{\partial s_t}{\partial t}=-\frac{\delta}{\delta \rho_t} \mathcal{H}\left[\rho_t, s_t, t\right]\)</p> <p>#</p>]]></content><author><name></name></author><category term="Optimal-transport"/><category term="Literature-review"/><summary type="html"><![CDATA[Preliminaries Machanics Curves: \(\gamma(t): [0:1] \rightarrow \mathcal{X}\); Velocity \(\dot{\gamma_t} \in \mathcal{T}_{\gamma(t)} \mathcal{X}\); Lagrangian function: \(L(\gamma_t \dot{\gamma_t}, t)\).]]></summary></entry><entry><title type="html">Denoising Diffusion Bridge Models</title><link href="https://szhan311.github.io//blog/2023/Denoising-Diffusion-Bridge-Models/" rel="alternate" type="text/html" title="Denoising Diffusion Bridge Models"/><published>2023-12-12T19:51:00+00:00</published><updated>2023-12-12T19:51:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Denoising%20Diffusion%20Bridge%20Models</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Denoising-Diffusion-Bridge-Models/"><![CDATA[<h1 id="prelimineries">Prelimineries</h1> <p>\(\mathbf{x}_0 \sim p_0(\mathbf{x}):=q_{\text {data }}(\mathbf{x})\) and \(\mathbf{x}_T \sim p_T(\mathbf{x}):=p_{\text {prior }}(\mathbf{x})\)</p> <p>Forward SDE: \(d \mathbf{x}_t=\mathbf{f}\left(\mathbf{x}_t, t\right) d t+g(t) d \mathbf{w}_t\)</p> <p>Reverse SDE: \(\left.d \mathbf{x}_t=\mathbf{f}\left(\mathbf{x}_t, t\right)-g(t)^2 \nabla_{\mathbf{x}_t} \log p\left(\mathbf{x}_t\right)\right) d t+g(t) d \mathbf{w}_t\)</p> <p>Reverse ODE: \(d \mathbf{x}_t=\left[\mathbf{f}\left(\mathbf{x}_t, t\right)-\frac{1}{g} g(t)^2 \nabla_{\mathbf{x}_t} \log p\left(\mathbf{x}_t\right)\right] d t\)</p> <p>The denoising score matching:</p> \[\mathcal{L}(\theta)=\mathbb{E}_{\mathbf{x}_t \sim p\left(\mathbf{x}_t \mid \mathbf{x}_0\right), \mathbf{x}_0 \sim q_{\text {data }}(\mathbf{x}), t \sim \mathcal{U}(0, T)}\left[\left\|\mathbf{s}_\theta\left(\mathbf{x}_t, t\right)-\nabla_{\mathbf{x}_t} \log p\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right\|^2\right]\] <h1 id="diffusion-process-with-fixed-endpoints">Diffusion Process with Fixed Endpoints</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion%20models/bridge01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion%20models/bridge01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion%20models/bridge01-1400.webp"/> <img src="/assets/img/diffusion%20models/bridge01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>a diffusion process \(d \mathbf{x}_t=\mathbf{f}\left(\mathbf{x}_t, t\right) d t+g(t) d \mathbf{w}_t\) can be driven to arrive at a particular point of intetest \(y \in \mathbb{R}^d\) almost surely via Doob’s \(h\)-transform: \(d \mathbf{x}_t= [\mathbf{f}\left(\mathbf{x}_t, t\right) +g(t)^2 \mathbf{h}\left(\mathbf{x}_t, t, y, T\right)]dt+g(t) d \mathbf{w}_t, \quad \mathbf{x}_0 \sim q_{\text {data }}(\mathbf{x}), \quad \mathbf{x}_T=y\)</p> </li> <li> <p>where \(\mathbf{h}(x, t, y, T)=\nabla_{\mathbf{x}_t} \log p\left(\mathbf{x}_T \mid \mathbf{x}_t\right) \vert_{\mathbf{x}_t=x, \mathbf{x}_T=y}\) is the gradient if the log transition kernel of from \(t\) to \(T\) generated by the original SDE, evaluated at points \(\mathbf{x}_t = \mathbf{x}\) and \(\mathbf{x}_T = y\), and each \(\mathbf{x}_t\) now explicitly depends on \(y\) at time \(T\).</p> </li> <li>**Theorem 1. **The evolution of conditional probability \(q(\mathbf{x}_t \vert \mathbf{x}_T)\) has a time-reversed SDE of the form\(d \mathbf{x}_t=\left[\mathbf{f}\left(\mathbf{x}_t, t\right)-g^2(t)\left(\mathbf{s}\left(\mathbf{x}_t, t, y, T\right)-\mathbf{h}\left(\mathbf{x}_t, t, y, T\right)\right)\right] d t+g(t) d \hat{\mathbf{w}}_t, \quad \mathbf{x}_T=y\)</li> <li>with an associated probability flow ODE \(d \mathbf{x}_t=\left[\mathbf{f}\left(\mathbf{x}_t, t\right)-g^2(t)\left(\frac{1}{2} \mathbf{s}\left(\mathbf{x}_t, t, y, T\right)-\mathbf{h}\left(\mathbf{x}_t, t, y, T\right)\right)\right] d t, \quad \mathbf{x}_T=y\)</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion%20models/bridge02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion%20models/bridge02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion%20models/bridge02-1400.webp"/> <img src="/assets/img/diffusion%20models/bridge02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Prelimineries \(\mathbf{x}_0 \sim p_0(\mathbf{x}):=q_{\text {data }}(\mathbf{x})\) and \(\mathbf{x}_T \sim p_T(\mathbf{x}):=p_{\text {prior }}(\mathbf{x})\)]]></summary></entry><entry><title type="html">Action matching</title><link href="https://szhan311.github.io//blog/2023/Action-Maching/" rel="alternate" type="text/html" title="Action matching"/><published>2023-12-11T19:23:00+00:00</published><updated>2023-12-11T19:23:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Action%20Maching</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Action-Maching/"><![CDATA[<p>Action matching: a method for learning population dynamics from samples of their temporal marginals The proposed method can learn dynamics which simulate an arbitrary path of marginal distributions, it can also be applied in the context of generative modeling.</p> <h1 id="continuity-equation">Continuity Equation</h1> <p>any <strong>continuous dynamics</strong> can be modeled by the <strong>continuity equation</strong>, and moreover any <strong>continuity equation</strong> results in a <strong>continuous dynamics</strong>.</p> <h2 id="state-space-to-density-space">State space to density space</h2> <ul> <li> <p>Suppose we have a set of particles in space \(\mathcal{X} \subset \mathbb{R}^d\), initially distributed as \(q_{t=0}\). Let each particle follow a timedependent ODE (continuous flow) with the velocity field \(v:[0,1] \times \mathcal{X} \rightarrow \mathbb{R}^d\) as follows</p> <ul> <li> \[\frac{d}{d t} x(t)=v_t(x(t)), \quad x(t=0)=x\] </li> <li> <p>The continuity equation describes how the density of the particles \(q_t\) evolves in time \(t\), i.e.,</p> </li> <li> \[\frac{\partial}{\partial t} q_t=-\nabla \cdot\left(q_t v_t\right)\] </li> </ul> </li> </ul> <h2 id="density-space-to-state-space">Density space to state space</h2> <ul> <li> <p><strong>Theorem 2.1</strong>. Consider a continuous dynamic with the density evolution of \(q_t\), which satisfies mild conditions (absolute continuty in the 2-Wassertein space of distributions \(\mathcal{P}_2(\mathcal{X})\). Then there exists a unique (up to a constant) function \(s_t^*(x)\), called the “action”, such that vector field \(v_t^*(x) = \nabla s_t^*(x)\) and \(q_t\) satisfies the continuity equation \(\frac{\partial}{\partial t} q_t=-\nabla \cdot\left(q_t \nabla s_t^*(x)\right)\).</p> </li> <li> <p>In other words, the ODE \(\frac{d}{d t} x(t)=\nabla s_t^*(x)\) can be used to move samples in time such that the marginals are \(q_t\).</p> </li> <li> <p>Using Theorem 2.1, the problem of learning the dynamics can be boiled down to learning the unique vector field \(\nabla s_t^*\)</p> </li> <li> <p>Based on Theorem 2.1, given density dynamic, we can learn a function \(s_t(x, \theta)\) to approximate \(s_t^*\). Then Given \(s_t(x, \theta)\), we can calculate the dynamic in the state space.</p> </li> </ul> <h1 id="action-matching">Action matching</h1> <h2 id="objective">Objective</h2> <p>Recover the true action \(s_t^*\) while having access only to samples from \(q_t\).</p> <p>The objective is to minimize the “ACTION-GAP” objective \(\operatorname{ACTION-GAP}\left(s, s^*\right):=\frac{1}{2} \int_0^1 \mathbb{E}_{q_t(x)}\left\|\nabla s_t(x)-\nabla s_t^*(x)\right\|^2 d t\)</p> <h2 id="tractable-objective">Tractable objective</h2> <p>Theorem 2.2. For an arbitrary variational action \(s\), the ACTION-GAP \(\left(s, s^*\right)\) can be decomposed as the sum of an intractable constant \(\mathcal{K}\), and a tractable term \(\mathcal{L}_{A M}(s)\)</p> \[\operatorname{ACTION-GAP}\left(s_t, s_t^*\right)=\mathcal{K}+\mathcal{L}_{\mathrm{AM}}\left(s_t\right) .\] <p>where \(\mathcal{L}_{\mathrm{AM}}(s)\) is the Action Matching objective, which we minimize</p> <p>\(\begin{aligned} \mathcal{L}_{\mathrm{AM}}(s) &amp; :=\mathbb{E}_{q_0(x)}\left[s_0(x)\right]-\mathbb{E}_{q_1(x)}\left[s_1(x)\right] \\ &amp; +\int_0^1 \mathbb{E}_{q_t(x)}\left[\frac{1}{2}\left\|\nabla s_t(x)\right\|^2+\frac{\partial s_t}{\partial t}(x)\right] d t \end{aligned}\) The term \(\mathcal{L}_{\mathrm{AM}}\) is tractable</p> <h2 id="connection-with-optimal-transport">Connection with Optimal Transport</h2> <p>The optimal dynamics of AM along the curve is also optimal in the sense of optimal transport with the 2-Wasserstein cost.</p> <p>the optimal vector field in the AM objective defines a mapping between two infinitesimally close distributions \(q_t\) and \(q_{t+h}\), which is of the form \(x \mapsto x+h \nabla s_t^*(x)\). This mapping is indeed the same as the Brenier map in optimal transport, which is of the form \(x \mapsto x+h \nabla \varphi_t(x)\), where \(\varphi_t\) is the (c-convex) Kantorovish potential.</p>]]></content><author><name></name></author><category term="Optimal-transport"/><category term="Literature-review"/><summary type="html"><![CDATA[Action matching: a method for learning population dynamics from samples of their temporal marginals The proposed method can learn dynamics which simulate an arbitrary path of marginal distributions, it can also be applied in the context of generative modeling.]]></summary></entry><entry><title type="html">Consistency models</title><link href="https://szhan311.github.io//blog/2023/Consistency-model/" rel="alternate" type="text/html" title="Consistency models"/><published>2023-12-10T20:23:00+00:00</published><updated>2023-12-10T20:23:00+00:00</updated><id>https://szhan311.github.io//blog/2023/Consistency%20model</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/Consistency-model/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Consistency model can be trained in two ways: <strong>distilling pre-trained diffusion models</strong>, <strong>as standalone generative models</strong></p> <h1 id="preliminaries">Preliminaries</h1> <h2 id="diffusion-models">Diffusion models</h2> <p>Let \(p_{\text{data}}(\mathbf{x})\) denote the data distribution. Diffusion models start by diffusing \(p_{\text{data}}(x)\) with a stochastic differential equation (SDE) \(\mathrm{d} \mathbf{x}_t=\boldsymbol{\mu}\left(\mathbf{x}_t, t\right) \mathrm{d} t+\sigma(t) \mathrm{d} \mathbf{w}_t\)</p> <p>where \(t \in [0, T], T &gt; 0\) is a fixed constant, \(\mu (\cdot, \cdot)\) and \(\sigma(\cdot)\) are the drift and diffusion coefficients respectively, and \(\{ \boldsymbol{\omega}_t\}_{t \in [0, T]}\) denotes the standard Brownian motion.</p> <p>We denote the distribution of \(\mathbf{x}_t\) as \(p_{t}(x)\) and as a result \(p_0(\mathbf{x}) \equiv p_{\text {data }}(\mathbf{x})\)</p> <p><strong>the Probability Flow (PF) ODE</strong>: \(\mathrm{d} \mathbf{x}_t=\left[\boldsymbol{\mu}\left(\mathbf{x}_t, t\right)-\frac{1}{2} \sigma(t)^2 \nabla \log p_t\left(\mathbf{x}_t\right)\right] \mathrm{d} t\)</p> <p>We adopt the setting: \(\boldsymbol{\mu}(\mathbf{x}, t)=\mathbf{0}\) and \(\sigma(t)=\sqrt{2 t}\), in this case, \(p_t(\mathbf{x})=p_{\text {data }}(\mathbf{x}) \otimes \mathcal{N}\left(\mathbf{0}, t^2 \boldsymbol{I}\right)\), where \(\otimes\) denotes the convolution operation, and \(\pi(\mathbf{x})=\mathcal{N}\left(\mathbf{0}, T^2 \boldsymbol{I}\right)\)</p> <p>For sampling, we first train a score model \(\boldsymbol{s}_\phi(\mathbf{x}, t) \approx \nabla \log p_t(\mathbf{x})\) via score matching, then we have the **empirical PF ODE: **\(\frac{\mathrm{d} \mathbf{x}_t}{\mathrm{~d} t}=-t s_\phi\left(\mathbf{x}_t, t\right)\)</p> <p>To solve this ODE, there are two main ways: <strong>numerical ODE solver **and **distillation techniques</strong></p> <h2 id="edm-denoiser">EDM Denoiser</h2> <h4 id="equation-2-mathbbe_boldsymboly-sim-p_text-data--mathbbe_boldsymboln-sim-mathcalnleftmathbf0-sigma2-mathbfirightdboldsymbolyboldsymboln--sigma-boldsymboly_22">Equation 2: \(\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}\|_2^2\)</h4> <p>The optimal analytically solution is \(D(\boldsymbol{x} ; \sigma) =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\)</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion%20models/consistency_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion%20models/consistency_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion%20models/consistency_01-1400.webp"/> <img src="/assets/img/diffusion%20models/consistency_01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Based on Fig. 1, it seems that we cannot get clean image from corrupted image in 1 step.</p> <h3 id="preconditioning-and-training">Preconditioning and training</h3> <p>In practice, instead of parameterize \(D_{\theta}\) directly, we train a different network \(F_{\theta}\) from which \(D_{\theta}\) is derived</p> \[D_\theta(\boldsymbol{x} ; \sigma)=c_{\text {skip }}(\sigma) \boldsymbol{x}+c_{\text {out }}(\sigma) F_\theta\left(c_{\text {in }}(\sigma) \boldsymbol{x} ; c_{\text {noise }}(\sigma)\right)\] <p>The loss is:</p> \[\mathbb{E}_{\sigma, \boldsymbol{y}, \boldsymbol{n}}[\underbrace{\lambda(\sigma) c_{\text {out }}(\sigma)^2}_{\text {effective weight }}\|\underbrace{F_\theta\left(c_{\text {in }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n}) ; c_{\text {noise }}(\sigma)\right)}_{\text {network output }}-\underbrace{\frac{1}{c_{\text {out }}(\sigma)}\left(\boldsymbol{y}-c_{\text {skip }}(\sigma) \cdot(\boldsymbol{y}+\boldsymbol{n})\right)}_{\text {effective training target }}\|_2^2]\] <h1 id="consistency-model">Consistency Model</h1> <h2 id="what-is-consistency-model">What is consistency model</h2> <p>Definition Given a solution trajectory \(\left\{\mathbf{x}_t\right\}_{t \in[\epsilon, T]}\) of the PF ODE\(\mathrm{d} \mathbf{x}_t=\left[\boldsymbol{\mu}\left(\mathbf{x}_t, t\right)-\frac{1}{2} \sigma(t)^2 \nabla \log p_t\left(\mathbf{x}_t\right)\right] \mathrm{d} t\), we define the consistency function as \(\boldsymbol{f}:\left(\mathbf{x}_t, t\right) \mapsto \mathbf{x}_\epsilon\). A consistency function has the property of self-consistency: it outputs are consistent for arbitrary pairs of \((\mathbf{x}_t, t)\) that belong to the same PF ODE trajectory, i.e., \(\boldsymbol{f}\left(\mathbf{x}_t, t\right)=\boldsymbol{f}\left(\mathbf{x}_{t^{\prime}}, t^{\prime}\right)\) for all \(t, t^{\prime} \in[\epsilon, T]\).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion%20models/consistency_02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion%20models/consistency_02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion%20models/consistency_02-1400.webp"/> <img src="/assets/img/diffusion%20models/consistency_02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>self-consistency</strong>: points on the same trajectory map to the same initial point.</p> <h2 id="parameterization">parameterization</h2> <p>parameterize the consistency model using skip connections:</p> \[\boldsymbol{f}_{\boldsymbol{\theta}}(\mathbf{x}, t)=c_{\text {skip }}(t) \mathbf{x}+c_{\text {out }}(t) F_{\boldsymbol{\theta}}(\mathbf{x}, t)\] <p>where \(c_{\text{skip}}(t)\) and \(c_{\text{out}}(t)\) are differeitiable functions such that \(c_{\text{skip}}(\epsilon) = 1\) and \(c_{\text{out}}(\epsilon) = 0\)</p> <h2 id="the-consistency-ditillation-loss">The consistency ditillation loss</h2> \[\begin{aligned} \mathcal{L}_{C D}^N\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{-} ; \boldsymbol{\phi}\right) &amp; := \\ &amp; \mathbb{E}\left[\lambda\left(t_n\right) d\left(\boldsymbol{f}_{\boldsymbol{\theta}}\left(\mathbf{x}_{t_{n+1}}, t_{n+1}\right), \boldsymbol{f}_{\boldsymbol{\theta}^{-}}\left(\hat{\mathbf{x}}_{t_n}^{\boldsymbol{\phi}}, t_n\right)\right)\right]\end{aligned}\]]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Introduction Consistency model can be trained in two ways: distilling pre-trained diffusion models, as standalone generative models]]></summary></entry><entry><title type="html">Phase Space Langevin Diffusion</title><link href="https://szhan311.github.io//blog/2023/PSLD/" rel="alternate" type="text/html" title="Phase Space Langevin Diffusion"/><published>2023-12-09T20:36:00+00:00</published><updated>2023-12-09T20:36:00+00:00</updated><id>https://szhan311.github.io//blog/2023/PSLD</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/PSLD/"><![CDATA[<h1 id="preliminaries">Preliminaries</h1> <h3 id="sgm">SGM</h3> <p>Forward process SDE for \(\mathbf{x}_t \in \mathbb{R}^d\): \(d\mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t) dt + \mathbf{G}(t) d \mathbf{w}_t\)</p> <p>The reverse SDE is given by:</p> \[d\mathbf{x}_t = (\mathbf{f} (\mathbf{x}_t, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla_{\mathbf{x}_t} \log \mathbf{p}_t(\mathbf{x}_t, t))dt + \mathbf{G}(t) d \bar {\mathbf{w}}_t\] <p>The denoising score matching:</p> \[\mathcal{L}(\theta)=\mathbb{E}_{\mathbf{x}_t \sim p\left(\mathbf{x}_t \mid \mathbf{x}_0\right), \mathbf{x}_0 \sim q_{\text {data }}(\mathbf{x}), t \sim \mathcal{U}(0, T)}\left[\left\|\mathbf{s}_\theta\left(\mathbf{x}_t, t\right)-\nabla_{\mathbf{x}_t} \log p\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right\|^2\right]\] <h3 id="sampling">Sampling</h3> <p>Given \(p_s(\mathbf{z}) \propto \exp(- H(\mathbf{z}))\), sampling SDE:</p> <ul> <li> \[d \mathbf{z}=\boldsymbol{f}(\mathbf{z}) d t+\sqrt{2 \boldsymbol{D}(\mathbf{z})} d \mathbf{w}_t\] </li> </ul> \[H(\mathbf{z})=H(\mathbf{x}, \mathbf{m})=U(\mathbf{x})+\frac{\mathbf{m}^T M^{-1} \mathbf{m}}{2}\] <ul> <li> \[\mathbf{f}(\mathbf{z})=-[\mathbf{D}(\mathbf{z})+\mathbf{Q}(\mathbf{z})] \nabla H(\mathbf{z})+\Gamma(\mathbf{z}), \quad \Gamma_i(\mathbf{z})=\sum_{j=1}^d \frac{\partial}{\partial \mathbf{z}_j}\left(\mathbf{D}_{i j}(\mathbf{z})+\mathbf{Q}_{i j}(\mathbf{z})\right)\] <ul> <li>\(\mathbf{D(z)}\): a <strong>positive semideﬁnite</strong> diffusion matrix; the strength of the Wienerprocess-driven diffusion.</li> <li>\(\mathbf{Q(z)}\) is a <strong>skew-symmetric</strong> curl matrix; the deterministic traversing effects.</li> <li>Adjuct \(\mathbf{D(z)}\)and \(\mathbf{Q(z)}\)to attain faster convergenve</li> </ul> </li> </ul> <h1 id="psld">PSLD</h1> \[p_s(\mathbf{z})=\mathcal{N}\left(\mathbf{x} ; \mathbf{0}_{d_x}, \boldsymbol{I}_{d_x}\right) \mathcal{N}\left(\mathbf{0}_{d_m}, M \boldsymbol{I}_{d_m}\right)\] <h3 id="forward-sde">Forward SDE</h3> <p>The choice of momentum \(\mathbf{m}_t\in \mathbb{R}^d\)</p> <p>The choice of \(\mathbf{D}\)and \(\mathbf{Q}\):</p> \[\boldsymbol{D}:=\frac{\beta}{2}\left(\left(\begin{array}{cc} \Gamma &amp; 0 \\ 0 &amp; M \nu \end{array}\right) \otimes \boldsymbol{I}_d\right), \boldsymbol{Q}:=\frac{\beta}{2}\left(\left(\begin{array}{cc} 0 &amp; -1 \\ 1 &amp; 0 \end{array}\right) \otimes \boldsymbol{I}_d\right)\] <p>Above \(\Gamma, \, M, \, v, \, \beta\)are positive scalars.</p> <p>The forward process is given by:</p> \[\begin{aligned} &amp; d \mathbf{z}_t=\boldsymbol{f}\left(\mathbf{z}_t\right) d t+\boldsymbol{G}(t) d \mathbf{w}_t, \\ &amp; \boldsymbol{f}\left(\mathbf{z}_t\right)=\left(\frac{\beta}{2}\left(\begin{array}{cc} -\Gamma &amp; M^{-1} \\ -1 &amp; -\nu \end{array}\right) \otimes \boldsymbol{I}_d\right) \mathbf{z}_t, \\ &amp; \boldsymbol{G}(t)=\sqrt{2 D\left(\mathbf{z}_t\right)}=\left(\begin{array}{cc} \sqrt{\Gamma \beta} &amp; 0 \\ 0 &amp; \sqrt{M \nu \beta} \end{array}\right) \otimes \boldsymbol{I}_d \\ &amp; \end{aligned}\] <h3 id="reverse-sde">Reverse SDE</h3> \[\begin{gathered} d \overline{\mathbf{z}}_t=\overline{\boldsymbol{f}}\left(\overline{\mathbf{z}}_t\right) d t+\boldsymbol{G}(T-t) d \overline{\mathbf{w}}_t \\ \overline{\boldsymbol{f}}\left(\overline{\mathbf{z}}_t\right)=\frac{\beta}{2}\left(\begin{array}{c} \Gamma \overline{\mathbf{x}}_t-M^{-1} \overline{\mathbf{m}}_t+\left.2 \Gamma \boldsymbol{s}_\theta\left(\overline{\mathbf{z}}_t, T-t\right)\right|_{0: d} \\ \left.\overline{\mathbf{x}}_t+\nu \overline{\mathbf{m}}_t+\left.2 M \nu \boldsymbol{s}_\theta\left(\overline{\mathbf{z}}_t, T-t\right)\right|_{d: 2 d}\right) \end{array}\right) \\ \boldsymbol{G}(T-t)=\left(\begin{array}{cc} \sqrt{\Gamma \beta} &amp; 0 \\ 0 &amp; \sqrt{M \nu \beta} \end{array}\right) \otimes \boldsymbol{I}_d \end{gathered}\] <h3 id="sde-solver-sscs">SDE solver: SSCS</h3> \[\begin{gathered} \left(\begin{array}{c} d \overline{\mathbf{x}}_t \\ d \overline{\mathbf{m}}_t \end{array}\right)=\mathcal{A}+\mathcal{S} \\ \mathcal{A}=\frac{\beta}{2}\left(\begin{array}{c} -\Gamma \overline{\mathbf{x}}_t-M^{-1} \overline{\mathbf{m}}_t \\ \overline{\mathbf{x}}_t-\nu \overline{\mathbf{m}}_t \end{array}\right) d t+\boldsymbol{G}(T-t) d \overline{\mathbf{w}}_t, \\ \mathcal{S}=\beta\left(\begin{array}{c} \Gamma \overline{\mathbf{x}}_t+\left.\Gamma \boldsymbol{s}_\theta\left(\overline{\mathbf{z}}_t, T-t\right)\right|_{0: d} \\ \nu \overline{\mathbf{m}}_t+\left.M \nu \boldsymbol{s}_\theta\left(\overline{\mathbf{z}}_t, T-t\right)\right|_{d: 2 d} \end{array}\right) d t \end{gathered}\] <h3 id="training-objectives">Training Objectives</h3> \[\min _{\boldsymbol{\theta}} \mathbb{E}_t \mathbb{E}_{p\left(\mathbf{z}_0\right)} \mathbb{E}_{p_t\left(\mathbf{z}_t \mid \mathbf{z}_0\right)}\left[\mathcal{L}_x\left(\boldsymbol{\theta}, \mathbf{z}_t, \mathbf{z}_0\right)+\mathcal{L}_m\left(\boldsymbol{\theta}, \mathbf{z}_t, \mathbf{z}_0\right)\right]\] \[\begin{aligned} \mathcal{L}_x &amp; =\Gamma \beta\left\|\left.\mathbf{s}_{\boldsymbol{\theta}}\left(\mathbf{z}_t, t\right)\right|_{0: d}-\nabla_{\mathbf{x}_t} \log p_t\left(\mathbf{z}_t \mid \mathbf{z}_0\right)\right\|_2^2, \\ \mathcal{L}_m &amp; =M \nu \beta\left\|\left.\mathbf{s}_{\boldsymbol{\theta}}\left(\mathbf{z}_t, t\right)\right|_{d: 2 d}-\nabla_{\mathbf{m}_t} \log p_t\left(\mathbf{z}_t \mid \mathbf{z}_0\right)\right\|_2^2,\end{aligned}\]]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Preliminaries]]></summary></entry><entry><title type="html">Where to diffuse, how to diffuse, and how to get back</title><link href="https://szhan311.github.io//blog/2023/MDM/" rel="alternate" type="text/html" title="Where to diffuse, how to diffuse, and how to get back"/><published>2023-12-08T20:36:00+00:00</published><updated>2023-12-08T20:36:00+00:00</updated><id>https://szhan311.github.io//blog/2023/MDM</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/MDM/"><![CDATA[<h1 id="background">Background</h1> <ul> <li> <p>The choice of this inference process affects both <strong>likelihoods and sample quality</strong>.</p> </li> <li> <p>On different datasets and models, different inference processes work better;</p> </li> <li> <p>A natural question: are there other auxiliary variable diffusions that would lead to improvements like CLD?</p> </li> <li> <p>**Auxiliary variables **have improved other generative models and inferences</p> <ul> <li>such as normalizing ﬂows, neural ordinary differential equations (ODEs), hierarchical variational models, ladder variational autoencoder</li> </ul> </li> </ul> <h1 id="requirement-of-design-a-diffusion-model">Requirement of design a diffusion model</h1> <ul> <li>Selecting an inference and model process pair <ul> <li>such that the inference process converges to the model prior</li> </ul> </li> <li> <p>Deriving the ELBO for this pair</p> </li> <li>Estimating the ELBO and its gradients <ul> <li>by deriving and computing the inference process’ transition kernel <h1 id="work-of-this-paper">Work of this paper</h1> </li> </ul> </li> <li> <p>provide a recipe for training MDMs beyond speciﬁc instantiations to all linear inference processes that have a stationary distribution, <strong>with any number of auxiliary variables</strong>.</p> </li> <li> <p>Using results from gradient-based Markov chain Monte Carlo (MCMC) to construct MDMs.</p> </li> <li> <p>derive the MDM ELBO.</p> </li> <li>the transition kernel of linear MDMs.</li> </ul> <h1 id="elbo-bound">ELBO bound</h1> <p>Reverse process: \(d \mathbf{z}=h_\theta(\mathbf{z}, t) d t+\beta_\theta(t) d \mathbf{B}_t, \quad t \in[0, T]\)</p> <p>Forward process: \(d \mathbf{y}=f_\phi(\mathbf{y}, s) d s+g_\phi(s) d \widehat{\mathbf{B}}_s, \quad s \in[0, T]\) \(\mathbf{z}_T\) approximates the data \(x \sim q_{\text{data}}\)</p> <p>When the model take the form</p> \[d \mathbf{z}=\left[g_\phi^2(T-t) s_\theta(\mathbf{z}, T-t)-f_\phi(\mathbf{z}, T-t)\right] d t+g_\phi(T-t) d \mathbf{B}_t\] <p>The ELBO is: \(\log p_\theta(x) \geq \mathcal{L}^{\mathrm{ism}}(x)=\mathbb{E}_{q_\phi(\mathbf{y} \mid x)}\left[\log \pi_\theta\left(\mathbf{y}_T\right)+\int_0^T-\frac{1}{2}\left\|s_\theta\right\|_{g_\phi^2}^2-\nabla \cdot\left(g_\phi^2 s_\theta-f_\phi\right) d s\right]\)</p> <p>where \(f_{\phi}, g_{\phi}, s_{\theta}\) are evaluated at \((\mathbf{y}_s, s)\), \(\|\mathbf{x}\|_{\mathbf{A}}^2=\mathbf{x}^{\top} \mathbf{A} \mathbf{x}\) and \(g^2 = gg^T\)</p> <p>ISM means Implicit Score Matching loss, which can be re-written as an ELBO \(L^{\text{dsm}}\) featuring Denoisng Score Matching (DSM).</p> <h1 id="multivariate-model-and-inference">Multivariate Model and Inference</h1> <p>\(\mathbf{u}=\left[\begin{array}{c}\mathbf{z}_t \\ \mathbf{v}_t\end{array}\right]\) \(\mathbf{u}_0 \sim \pi_\theta, \quad d \mathbf{u}=h_\theta\left(\mathbf{u}_t, t\right) d t+\beta_\theta(t) d \mathbf{B}_t\)</p>]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">Score-based generative modeling with critically-damped Langevin diffusion</title><link href="https://szhan311.github.io//blog/2023/CLD/" rel="alternate" type="text/html" title="Score-based generative modeling with critically-damped Langevin diffusion"/><published>2023-12-07T19:23:00+00:00</published><updated>2023-12-07T19:23:00+00:00</updated><id>https://szhan311.github.io//blog/2023/CLD</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/CLD/"><![CDATA[<h1 id="ideas">Ideas</h1> <ul> <li> <p>The diffusion process is the key to improve <strong>synthesis quality</strong> or** sampling speed**.</p> </li> <li> <p>Inspired by <strong>statistical mechanics.</strong></p> </li> <li> <p>The score of the conditional distribution \(p_t(\boldsymbol{v}_t \vert \boldsymbol{x}_t)\) is an arguably easier task than learning the score of \(p_t(\boldsymbol{x}_t)\).</p> </li> </ul> <h1 id="background">Background</h1> <ul> <li> <p>The forward process: \(d \mathbf{u}_t=\boldsymbol{f}\left(\mathbf{u}_t, t\right) d t+\boldsymbol{G}\left(\mathbf{u}_t, t\right) d \mathbf{w}_t, \quad t \in[0, T]\)</p> </li> <li> <p>The backward process: \(d \overline{\mathbf{u}}_t=\left[-\boldsymbol{f}\left(\overline{\mathbf{u}}_t, T-t\right)+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) \boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right)^{\top} \nabla_{\overline{\mathbf{u}}_t} \log p_{T-t}\left(\overline{\mathbf{u}}_t\right)\right] d t+\boldsymbol{G}\left(\overline{\mathbf{u}}_t, T-t\right) d \mathbf{w}_t\)</p> </li> <li> <p>Currently used SDEs have drift and diffusion coefficients of the symple form: \(\boldsymbol{f}\left(\mathbf{x}_t, t\right)=f(t) \mathbf{x}_t\) and \(\boldsymbol{G}\left(\mathbf{x}_t, t\right)=g(t) \boldsymbol{I}_d\)</p> </li> <li> <p>Generally, setting \(p\left(\mathbf{u}_0\right)=p_{\text {data }}(\mathbf{x})\), \(p\left(\mathbf{u}_T\right)= \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}_d)\).</p> </li> <li> <p>If \(\boldsymbol{f}\) and \(\boldsymbol{G}\) take the simple form above, the denoising score matching objective is: \(\min _{\boldsymbol{\theta}} \mathbb{E}_{t \sim \mathcal{U}[0, T]} \mathbb{E}_{\mathbf{x}_0 \sim p\left(\mathbf{x}_0\right)} \mathbb{E}_{\mathbf{x}_t \sim p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left\|\mathbf{s}_{\boldsymbol{\theta}}\left(\mathbf{x}_t, t\right)-\nabla_{\mathbf{x}_t} \log p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\right\|_2^2\right]\)</p> </li> <li> <p>If \(\boldsymbol{f}\) and \(\boldsymbol{G}\) are affine, the conditional distribution \(p_t\left(\mathbf{x}_t \mid \mathbf{x}_0\right)\) is Normal and avaiable analytically.</p> </li> </ul> <h1 id="critically-damped-langevin-dynamic">Critically-damped Langevin Dynamic</h1> <ul> <li>We propose to augment the data \(\mathbf{x}_t \in \mathbb{R}^d\) and \(\mathbf{v}_t \in \mathbb{R}^d\). With \(\mathbf{u}_t=\left(\mathbf{x}_t, \mathbf{v}_t\right)^{\top} \in \mathbb{R}^{2 d}\), we set</li> <li> \[\boldsymbol{f}\left(\mathbf{u}_t, t\right):=\left(\left(\begin{array}{cc}0 &amp; \beta M^{-1} \\ -\beta &amp; -\Gamma \beta M^{-1}\end{array}\right) \otimes \boldsymbol{I}_d\right) \mathbf{u}_t, \quad \boldsymbol{G}\left(\mathbf{u}_t, t\right):=\left(\begin{array}{cc}0 &amp; 0 \\ 0 &amp; \sqrt{2 \Gamma \beta}\end{array}\right) \otimes \boldsymbol{I}_d\] </li> <li> <p>The coupled SDE that describes the diffusion process: \(\left(\begin{array}{l}d \mathbf{x}_t \\ d \mathbf{v}_t\end{array}\right)=\underbrace{\left(\begin{array}{c}M^{-1} \mathbf{v}_t \\ -\mathbf{x}_t\end{array}\right) \beta d t}_{\text {Hamiltonian component }=: H}+\underbrace{\left(\begin{array}{c}\mathbf{0}_d \\ -\Gamma M^{-1} \mathbf{v}_t\end{array}\right) \beta d t+\left(\begin{array}{c}0 \\ \sqrt{2 \Gamma \beta}\end{array}\right) d \mathbf{w}_t}_{\text {Ornstein-Uhlenbeck process=:O }}\)</p> </li> <li> <p>The mass \(M \in \mathbb{R}^+\) is a hyperparameter that determines the coupling between the \(\mathbf{x}_t\)and \(\mathbf{v}_t\) variables.</p> </li> <li> <p>\(\beta \in \mathbb{R}^+\) is a constant time rescaling chosen such that the diffusion <strong>converges to its equilibrium distribution</strong> (we found constant \(\beta\)’s to work well).</p> </li> <li> <p>\(\Gamma \in \mathbb{R}^+\) is a friction coefficient that determines <strong>the strength of the noise injection</strong> into the velocities.</p> </li> <li> <p>The Hamiltonian component plays a role to <strong>accelerate sampling</strong> and <strong>efﬁciently explore complex probability distributions</strong>.</p> </li> <li> <p>The \(O\) term corresponds to an <strong>Ornstein-Uhlenbeck process</strong> in the velocity, which injects noise such that the diffusion dynamics properly converge to equilibrium for any \(\Gamma &gt; 0\).</p> </li> <li> <p>It can be shown that the equilibrium distribution of this diffusion is \(p_{\mathrm{EQ}}(\mathbf{u})=\mathcal{N}\left(\mathbf{x} ; \mathbf{0}_d, \boldsymbol{I}_d\right) \mathcal{N}\left(\mathbf{v} ; \mathbf{0}_d, M \boldsymbol{I}_d\right)\)</p> </li> <li> <p>The balance between \(M\) and \(\Gamma\)</p> <ul> <li> <p>For \(\Gamma^2 &lt; 4M\)(underdamped Langevin dynamics): oscillatory dynamics of \(\mathbf{x}_t\) and \(\boldsymbol{v}_t\) that slow down converge to equilibrium.</p> </li> <li> <p>For \(\Gamma^2 &gt; 4M\)(overdamped Langevin dynamics): the \(O\) term dominates wihci also slows down convergence.</p> </li> <li> <p>For \(\Gamma^2 = 4M\)(critically-damped Langevin dynamics): an ideal balance is achieved and convergence to \(p_{\mathrm{EQ}}(\mathbf{u})\) as fast as possible in a smooth manner without oscillations.</p> </li> </ul> </li> </ul> <h1 id="score-matching-objective">Score Matching Objective</h1> <ul> <li> <p>we initialize the joint \(\bar{p}\left(\mathbf{u}_0\right)=p\left(\mathbf{x}_0\right) p\left(\mathbf{v}_0\right)=p_{\text {data }}\left(\mathbf{x}_0\right) \mathcal{N}\left(\mathbf{v}_0 ; \mathbf{0}_d, \gamma M \boldsymbol{I}_d\right)\) with hyperparameter \(\gamma &lt; 1\)</p> </li> <li> <p>let the distribution diffuse towards the tractable equilibrium—or prior—distribution \(p_{\mathrm{EQ}}(\mathbf{u})\).</p> </li> <li> <p>The score matching (SM) objective: \(\min _{\boldsymbol{\theta}} \mathbb{E}_{t \sim \mathcal{U}[0, T]} \mathbb{E}_{\mathbf{u}_t \sim p_t\left(\mathbf{u}_t\right)}\left[\lambda(t)\left\|s_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)-\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t\right)\right\|_2^2\right]\)</p> </li> <li> \[\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t\right)=\nabla_{\mathbf{v}_t}\left[\log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)+\log p_t\left(\mathbf{x}_t\right)\right]=\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)\] </li> <li> <p>Why \(p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)\) is eaiser to learn</p> <ul> <li> <p>our velocity distribution is initialized from a simple Normal distribution, such that \(p_t\left(\mathbf{v}_t \mid \mathbf{x}_t\right)\) is closer to a Normal distribution for all \(t \geq 0\) than \(p_t\left( \mathbf{x}_t\right)\) itself.</p> </li> <li> <p>empirically verify the reduced complexity</p> </li> </ul> </li> </ul> <h1 id="hybrid-score-matching">Hybrid score matching</h1> <ul> <li>Objective: \(\min _{\boldsymbol{\theta}} \mathbb{E}_{t \in[0, T]} \mathbb{E}_{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}_{\mathbf{u}_t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left\|s_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)-\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)\right\|_2^2\right]\)</li> </ul> <h1 id="score-model-parameterization">Score Model Parameterization</h1> <ul> <li>\(\mathbf{u}_t=\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}_{2 d}\), where \(\boldsymbol{\Sigma}_t=\boldsymbol{L}_t \boldsymbol{L}_t^{\top}\) is the Cholesky decomposition of \(p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)\)’s covariance matrix, \(\boldsymbol{\epsilon}_{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}_{2 d} ; \mathbf{0}_{2 d}, \boldsymbol{I}_{2 d}\right)\), and \(\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)\) is \(p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)\)’s mean.</li> <li> \[\nabla_{\mathbf{v}_t} \log p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)=-\ell_t \boldsymbol{\epsilon}_{d: 2 d}\] </li> <li> <p>With \(\boldsymbol{\Sigma}_t=\underbrace{\left(\begin{array}{cc}\Sigma_t^{x x} &amp; \Sigma_t^{x v} \\ \Sigma_t^{x v} &amp; \Sigma_t^{v v}\end{array}\right)}_{\text {"per-dimension" covariance matrix }} \otimes \boldsymbol{I}_d, \quad\) we have \(\quad \ell_t:=\sqrt{\frac{\Sigma_t^{x x}}{\Sigma_t^{x x} \Sigma_t^{v v}-\left(\Sigma_t^{x v}\right)^2}}\)</p> </li> <li> <p>We parameterize \(s_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)=-\ell_t \alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)\) with \(\alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)=\ell_t^{-1} \mathbf{v}_t / \Sigma_t^{v v}+ \alpha'_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)\), where \(\Sigma_t^{vv}\) corresponds to the \(v-v\) component of the “per-dimension” covariance matrix of the Normal Distribution \(p_t\left(\mathbf{u}_t \mid \mathbf{x}_0=\mathbf{0}_d\right)\)</p> </li> <li> \[\operatorname{HSM}(\lambda(t))=\mathbb{E}_{t \sim \mathcal{U}[0, T], \mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right), \mathbf{u}_t \sim p_t\left(\mathbf{u}_t \mid \mathbf{x}_0\right)}\left[\lambda(t)\left(\ell_t^{\mathrm{HSM}}\right)^2\left\|\boldsymbol{\epsilon}_{d: 2 d}-\alpha_{\boldsymbol{\theta}}\left(\mathbf{u}_t, t\right)\right\|_2^2\right]\] </li> <li>Training objective: \(\min _{\boldsymbol{\theta}} \mathbb{E}_{t \sim \mathcal{U}[0, T]} \mathbb{E}_{\mathbf{x}_0 \sim p_0\left(\mathbf{x}_0\right)} \mathbb{E}_{\boldsymbol{\epsilon}_{2 d} \sim \mathcal{N}\left(\boldsymbol{\epsilon}_{2 d} ; \mathbf{0}_{2 d}, \boldsymbol{I}_{2 d}\right)}\left[\lambda(t) \ell_t^2\left\|\boldsymbol{\epsilon}_{d: 2 d}-\alpha_{\boldsymbol{\theta}}\left(\boldsymbol{\mu}_t\left(\mathbf{x}_0\right)+\boldsymbol{L}_t \boldsymbol{\epsilon}_{2 d}, t\right)\right\|_2^2\right]\)</li> </ul>]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Ideas]]></summary></entry><entry><title type="html">Elucidating the Design Space of Diffusion-Based Generative Models</title><link href="https://szhan311.github.io//blog/2023/EDM/" rel="alternate" type="text/html" title="Elucidating the Design Space of Diffusion-Based Generative Models"/><published>2023-12-06T20:36:00+00:00</published><updated>2023-12-06T20:36:00+00:00</updated><id>https://szhan311.github.io//blog/2023/EDM</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/EDM/"><![CDATA[<h1 id="original-ode--sde-formulation-from-previous-work">Original ODE / SDE formulation from previous work</h1> <ul> <li>Song et al. define their forward SDE as: \(dx = f(x,t)dt + g(t) dw_t\), where \(f(\cdot, t):\mathbb{R}^d \rightarrow \mathbb{R}^d\)and \(g(\cdot): \mathbb{R} \rightarrow \mathbb{R}\)are the drift and diffusion coefficients, repsectively, where \(d\)is the dimensionality of the dataset. \(f(\cdot)\)is always of the form</li> </ul> <p>\(f(x, t) = f(t) x\), where \(f(\cdot):\mathbb{R} \rightarrow \mathbb{R}.\)</p> <ul> <li> <p>Thus, the SDE can be equivalently written as\(dx = f(t)x + g(t) d w_t\).</p> </li> <li> <p>The pertubation kernels of this SDE have the general form \(p_{0t}(x(t) \vert x(0)) = \mathcal{N} (x(t); s(t)x(0), s^2(t) \sigma^2(t)\mathbf{I})\),</p> </li> <li> <p>where \(\mathcal{N}(x; \mu, \Sigma)\)denotes the probability density function of \(\mathcal{N}(\mu, \Sigma)\)evaluated at \(x\),</p> </li> </ul> \[s(t)=\exp \left(\int_0^t f(\xi) \mathrm{d} \xi\right), \quad \text { and } \quad \sigma(t)=\sqrt{\int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi}\] <ul> <li>The marginal distribution of \(p_t(x)\)is</li> </ul> \[p_t(\boldsymbol{x})=\int_{\mathbb{R}^d} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}_0\right) p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0\] <ul> <li>The probability flow ODE</li> </ul> \[\mathrm{d} \boldsymbol{x}=\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})\right] \mathrm{d} t .\] <h1 id="understanding-the-diffusion-process">Understanding the diffusion process</h1> <h3 id="idea">Idea</h3> <p>\(f\)and \(g\)are of little practical interest, the marginal distribution are of utmost inportance</p> <h3 id="the-perturbation-kernels-and-marginal-distribution">The perturbation kernels and marginal distribution</h3> \[p_{0 t}(\boldsymbol{x}(t) \mid \boldsymbol{x}(0))=\mathcal{N}\left(\boldsymbol{x}(t) ; s(t) \boldsymbol{x}(0), s(t)^2 \sigma(t)^2 \mathbf{I}\right)\] \[p_t(\boldsymbol{x})=\int_{\mathbb{R}^d} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}_0\right) p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0\] <h3 id="convolution-form">Convolution form</h3> <p>\(p(\boldsymbol{x} ; \sigma) :=p_{\text {data }} * \mathcal{N}(\mathbf{0}, \sigma(t)^2 \mathbf{I})\) \(p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))\)</p> <h3 id="ode">ODE</h3> <h4 id="mathrmd-boldsymbolx-dotsigmat-sigmat-nabla_boldsymbolx-log-pboldsymbolx--sigmat-mathrmd-t">\(\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t\)</h4> \[\mathrm{d} \boldsymbol{x}=\left[\frac{\dot{\boldsymbol{s}}(t)}{s(t)} \boldsymbol{x}-s(t)^2 \dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p\left(\frac{\boldsymbol{x}}{s(t)} ; \sigma(t)\right)\right] \mathrm{d} t\] <h3 id="denosing-score-matching">Denosing score matching</h3> <p>\(D(\boldsymbol{x}; \sigma)\)is a denoiser function that minimizes the expeced \(L_2\)denosing error for samples drawn from \(p_{\text{data}}\),</p> <p>\(\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}\|_2^2\), then \(\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2\)</p> <h3 id="heat-equation">Heat equation</h3> <p>$$\left{\begin{matrix}</p> <p>&amp;\frac{\partial q}{\partial t} -\dot{\sigma} \sigma \Delta_{\boldsymbol{x}} q = 0 &amp;\text{in} \quad \mathbb{R}^d \times(0, \, \infty) \ &amp;q= p_{data}(\boldsymbol{x}) &amp; \text{on} \quad \mathbb{R}^d \times {t=0}</p> <p>\end{matrix} \right.$$</p> <h3 id="sde">SDE</h3> <p>\(\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t\)</p> \[\mathrm{d} \boldsymbol{x}_{ \pm}=\underbrace{-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t}_{\text {probability flow ODE}} \pm \underbrace{\beta(t) \sigma(t)^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t}_{\text {deterministic noise decay }}+\underbrace{\sqrt{2 \beta(t)} \sigma(t) \mathrm{d} \omega_t}_{\text {noise injection }}\] <h1 id="ode-formulation">ODE formulation</h1> <h4 id="equation-1-mathrmd-boldsymbolx-dotsigmat-sigmat-nabla_boldsymbolx-log-pboldsymbolx--sigmat-mathrmd-t">Equation 1: \(\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t\)</h4> <h4 id="proof">Proof</h4> <p>The marginal distrobution</p> \[\begin{aligned} p_t(\boldsymbol{x}) &amp; =\int_{\mathbb{R}^d} p_{0 t}\left(\boldsymbol{x} \mid \boldsymbol{x}_0\right) p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_0 \\ &amp; =\int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right)\left[\mathcal{N}\left(\boldsymbol{x} ; s(t) \boldsymbol{x}_0, s(t)^2 \sigma(t)^2 \mathbf{I}\right)\right] \mathrm{d} \boldsymbol{x}_0 \\ &amp; =\int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right)\left[s(t)^{-d} \mathcal{N}\left(\boldsymbol{x} / s(t) ; \boldsymbol{x}_0, \sigma(t)^2 \mathbf{I}\right)\right] \mathrm{d} \boldsymbol{x}_0 \\ &amp; =s(t)^{-d} \int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathcal{N}\left(\boldsymbol{x} / s(t) ; \boldsymbol{x}_0, \sigma(t)^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \\ &amp; =s(t)^{-d}\left[p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)\right](\boldsymbol{x} / s(t)), \end{aligned}\] <p>where \(p_a * p_b\)denotes the convolution of probability density functions \(p_a\)and \(p_b\)</p> <p>Let denote:</p> \[p(\boldsymbol{x} ; \sigma)=p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right) \quad \text { and } \quad p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))\] <p>the probability ﬂow ODE</p> \[\begin{aligned} \mathrm{d} \boldsymbol{x} &amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log \left[p_t(\boldsymbol{x})\right]\right] \mathrm{d} t \\ &amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log \left[s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))\right]\right] \mathrm{d} t \\ &amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2\left[\nabla_{\boldsymbol{x}} \log s(t)^{-d}+\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right]\right] \mathrm{d} t \\ &amp; =\left[f(t) \boldsymbol{x}-\frac{1}{2} g(t)^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t . \end{aligned}\] <p>Next, \(f(t) = {\dot s(t)}/{s(t)}\), \(g(t) = s(t) \sqrt{2 \dot{\sigma}(t) \sigma(t)}\)</p> \[\begin{aligned} \exp \left(\int_0^t f(\xi) \mathrm{d} \xi\right) &amp; =s(t) \\ \int_0^t f(\xi) \mathrm{d} \xi &amp; =\log s(t) \\ \mathrm{d}\left[\int_0^t f(\xi) \mathrm{d} \xi\right] / \mathrm{d} t &amp; =\mathrm{d}[\log s(t)] / \mathrm{d} t \\ f(t) &amp; =\dot{s}(t) / s(t) . \end{aligned}\] \[\begin{aligned} \sqrt{\int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi} &amp; =\sigma(t) \\ \int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi &amp; =\sigma(t)^2 \\ \mathrm{~d}\left[\int_0^t \frac{g(\xi)^2}{s(\xi)^2} \mathrm{~d} \xi\right] / \mathrm{d} t &amp; =\mathrm{d}\left[\sigma(t)^2\right] / \mathrm{d} t \\ g(t)^2 / s(t)^2 &amp; =2 \dot{\sigma}(t) \sigma(t) \\ g(t) / s(t) &amp; =\sqrt{2 \dot{\sigma}(t) \sigma(t)} \\ g(t) &amp; =s(t) \sqrt{2 \dot{\sigma}(t) \sigma(t)} . \end{aligned}\] <p>Finally</p> \[\begin{aligned} \mathrm{d} \boldsymbol{x} &amp; =\left[[f(t)] \boldsymbol{x}-\frac{1}{2}[g(t)]^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t \\ &amp; =\left[[\dot{s}(t) / s(t)] \boldsymbol{x}-\frac{1}{2}[s(t) \sqrt{2 \dot{\sigma}(t) \sigma(t)}]^2 \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t \\ &amp; =\left[[\dot{s}(t) / s(t)] \boldsymbol{x}-\frac{1}{2}\left[2 s(t)^2 \dot{\sigma}(t) \sigma(t)\right] \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} / s(t) ; \sigma(t))\right] \mathrm{d} t \\ &amp; =\left[\frac{\dot{s}(t)}{s(t)} \boldsymbol{x}-s(t)^2 \dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p\left(\frac{\boldsymbol{x}}{s(t)} ; \sigma(t)\right)\right] \mathrm{d} t \end{aligned}\] <p>By setting \(s(t) = 1\):</p> \[\mathrm{d} \boldsymbol{x}=-\dot{\sigma}(t) \sigma(t) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t\] <h1 id="denoising-score-matching">Denoising score matching</h1> <h4 id="equation-2--3">Equation 2 &amp; 3</h4> <p>\(\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}\|_2^2\), then \(\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2\)</p> <h4 id="proof-1">Proof</h4> <p>Finite number of samples \(\{\mathbf{y}_1, \cdots, \mathbf{y}_Y \}\),\(p_{data}(x)\)is represented by a mixture of Dirac delta distributions:\(p_{\text{data}}(x) = \frac{1}{Y} \sum_{i=1}^{Y} \delta(\mathbf{x} - \mathbf{y}_i)\)</p> \[\begin{aligned} p(\boldsymbol{x} ; \sigma) &amp; =p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right) \\ &amp; =\int_{\mathbb{R}^d} p_{\text {data }}\left(\boldsymbol{x}_0\right) \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \\ &amp; =\int_{\mathbb{R}^d}\left[\frac{1}{Y} \sum_{i=1}^Y \delta\left(\boldsymbol{x}_0-\boldsymbol{y}_i\right)\right] \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \mathrm{d} \boldsymbol{x}_0 \\ &amp; =\frac{1}{Y} \sum_{i=1}^Y \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{x}_0, \sigma^2 \mathbf{I}\right) \delta\left(\boldsymbol{x}_0-\boldsymbol{y}_i\right) \mathrm{d} \boldsymbol{x}_0 \\ &amp; =\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\end{aligned}\] <p>Considering the Eq. 2. by expanding the expections:</p> \[\begin{aligned} \mathcal{L}(D ; \sigma) &amp; =\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{y}+\boldsymbol{n} ; \sigma)-\boldsymbol{y}\|_2^2 \\ &amp; =\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \mathbb{E}_{\boldsymbol{x} \sim \mathcal{N}\left(\boldsymbol{y}, \sigma^2 \mathbf{I}\right)}\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}\|_2^2 \\ &amp; =\mathbb{E}_{\boldsymbol{y} \sim p_{\text {data }}} \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}, \sigma^2 \mathbf{I}\right)\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}\|_2^2 \mathrm{~d} \boldsymbol{x} \\ &amp; =\frac{1}{Y} \sum_{i=1}^Y \int_{\mathbb{R}^d} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2 \mathrm{~d} \boldsymbol{x} \\ &amp; =\int_{\mathbb{R}^d} \underbrace{\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2}_{=: \mathcal{L}(D ; \boldsymbol{x}, \sigma)} \mathrm{d} \boldsymbol{x} .\end{aligned}\] <p>we can minimize \(\mathcal{L}(D ; \sigma)\)by minimizing $$\mathcal{L}(D ; \boldsymbol{x}, \sigma)</p> <p>\(independently for each\)\boldsymbol{x}$$:</p> <p>\(D(\boldsymbol{x} ; \sigma)=\arg \min _{D(\boldsymbol{x} ; \sigma)} \mathcal{L}(D ; \boldsymbol{x}, \sigma)\) This is a convex optimization problem; its solution is uniquely identiﬁed by setting the gradient w.r.t. \(D(\boldsymbol{x}; \sigma)\)to zero:</p> \[\begin{aligned} \mathbf{0} &amp; =\nabla_{D(\boldsymbol{x} ; \sigma)}[\mathcal{L}(D ; \boldsymbol{x}, \sigma)] \\ \mathbf{0} &amp; =\nabla_{D(\boldsymbol{x} ; \sigma)}\left[\frac{1}{Y} \sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2\right] \\ \mathbf{0} &amp; =\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \nabla_{D(\boldsymbol{x} ; \sigma)}\left[\left\|D(\boldsymbol{x} ; \sigma)-\boldsymbol{y}_i\right\|_2^2\right] \\ \mathbf{0} &amp; =\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[2 D(\boldsymbol{x} ; \sigma)-2 \boldsymbol{y}_i\right] \\ \mathbf{0} &amp; =\left[\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\right] D(\boldsymbol{x} ; \sigma)-\sum_{i=1}^Y \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i \\ D(\boldsymbol{x} ; \sigma) &amp; =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\end{aligned}\] <p>which gives a closed-form solution for the ideal denoiser \(D(\boldsymbol{x}; \sigma)\).</p> \[D(\boldsymbol{x} ; \sigma) =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\] <p>Next, let us consider the score of the distribution</p> \[\begin{aligned} \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma) &amp; =\frac{\nabla_{\boldsymbol{x}} p(\boldsymbol{x} ; \sigma)}{p(\boldsymbol{x} ; \sigma)} \\ &amp; =\frac{\nabla_{\boldsymbol{x}}\left[\frac{1}{Y} \sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\right]}{\left[\frac{1}{Y} \sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\right]} \\ &amp; =\frac{\sum_i \nabla_{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}\end{aligned}\] <p>Then</p> \[\begin{aligned} \nabla_{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) &amp; =\nabla_{\boldsymbol{x}}\left[\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \exp \frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ &amp; =\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \nabla_{\boldsymbol{x}}\left[\exp \frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ &amp; =\left[\left(2 \pi \sigma^2\right)^{-\frac{d}{2}} \exp \frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \nabla_{\boldsymbol{x}}\left[\frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ &amp; =\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \nabla_{\boldsymbol{x}}\left[\frac{\left\|\boldsymbol{x}-\boldsymbol{y}_i\right\|_2^2}{-2 \sigma^2}\right] \\ &amp; =\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[\frac{\boldsymbol{y}_i-\boldsymbol{x}}{\sigma^2}\right] .\end{aligned}\] <p>Next</p> \[\begin{aligned} \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma) &amp; =\frac{\sum_i \nabla_{\boldsymbol{x}} \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)} \\ &amp; =\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)\left[\frac{\boldsymbol{y}_i-\boldsymbol{x}}{\sigma^2}\right]}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)} \\ &amp; =\left(\frac{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right) \boldsymbol{y}_i}{\sum_i \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{y}_i, \sigma^2 \mathbf{I}\right)}-\boldsymbol{x}\right) / \sigma^2 .\end{aligned}\] <p>So \(\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma)=(D(\boldsymbol{x} ; \sigma)-\boldsymbol{x}) / \sigma^2\)</p> <h1 id="proposed-sde">Proposed SDE</h1> <h3 id="task-1-find-a-pde-given-initial-value-qboldsymbolx-0-p_databoldsymbolx-and-solution-qboldsymbolx-t--pboldsymbolx-sigmat">Task 1: Find a PDE given initial value \(q(\boldsymbol{x}, 0):= p_{data}(\boldsymbol{x})\), and solution \(q(\boldsymbol{x}, t) = p(\boldsymbol{x}, \sigma(t))\)</h3> <h4 id="the-solution-is-a-heat-equation">The solution is a heat equation</h4> \[\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)\] <h4 id="proof-2">Proof</h4> <p>\(p(\boldsymbol{x} ; \sigma)=p_{\text {data }} * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)\) and \(p_t(\boldsymbol{x})=s(t)^{-d} p(\boldsymbol{x} / s(t) ; \sigma(t))\)</p> <p>the density evolves according to a heat diffusion PDE with time-varying diffusivity. As a ﬁrst step, we ﬁnd this PDE.</p> <p>The solution can be generated by the heat equation with time-varying diffusivity \(\kappa(t)\).</p> <p>The heat equation PDE:</p> \[\frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t}=-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)\] <p>Take Fourier transform along the \(\boldsymbol{x}-\text{dimension}\)</p> \[\frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t}=-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)\] <p>Since \(q(\boldsymbol{x}, t)=p(\boldsymbol{x} ; \sigma(t))=p_{\text {data }}(\boldsymbol{x}) * \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)\), \(\hat{q}(\boldsymbol{\nu}, t)=\hat{p}_{\text {data }}(\boldsymbol{\nu}) \exp \left(-\frac{1}{2}|\boldsymbol{\nu}|^2 \sigma(t)^2\right)\)</p> <p>Differentiating the target solution along the time axis, we have</p> <p>\(\begin{aligned} \frac{\partial \hat{q}(\boldsymbol{\nu}, t)}{\partial t} &amp; =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{p}_{\text {data }}(\boldsymbol{\nu}) \exp \left(-\frac{1}{2}|\boldsymbol{\nu}|^2 \sigma(t)^2\right) \\ &amp; =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t)\end{aligned}\) Then we have</p> \[\begin{aligned}-\kappa(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t) &amp; =-\dot{\sigma}(t) \sigma(t)|\boldsymbol{\nu}|^2 \hat{q}(\boldsymbol{\nu}, t) \\ \kappa(t) &amp; =\dot{\sigma}(t) \sigma(t) .\end{aligned}\] <p>To summzrize</p> \[\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)\] <h3 id="task-2-seek-an-sde-whose-solution-density-is-described-by-the-pde-fracpartial-qboldsymbolx-tpartial-tdotsigmat-sigmat-delta_boldsymbolx-qboldsymbolx-t">Task 2: Seek an SDE whose solution density is described by the PDE \(\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)\)</h3> <h4 id="solution-mathrmd-boldsymbolxleftfrac12-gt2-dotsigmat-sigmatright-nabla_boldsymbolx-log-pboldsymbolx--sigmat-mathrmd-tgt-mathrmd-omega_t">Solution: \(\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t\)</h4> <h4 id="proof-3">Proof</h4> <p>Given an SDE,\(\mathrm{d} \boldsymbol{x}=\boldsymbol{f}(\boldsymbol{x}, t) \mathrm{d} t+\boldsymbol{g}(\boldsymbol{x}, t) \mathrm{d} \omega_t\), The Fokker–Planck PDE describes the time evolution of its solution probability density \(r(\boldsymbol{x},t)\) as</p> \[\frac{\partial r(\boldsymbol{x}, t)}{\partial t}=-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))+\frac{1}{2} \nabla_{\boldsymbol{x}} \nabla_{\boldsymbol{x}}:(\mathbf{D}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))\] <p>where \(\mathbf{D}_{i j}=\sum_k \boldsymbol{g}_{i k} \boldsymbol{g}_{j k}\) is diffusion tensor. We consider a special case \(\boldsymbol{g}(\boldsymbol{x}, t) = g(t) \boldsymbol{I}\)</p> \[\frac{\partial r(\boldsymbol{x}, t)}{\partial t}=-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) r(\boldsymbol{x}, t))+\frac{1}{2} g(t)^2 \Delta_{\boldsymbol{x}} r(\boldsymbol{x}, t)\] <p>We can find a sufﬁcient condition that sastify the PDE \(\frac{\partial q(\boldsymbol{x}, t)}{\partial t}=\dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)\)</p> \[-\nabla_{\boldsymbol{x}} \cdot(\boldsymbol{f}(\boldsymbol{x}, t) q(\boldsymbol{x}, t))+\frac{1}{2} g(t)^2 \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) = \dot{\sigma}(t) \sigma(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t)\] <p>The key is given by the identity \(\nabla_{\boldsymbol{x}} \cdot \nabla_{\boldsymbol{x}}=\Delta_{\boldsymbol{x}}\). We set \(\boldsymbol{f}(\boldsymbol{x}, t) q(\boldsymbol{x}, t)=v(t) \nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)\), then</p> \[\begin{aligned} \nabla_{\boldsymbol{x}} \cdot\left(v(t) \nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)\right) &amp; =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) \\ v(t) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) &amp; =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \Delta_{\boldsymbol{x}} q(\boldsymbol{x}, t) \\ v(t) &amp; =\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t) .\end{aligned}\] <p>\(\boldsymbol{f}(\boldsymbol{x}, t)\)is in fact proportional to the score function:</p> \[\begin{aligned} \boldsymbol{f}(\boldsymbol{x}, t) &amp; =v(t) \frac{\nabla_{\boldsymbol{x}} q(\boldsymbol{x}, t)}{q(\boldsymbol{x}, t)} \\ &amp; =v(t) \nabla_{\boldsymbol{x}} \log q(\boldsymbol{x}, t) \\ &amp; =\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log q(\boldsymbol{x}, t)\end{aligned}\] <p>we recover a family of SDEs whose solution densities have the desired marginals with noise levels \(\sigma(t)\) for any choice of \(g(t)\):</p> \[\mathrm{d} \boldsymbol{x}=\left(\frac{1}{2} g(t)^2-\dot{\sigma}(t) \sigma(t)\right) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(t)) \mathrm{d} t+g(t) \mathrm{d} \omega_t\]]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Original ODE / SDE formulation from previous work]]></summary></entry><entry><title type="html">Generative Modeling by Estimating Gradients of the Data Distribution</title><link href="https://szhan311.github.io//blog/2023/SGM-(NCSN)/" rel="alternate" type="text/html" title="Generative Modeling by Estimating Gradients of the Data Distribution"/><published>2023-12-05T20:36:00+00:00</published><updated>2023-12-05T20:36:00+00:00</updated><id>https://szhan311.github.io//blog/2023/SGM%20(NCSN)</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/SGM-(NCSN)/"><![CDATA[<p>Gradients can be ill-deﬁned and hard to estimate when the data resides on low-dimensional manifolds.</p> <p>Two ingredients: score matching and Langevin dynamics</p> <p>Since Langevin dynamics will often be initialized in low-density regions of the data distribution, inaccurate score estimation in these regions will negatively affect the sampling process.</p> <p>In practice, this analysis also holds when different modes have approximately disjoint supports—they may share the same support but be connected by regions of small data density.</p> <h1 id="score-matching">Score matching</h1> <p>The diffusion model uses energy-based models: : \(p_\theta(\mathbf{x})=\frac{e^{f_\theta(\mathbf{x})}}{Z(\theta)}\) Score function: \(s_{\theta}(\textbf{x}) = \nabla_\mathbf{x} \log p_\theta(\mathbf{x})\)</p> <p>The goal of score-based model:</p> \[\begin{equation*} \mathbf{s}_\theta (\mathbf{x}) = \nabla_{\mathbf{x}} \log p_\theta (\mathbf{x} ) = \nabla_{\mathbf{x}} f_\theta (\mathbf{x}) - \underbrace{\nabla_\mathbf{x} \log Z_\theta}_{=0} = \nabla_\mathbf{x} f_\theta(\mathbf{x}). \end{equation*}\] <p>Minimize the Fisher divergence, or simply the MSE: \(\mathcal{L}_{mse} = \mathbb{E}_{p_{\text {data }}} \left[\frac{1}{2} \left\lVert s_{\theta}(\mathbf{x}) - \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x}) \right\lVert_2^2 \right]\)</p> <p>However, Fisher divergence is not directly computable, the real \(\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})\) is usually unknown.</p> <p>To simply discussion, consider 1-D random variables:</p> \[\begin{aligned} \mathcal{L}_{mse} = &amp; \frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\nabla_x \log p_{\text {data }}(x)-\nabla_x \log p_\theta(x)\right)^2\right] \\ = &amp; \frac{1}{2} \int p_{\text {data }}(x)\left(\nabla_x \log p_{\text {data }}(x)-\nabla_x \log p_\theta(x)\right)^2 \mathrm{~d} x \\ = &amp; \underbrace{\frac{1}{2} \int p_{\text {data }}(x)\left(\nabla_x \log p_{\text {data }}(x)\right)^2 \mathrm{~d} x}_{\text {const }}+\frac{1}{2} \int p_{\text {data }}(x)\left(\nabla_x \log p_\theta(x)\right)^2 \mathrm{~d} x \\ &amp; -\int p_{\text {data }}(x) \nabla_x \log p_\theta(x) \nabla_x \log p_{\text {data }}(x) \mathrm{d} x .\end{aligned}\] <p>The objective can be written in 3 terms, the first term is a constant, the second term is easy to calculate, next we handle the thirt term by integration by parts:</p> \[\begin{aligned} &amp;-\int p_{\text {data }}(x) \nabla_x \log p_\theta(x) \nabla_x \log p_{\text {data }}(x) \mathrm{d} x \\ &amp;=-\int \nabla_x \log p_\theta(x) \nabla_x p_{\text {data }}(x) \mathrm{d} x \\ &amp;=-\left.p_{\text {data }}(x) \nabla_x \log p_\theta(x)\right|_{-\infty} ^{\infty}+\int p_{\text {data }}(x) \nabla_x^2 \log p_\theta(x) \mathrm{d} x \\ &amp; \stackrel{(i)}{=} \mathbb{E}_{p_{\text {data }}}\left[\nabla_x^2 \log p_\theta(x)\right]\end{aligned}\] <p>where \((i)\)holds if we assume \(p_{\text{data}}(x) \rightarrow 0\)where \(|x|\rightarrow 0\). Now, \(\begin{aligned} &amp; \frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\nabla_x \log p_{\text {data }}(x)-\nabla_x \log p_\theta(x)\right)^2\right] \\ = &amp; \mathbb{E}_{p_{\text {data }}}\left[\nabla_x^2 \log p_\theta(x)\right]+\frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\nabla_x \log p_\theta(x)\right)^2\right]+\text { const. }\end{aligned}\)</p> <p>Generalizing the integration by parts to multi-dimentsional data, the minimum of \(\mathcal{L}_{mse}\) can be found through a tractable objective:</p> <p>\(\mathcal{L}_{matching} = \mathbb{E}_{p_{\text {data }}} \left[ \text{ tr}\left( \nabla_{\mathbf{x}} s_{\theta}(\mathbf{x}) \right) + \frac{1}{2} \left\Vert s_{\theta}(\mathbf{x}) \right\lVert_2^2 \right]\) where \(\nabla_{\mathbf{x}} s_{\theta}(\mathbf{x})\) denotes the Jacobian of \(s_{\theta}(\mathbf{x})\) with respect to \(\mathbf{x}\), and \(\text{tr}(\cdot)\) is the <em>trace</em> operation.</p> <p>Cons: the computation of\(\text{ tr}\left( \nabla_{\mathbf{x}} s_{\theta}(\mathbf{x}) \right)\)is a \(O(N^2+N)\), thus not being suitable for high-dimensional problems.</p> <h1 id="sliced-score-matching">Sliced score matching</h1> <p>Idea: <strong>one dimensional data</strong> distribution is much easier to estimate for score matching.</p> <p>We denote \(\mathbf{v}\)as the random projection direction, and \(p_\mathbf{v}\)as its distribution. The random projected version of Fisher divergence is:</p> <p>\(\frac{1}{2} \mathbb{E}_{p_{\text {data }}}\left[\left(\mathbf{v}^{\top} \nabla_{\mathbf{x}} \log p_{\text {data }}(\mathbf{x})-\mathbf{v}^{\top} \nabla_{\mathbf{x}} \log p_\theta(\mathbf{x})\right)^2\right]\) which we name as the sliced Fisher divergence. Therefore play the same trick of integration by parts to obatain the following tractable form:</p> <ul> <li> \[\mathbb{E}_{p_{\text {data }}}\left[\mathbf{v}^{\top} \nabla_{\mathbf{x}}^2 \log p_\theta(\mathbf{x}) \mathbf{v}+\frac{1}{2}\left(\mathbf{v}^{\top} \nabla_{\mathbf{x}} \log p_\theta(\mathbf{x})\right)^2\right]+ \text{const}\] </li> <li> \[\mathbb{E}_{p_{\text {\textbf{v} }}}\mathbb{E}_{p_{\text {data }}} \left[ \mathbf{v}^T \nabla_{\mathbf{x}} \mathcal{F}_{\theta}(\mathbf{x}) \mathbf{v} + \frac{1}{2} \left\Vert \mathbf{v}^T \mathcal{F}_{\theta}(\mathbf{x}) \right\lVert_2^2 \right]\] </li> </ul> <h1 id="denoising-score-matching">Denoising score matching</h1> <p>The objective is:</p> \[E_{q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})} E_{\mathbf{x} \sim p(\mathbf{x})} \left[ \left\Vert s_{\theta}(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x}) \right\lVert_2^2 \right]\] <p>However, it should be noted that \(s_{\theta}(\mathbf{x}) = \nabla_{\mathbf{x}} \log q_{\sigma}(\mathbf{x}) \approx \nabla_{\mathbf{x}} \log p(\mathbf{x})\) is only true when the noise is small enough to consider that \(q_{\sigma}(\mathbf{x}) \approx p(\mathbf{x})\), if we choose the noise distribution to be \(q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})=\mathcal{N}(\tilde{\mathbf{x}}\mid\mathbf{x}, \sigma^{2}\mathbf{I})\), then we have \(\nabla_{\tilde{\mathbf{x}}} \log q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x}) = \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^{2}}\). Therefore, the denoising score matching loss simply becomes:</p> \[\mathcal{l}(\theta;\sigma) = E_{q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})} E_{\mathbf{x} \sim p(\mathbf{x})} \left[ \left\Vert \mathcal{F}_{\theta}(\tilde{\mathbf{x}}) + \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^{2}} \right\lVert_2^2 \right].\] <h1 id="noise-conditional-score-networks-ncsn">Noise conditional score networks (NCSN)</h1> <p>We consider a positive geometric sequence of noise variances \(\{\sigma_{i}\}_{i=1}^{L}\), choosing \(\sigma_{1}\) to be large enough to mitigate manifold issue, and satisfying \(\frac{\sigma_{1}}{\sigma_{2}} = \cdots = \frac{\sigma_{L-1}}{\sigma_{L}} &gt; 1\). The goal is to train a conditional network to estimate the gradients of all perturbed data distributions, ie.</p> \[\forall \sigma \in \{\sigma_{i}\}_{i=1}^{L}, s_{\theta}(\tilde{\mathbf{x}}, \sigma) \approx \nabla_{\mathbf{x}} \log q_{\sigma}(\mathbf{x})\] <p>The objective</p> \[\mathcal{l}(\theta;\sigma) = E_{q_{\sigma}(\tilde{\mathbf{x}}\mid\mathbf{x})} E_{\mathbf{x} \sim p(\mathbf{x})} \left[ \left\Vert s_{\theta}(\tilde{\mathbf{x}}, \sigma) + \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^{2}} \right\lVert_2^2 \right]\] <p>this objective can be combined to obtain a single unified objective as:</p> \[\mathcal{L}(\theta;\{\sigma_{i}\}_{i=1}^{L}) = \frac{1}{L} \sum_{i=1}^{L} \lambda(\sigma_{i})\mathcal{l}(\theta;\sigma_{i})\] <p>where \(\lambda(\sigma_{i}) &gt; 0\) is a coefficient function depending on \(\sigma_{i}\).</p> <h1 id="sampling-with-langevin-dynamics">Sampling with Langevin dynamics</h1> <ul> <li> <p>Langevin dynamics can produce samples from a probability density \(p(\mathbf{x})\)using only the score function \(\nabla_{\mathbf{x}} \log p(\mathbf{x})\).</p> </li> <li> \[\tilde{\mathbf{x}}_t = \tilde{\mathbf{x}}_{t-1} + \frac{\epsilon}{2} \nabla_{\mathbf{x}} \log p(\tilde{\mathbf{x}}_{t-1}) + \sqrt{\epsilon} \mathbf{z}_t\] </li> <li>a Metropolis-Hastings update is ignored</li> <li>Inaccurate score estimation with score matching in regions of low data density</li> <li>Annealed Langevin dynamic samples</li> </ul>]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Gradients can be ill-deﬁned and hard to estimate when the data resides on low-dimensional manifolds.]]></summary></entry><entry><title type="html">Denoising Diffusion Probabilistic Models</title><link href="https://szhan311.github.io//blog/2023/DDPM/" rel="alternate" type="text/html" title="Denoising Diffusion Probabilistic Models"/><published>2023-12-04T20:23:00+00:00</published><updated>2023-12-04T20:23:00+00:00</updated><id>https://szhan311.github.io//blog/2023/DDPM</id><content type="html" xml:base="https://szhan311.github.io//blog/2023/DDPM/"><![CDATA[<h1 id="background">Background</h1> <h3 id="vlb-bound">VLB Bound</h3> \[\begin{align} &amp; \mathbb{E}_{q(\mathbf{x}_0)}[-\log p_{\theta}(\mathbf{x}_0)] \\ &amp;= -\mathbb{E}_{q(\mathbf{x}_0)}[\log (p_{\theta}(\mathbf{x}_0) \int p_{\theta} (\mathbf{x}_{1:T})d\mathbf{x}_{1:T})] \\ &amp;= -\mathbb{E}_{q(\mathbf{x}_0)}[ \log(\int p_{\theta} (\mathbf{x}_{0:T})d\mathbf{x}_{1:T})] \\ &amp;= -\mathbb{E}_{q(\mathbf{x}_0)}[\log (\int q (\mathbf{x}_{1:T}|\mathbf{x}_0) \frac{ p_{\theta} (\mathbf{x}_{0:T}) }{q(\mathbf{x}_{1:T}|\mathbf{x}_0) }d\mathbf{x}_{1:T}] \\ &amp;= -\mathbb{E}_{q(\mathbf{x}_0)}[\log (E_{ q (\mathbf{x}_{1:T}|\mathbf{x}_0)} \frac{ p_{\theta} (\mathbf{x}_{0:T}) }{q(\mathbf{x}_{1:T}|\mathbf{x}_0) })] \\ &amp;\leq -\mathbb{E}_{q(\mathbf{x}_{0:T})}[\log \frac{ p_{\theta} (\mathbf{x}_{0:T}) }{q(\mathbf{x}_{1:T}|\mathbf{x}_0) })] = L_{VLB} \end{align}\] \[\begin{align} L_{VLB} &amp;= \mathbb{E_q}[-\log \frac{p_{\theta}(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)})] \\ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum_{t \geq 1}\log \frac{p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t}|\mathbf{x}_{t-1})})] \\ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum_{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t}|\mathbf{x}_{t-1})}) - \log \frac{p_{\theta}(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_{1}|\mathbf{x}_{0})}] \\ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T)- \sum_{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0)}\frac{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_0)}) - \log \frac{p_{\theta}(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_{1}|\mathbf{x}_{0})}] \\ &amp;= \mathbb{E_q}[-\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_0)}- \sum_{t &gt;1}\log \frac{p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0)} - \log p_{\theta}(\mathbf{x}_{0}|\mathbf{x}_1)] \\ &amp;= \mathbb{E_q}[ D_{\text{KL}}(q(\mathbf{x}_T|\mathbf{x}_0)||p_(\mathbf{x}_T)) + \sum_{t&gt;1} D_{\text{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)||p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t))-\log p_{\theta}(\mathbf{x}_{0}|\mathbf{x}_1)] \end{align}\] <h3 id="some-distributions">Some Distributions</h3> <ul> <li> \[q(x_t|x_{t-1}):= \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \sqrt{\beta_t}\mathbf{I})\] </li> <li> \[q(x_t|x_0) \sim \mathcal{N} (x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t) \mathbf{I})\] <ul> <li> <p>Let \(\alpha_t = 1 - \beta_t\), \(\bar{\alpha}_t = \alpha_1 \alpha_2 \cdots \alpha_t\)</p> </li> <li> \[\begin{align} x_t &amp;= \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_1 \\ &amp;= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1-\alpha_t} \epsilon_t + \sqrt{\alpha_t - \alpha_t \alpha_{t-1}} \epsilon_{2} \\ &amp;=\sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}} \bar{\epsilon}_{2} \\ &amp;= \sqrt{\alpha_t \alpha_{t-1} \cdots \alpha_1}x_{0} + \sqrt{1-\alpha_t \alpha_{t-1}\cdots \alpha_0} \bar{\epsilon}_{t} \\ &amp;:= \sqrt{\bar{\alpha_t}} x_0 + \sqrt{1-\bar{\alpha_t}} \bar{\epsilon}_t \end{align}\] </li> </ul> </li> <li> \[q(x_{t-1}|x_t, x_0) \sim \mathcal{N} (x_{t-1}; \tilde{\mu}_t (x_t, x_0), \tilde{\beta}_t \mathbf{I})\] <ul> <li>\(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right), \tilde{\beta}_t \mathbf{I}\right)\) where \(\quad \tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right):=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{x}_t \quad$ and $\quad \tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t\)</li> </ul> </li> <li> \[\begin{aligned} &amp; q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)=\frac{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0\right) q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right)}{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)} \\ &amp; =\frac{\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\alpha_t} \boldsymbol{x}_{t-1},\left(1-\alpha_t\right) \mathbf{I}\right) \mathcal{N}\left(\boldsymbol{x}_{t-1} ; \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0,\left(1-\bar{\alpha}_{t-1}\right) \mathbf{I}\right)}{\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\bar{\alpha}_t} \boldsymbol{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)} \\ &amp; \propto \exp \left\{-\left[\frac{\left(\boldsymbol{x}_t-\sqrt{\alpha_t} \boldsymbol{x}_{t-1}\right)^2}{2\left(1-\alpha_t\right)}+\frac{\left(\boldsymbol{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0\right)^2}{2\left(1-\bar{\alpha}_{t-1}\right)}-\frac{\left(\boldsymbol{x}_t-\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0\right)^2}{2\left(1-\bar{\alpha}_t\right)}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left[\frac{\left(\boldsymbol{x}_t-\sqrt{\alpha_t} \boldsymbol{x}_{t-1}\right)^2}{1-\alpha_t}+\frac{\left(\boldsymbol{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0\right)^2}{1-\bar{\alpha}_{t-1}}-\frac{\left(\boldsymbol{x}_t-\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0\right)^2}{1-\bar{\alpha}_t}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left[\frac{\left(-2 \sqrt{\alpha_t} \boldsymbol{x}_t \boldsymbol{x}_{t-1}+\alpha_t \boldsymbol{x}_{t-1}^2\right)}{1-\alpha_t}+\frac{\left(\boldsymbol{x}_{t-1}^2-2 \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_{t-1} \boldsymbol{x}_0\right)}{1-\bar{\alpha}_{t-1}}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\ &amp; \propto \exp \left\{-\frac{1}{2}\left[-\frac{2 \sqrt{\alpha_t} \boldsymbol{x}_t \boldsymbol{x}_{t-1}}{1-\alpha_t}+\frac{\alpha_t \boldsymbol{x}_{t-1}^2}{1-\alpha_t}+\frac{\boldsymbol{x}_{t-1}^2}{1-\bar{\alpha}_{t-1}}-\frac{2 \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_{t-1} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left[\left(\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left[\frac{\alpha_t\left(1-\bar{\alpha}_{t-1}\right)+1-\alpha_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} x_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left[\frac{\alpha_t-\bar{\alpha}_t+1-\alpha_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left[\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left(\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}\right)\left[\boldsymbol{x}_{t-1}^2-2 \frac{\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right)}{\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}} \boldsymbol{x}_{t-1}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left(\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}\right)\left[\boldsymbol{x}_{t-1}^2-2 \frac{\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right)\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \boldsymbol{x}_{t-1}\right]\right\} \\ &amp; =\exp \left\{-\frac{1}{2}\left(\frac{1}{\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t}}\right)\left[\boldsymbol{x}_{t-1}^2-2 \frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) \boldsymbol{x}_0}{1-\bar{\alpha}_t} \boldsymbol{x}_{t-1}\right]\right\} \\ &amp; \propto \mathcal{N}(\boldsymbol{x}_{t-1} ; \underbrace{\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) \boldsymbol{x}_0}{1-\bar{\alpha}_t}}_{\mu_q\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}, \underbrace{\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{I}}_{\boldsymbol{\Sigma}_q(t)}) \\ &amp; \end{aligned}\] </li> <li>KL divergence of two Gaussian: \(D_{KL}(P \| Q) = \log\left(\frac{\sigma_Q}{\sigma_P}\right) + \frac{\sigma_P^2 + (\mu_P - \mu_Q)^2}{2\sigma_Q^2} - \frac{1}{2}\)</li> </ul> <h1 id="reverse-process">Reverse process</h1> <ul> <li>\(p_{\theta}(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_{\theta}(\mathbf{x}_t, t), \boldsymbol{\Sigma}_{\theta}(\mathbf{x}_t, t))\)for \(1 \leq t \leq T\)</li> <li> <p>We set \(\boldsymbol{\Sigma}_{\theta}(x_t, t) = \sigma_t^2 \mathbf{I}\). Experimentally, both \(\sigma_t^2 = \beta_t\) and \(\sigma_t^2 = \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t\) had similar results</p> </li> <li>Q: In the reverse, why we set\(\boldsymbol{\Sigma}_{\theta}(x_t, t)\)to be independent of \(x_t\)</li> </ul>]]></content><author><name></name></author><category term="Diffusion-models"/><category term="Literature-review"/><summary type="html"><![CDATA[Background]]></summary></entry></feed>